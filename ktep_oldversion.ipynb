{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dxrksvng/eng-score-exit/blob/main/ktep_oldversion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RUN all à¹ƒà¸«à¸¡à¹ˆà¹à¸¥à¹‰à¸§ GPU à¸«à¸¡à¸”à¸‚à¸µà¸”à¸ˆà¸³à¸à¸±à¸”à¸ˆà¸£à¹‰à¸² à¸£à¸°à¹€à¸šà¸´à¹‰à¸”à¹€à¸šà¸´à¹‰à¸”à¹à¸«à¸¥à¹ˆà¸§**"
      ],
      "metadata": {
        "id": "wdBzxjSQBRyp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fojeGgi4JNa",
        "outputId": "984ad326-da93-4fe7-a7c4-6db0449cc652"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZchumLaT2_-Z"
      },
      "source": [
        "# SET UP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "tVLhvN9S2_xM",
        "outputId": "b23d62e0-5509-4fdf-bf61-e59cbecf5f2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdf2image\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from pdf2image) (11.2.1)\n",
            "Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pdf2image\n",
            "Successfully installed pdf2image-1.17.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 697 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.8 [186 kB]\n",
            "Fetched 186 kB in 0s (406 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 126111 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.8_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.8) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.8) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting jiwer\n",
            "  Downloading jiwer-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from jiwer) (8.2.1)\n",
            "Collecting rapidfuzz>=3.9.7 (from jiwer)\n",
            "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Downloading jiwer-3.1.0-py3-none-any.whl (22 kB)\n",
            "Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\n",
            "Successfully installed jiwer-3.1.0 rapidfuzz-3.13.0\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (24.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (11.2.1)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.13\n",
            "Collecting easyocr\n",
            "  Downloading easyocr-1.7.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from easyocr) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.11/dist-packages (from easyocr) (0.21.0+cu124)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.11/dist-packages (from easyocr) (4.11.0.86)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from easyocr) (1.15.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from easyocr) (2.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from easyocr) (11.2.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from easyocr) (0.25.2)\n",
            "Collecting python-bidi (from easyocr)\n",
            "  Downloading python_bidi-0.6.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from easyocr) (6.0.2)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.11/dist-packages (from easyocr) (2.1.1)\n",
            "Collecting pyclipper (from easyocr)\n",
            "  Downloading pyclipper-1.3.0.post6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Collecting ninja (from easyocr)\n",
            "  Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->easyocr)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->easyocr)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->easyocr)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->easyocr)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->easyocr)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->easyocr)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->easyocr)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->easyocr)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->easyocr)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->easyocr)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->easyocr) (1.3.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr) (2025.6.1)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr) (0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->easyocr) (3.0.2)\n",
            "Downloading easyocr-1.7.2-py3-none-any.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m121.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyclipper-1.3.0.post6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (969 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m969.6/969.6 kB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_bidi-0.6.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (292 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m292.9/292.9 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-bidi, pyclipper, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ninja, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, easyocr\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed easyocr-1.7.2 ninja-1.11.1.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pyclipper-1.3.0.post6 python-bidi-0.6.6\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.32.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"
          ]
        }
      ],
      "source": [
        "!pip install pdf2image\n",
        "!apt-get install poppler-utils\n",
        "!pip install jiwer\n",
        "!pip install pytesseract\n",
        "!pip install easyocr\n",
        "!pip install transformers torch sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7zujbWD5iSE"
      },
      "source": [
        "# Tranfrom to image (.png) AUTO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "vAtBHGjn5m0r"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from pdf2image import convert_from_path\n",
        "\n",
        "# --- 1. à¸à¸²à¸£à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸² ---\n",
        "\n",
        "# à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œà¸—à¸µà¹ˆà¹€à¸à¹‡à¸šà¹„à¸Ÿà¸¥à¹Œà¸•à¹‰à¸™à¸—à¸²à¸‡ (PDF, JPG, etc.)\n",
        "INPUT_DIR = '/content/drive/MyDrive/kmitl_dataset/dataset'\n",
        "\n",
        "# à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œà¸ªà¸³à¸«à¸£à¸±à¸šà¹€à¸à¹‡à¸šà¹„à¸Ÿà¸¥à¹Œà¸ à¸²à¸ .png à¸—à¸µà¹ˆà¹à¸›à¸¥à¸‡à¹€à¸ªà¸£à¹‡à¸ˆà¹à¸¥à¹‰à¸§\n",
        "OUTPUT_DIR = '/content/drive/MyDrive/kmitl_dataset/final-tran-to-img'\n",
        "\n",
        "# --- 2. à¹‚à¸„à¹‰à¸”à¸«à¸¥à¸±à¸à¹ƒà¸™à¸à¸²à¸£à¹à¸›à¸¥à¸‡à¹„à¸Ÿà¸¥à¹Œ ---\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¸«à¸¥à¸±à¸à¸—à¸µà¹ˆà¸ˆà¸°à¸§à¸™à¸¥à¸¹à¸›à¹€à¸à¸·à¹ˆà¸­à¹à¸›à¸¥à¸‡à¹„à¸Ÿà¸¥à¹Œà¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”\n",
        "    \"\"\"\n",
        "    # à¸ªà¸£à¹‰à¸²à¸‡à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œà¸›à¸¥à¸²à¸¢à¸—à¸²à¸‡à¸«à¸²à¸à¸¢à¸±à¸‡à¹„à¸¡à¹ˆà¸¡à¸µ\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "    # à¸„à¹‰à¸™à¸«à¸²à¹„à¸Ÿà¸¥à¹Œà¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”à¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¹à¸›à¸¥à¸‡à¹ƒà¸™à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œà¸•à¹‰à¸™à¸—à¸²à¸‡\n",
        "    files_to_process = [f for f in os.listdir(INPUT_DIR) if f.lower().endswith(('.pdf', '.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "    if not files_to_process:\n",
        "        print(f\"à¹„à¸¡à¹ˆà¸à¸šà¹„à¸Ÿà¸¥à¹Œà¸—à¸µà¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¹à¸›à¸¥à¸‡à¹„à¸”à¹‰à¹ƒà¸™à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œ: {INPUT_DIR}\")\n",
        "        return\n",
        "\n",
        "    print(f\"à¸à¸šà¹„à¸Ÿà¸¥à¹Œà¸—à¸±à¹‰à¸‡à¸«à¸¡à¸” {len(files_to_process)} à¹„à¸Ÿà¸¥à¹Œ à¹€à¸£à¸´à¹ˆà¸¡à¸”à¸³à¹€à¸™à¸´à¸™à¸à¸²à¸£à¹à¸›à¸¥à¸‡...\")\n",
        "\n",
        "    # à¸§à¸™à¸¥à¸¹à¸›à¸—à¸³à¸‡à¸²à¸™à¸—à¸µà¸¥à¸°à¹„à¸Ÿà¸¥à¹Œ\n",
        "    for filename in files_to_process:\n",
        "        try:\n",
        "            filepath = os.path.join(INPUT_DIR, filename)\n",
        "\n",
        "            # --- à¸ªà¹ˆà¸§à¸™à¸‚à¸­à¸‡à¸à¸²à¸£à¹à¸›à¸¥à¸‡à¹„à¸Ÿà¸¥à¹Œ ---\n",
        "            if filename.lower().endswith('.pdf'):\n",
        "                # à¸–à¹‰à¸²à¹€à¸›à¹‡à¸™ PDF à¹ƒà¸«à¹‰à¹ƒà¸Šà¹‰ pdf2image à¹à¸›à¸¥à¸‡à¸«à¸™à¹‰à¸²à¹à¸£à¸\n",
        "                images = convert_from_path(filepath)\n",
        "                bgr_image = np.array(images[0])\n",
        "                bgr_image = cv2.cvtColor(bgr_image, cv2.COLOR_RGB2BGR)\n",
        "            else:\n",
        "                # à¸–à¹‰à¸²à¹€à¸›à¹‡à¸™à¹„à¸Ÿà¸¥à¹Œà¸ à¸²à¸à¸­à¸¢à¸¹à¹ˆà¹à¸¥à¹‰à¸§ à¹ƒà¸«à¹‰à¹ƒà¸Šà¹‰ OpenCV à¸­à¹ˆà¸²à¸™à¹€à¸‚à¹‰à¸²à¸¡à¸²à¹‚à¸”à¸¢à¸•à¸£à¸‡\n",
        "                bgr_image = cv2.imread(filepath)\n",
        "\n",
        "            # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸§à¹ˆà¸²à¹‚à¸«à¸¥à¸”à¸ à¸²à¸à¸ªà¸³à¹€à¸£à¹‡à¸ˆà¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆ\n",
        "            if bgr_image is None:\n",
        "                print(f\"  -> à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¸­à¹ˆà¸²à¸™à¹„à¸Ÿà¸¥à¹Œ: {filename}, à¸‚à¸­à¸‚à¹‰à¸²à¸¡à¹„à¸Ÿà¸¥à¹Œà¸™à¸µà¹‰\")\n",
        "                continue\n",
        "\n",
        "            # --- à¸ªà¹ˆà¸§à¸™à¸‚à¸­à¸‡à¸à¸²à¸£à¸šà¸±à¸™à¸—à¸¶à¸à¹„à¸Ÿà¸¥à¹Œ ---\n",
        "            # à¸ªà¸£à¹‰à¸²à¸‡à¸Šà¸·à¹ˆà¸­à¹„à¸Ÿà¸¥à¹Œà¹ƒà¸«à¸¡à¹ˆà¹ƒà¸«à¹‰à¹€à¸›à¹‡à¸™ .png à¹€à¸ªà¸¡à¸­\n",
        "            new_filename = f\"{os.path.splitext(filename)[0]}.png\"\n",
        "            save_path = os.path.join(OUTPUT_DIR, new_filename)\n",
        "\n",
        "            # à¸šà¸±à¸™à¸—à¸¶à¸à¹„à¸Ÿà¸¥à¹Œà¸ à¸²à¸\n",
        "            cv2.imwrite(save_path, bgr_image)\n",
        "\n",
        "            print(f\"  -> à¹à¸›à¸¥à¸‡à¹„à¸Ÿà¸¥à¹Œ '{filename}' à¹€à¸›à¹‡à¸™ '{new_filename}' à¹€à¸£à¸µà¸¢à¸šà¸£à¹‰à¸­à¸¢à¹à¸¥à¹‰à¸§\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  !!!! à¹€à¸à¸´à¸”à¸‚à¹‰à¸­à¸œà¸´à¸”à¸à¸¥à¸²à¸”à¸à¸±à¸šà¹„à¸Ÿà¸¥à¹Œ {filename}: {e}\")\n",
        "\n",
        "    print(\"-\" * 50)\n",
        "    print(\"à¸à¸²à¸£à¹à¸›à¸¥à¸‡à¹„à¸Ÿà¸¥à¹Œà¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”à¹€à¸ªà¸£à¹‡à¸ˆà¸ªà¸´à¹‰à¸™!\")\n",
        "    print(f\"à¹„à¸Ÿà¸¥à¹Œà¸ à¸²à¸ .png à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”à¸–à¸¹à¸à¸šà¸±à¸™à¸—à¸¶à¸à¹„à¸§à¹‰à¸—à¸µà¹ˆ: {OUTPUT_DIR}\")\n",
        "\n",
        "\n",
        "# à¸ªà¸±à¹ˆà¸‡à¹ƒà¸«à¹‰à¹‚à¸›à¸£à¹à¸à¸£à¸¡à¹€à¸£à¸´à¹ˆà¸¡à¸—à¸³à¸‡à¸²à¸™à¹€à¸¡à¸·à¹ˆà¸­à¸£à¸±à¸™à¹„à¸Ÿà¸¥à¹Œà¸™à¸µà¹‰\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEo-U66vu107"
      },
      "source": [
        "# Preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qewHtLn_CzPp"
      },
      "source": [
        "**Test single image**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "CzZMOIxQu51F"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# à¸­à¹ˆà¸²à¸™à¸ à¸²à¸\n",
        "image = cv2.imread('/content/drive/MyDrive/kmitl_dataset/final-tran-to-img/KMITL-TEP PILOT-1.png')  # à¹ƒà¸ªà¹ˆà¸Šà¸·à¹ˆà¸­à¹„à¸Ÿà¸¥à¹Œà¸ à¸²à¸à¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¸à¸²à¸£\n",
        "\n",
        "# à¹à¸›à¸¥à¸‡à¸ à¸²à¸à¹€à¸›à¹‡à¸™ HSV à¹€à¸à¸·à¹ˆà¸­à¹à¸¢à¸à¹€à¸‰à¸à¸²à¸°à¸ªà¸µà¸‚à¸²à¸§\n",
        "hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "## à¸à¸³à¸«à¸™à¸”à¸Šà¹ˆà¸§à¸‡à¸ªà¸µà¸”à¸³à¹ƒà¸™ HSV\n",
        "lower_black = np.array([0, 0, 0])  # à¸„à¹ˆà¸²à¸ªà¸µà¸”à¸³à¸•à¹ˆà¸³à¸ªà¸¸à¸” à¸”à¸³à¸ªà¸™à¸´à¸—\n",
        "upper_black = np.array([255, 255, 110])  # à¸„à¹ˆà¸²à¸ªà¸µà¸”à¸³à¸ªà¸¹à¸‡à¸ªà¸¸à¸” à¸”à¸³à¸­à¹ˆà¸­à¸™\n",
        "# H à¸ªà¸µ\n",
        "#S à¸„à¸§à¸²à¸¡à¹€à¸‚à¹‰à¸¡à¸ªà¸µ 0-255 (à¹„à¸¡à¹ˆà¸ªà¸”-à¸ªà¸”à¸¡à¸²à¸)\n",
        "#V à¸„à¸§à¸²à¸¡à¸ªà¸§à¹ˆà¸²à¸‡ à¸”à¸³-à¸ªà¸§à¹ˆà¸²à¸‡à¸ªà¸¸à¸”\n",
        "\n",
        "\n",
        "# à¸à¸³à¸«à¸™à¸”à¸Šà¹ˆà¸§à¸‡à¸ªà¸µà¸™à¹‰à¸³à¹€à¸‡à¸´à¸™à¹ƒà¸™ HSV\n",
        "lower_blue = np.array([90, 30, 50])  # à¸„à¹ˆà¸²à¸ªà¸µà¸™à¹‰à¸³à¹€à¸‡à¸´à¸™à¸•à¹ˆà¸³à¸ªà¸¸à¸”\n",
        "upper_blue = np.array([130, 255, 255])  # à¸„à¹ˆà¸²à¸ªà¸µà¸™à¹‰à¸³à¹€à¸‡à¸´à¸™à¸ªà¸¹à¸‡à¸ªà¸¸à¸”\n",
        "\n",
        "# à¸ªà¸£à¹‰à¸²à¸‡ Mask à¸ªà¸³à¸«à¸£à¸±à¸šà¸ªà¸µà¸”à¸³à¹à¸¥à¸°à¸™à¹‰à¸³à¹€à¸‡à¸´à¸™\n",
        "mask_black = cv2.inRange(hsv, lower_black, upper_black)\n",
        "mask_blue = cv2.inRange(hsv, lower_blue, upper_blue)\n",
        "\n",
        "# à¸£à¸§à¸¡ Mask à¸ªà¸µà¸”à¸³à¹à¸¥à¸°à¸™à¹‰à¸³à¹€à¸‡à¸´à¸™à¹€à¸‚à¹‰à¸²à¸”à¹‰à¸§à¸¢à¸à¸±à¸™\n",
        "mask = cv2.bitwise_or(mask_black, mask_blue)\n",
        "\n",
        "# à¸ªà¸£à¹‰à¸²à¸‡à¸à¸·à¹‰à¸™à¸«à¸¥à¸±à¸‡à¸—à¸µà¹ˆà¸¡à¸µà¸ªà¸µà¹€à¸‰à¸¥à¸µà¹ˆà¸¢à¸‚à¸­à¸‡à¸ à¸²à¸\n",
        "background_color = cv2.mean(image, mask=cv2.bitwise_not(mask))[:3]  # à¸«à¸²à¸„à¹ˆà¸²à¸ªà¸µà¸à¸·à¹‰à¸™à¸«à¸¥à¸±à¸‡\n",
        "background = np.full_like(image, background_color, dtype=np.uint8)\n",
        "\n",
        "# à¹à¸—à¸™à¸—à¸µà¹ˆà¸ªà¹ˆà¸§à¸™à¸—à¸µà¹ˆà¹„à¸¡à¹ˆà¹ƒà¸Šà¹ˆà¸•à¸±à¸§à¸­à¸±à¸à¸©à¸£à¸”à¹‰à¸§à¸¢à¸à¸·à¹‰à¸™à¸«à¸¥à¸±à¸‡\n",
        "inverted_mask = cv2.bitwise_not(mask)\n",
        "result = cv2.bitwise_and(background, background, mask=inverted_mask)\n",
        "final_result = cv2.bitwise_or(image & mask[:, :, None], result)\n",
        "\n",
        "# à¹à¸ªà¸”à¸‡à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œ\n",
        "cv2_imshow(image)\n",
        "cv2_imshow(final_result)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "# à¸šà¸±à¸™à¸—à¸¶à¸à¸ à¸²à¸à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œ\n",
        "cv2.imwrite('output.png', final_result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxYJUmdpu1zR"
      },
      "source": [
        "**AUTO Preprocess**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "NCtFn55mu1Vh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# --- 1. à¸ªà¹ˆà¸§à¸™à¸‚à¸­à¸‡à¸à¸²à¸£à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸² ---\n",
        "\n",
        "# à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œà¸—à¸µà¹ˆà¹€à¸à¹‡à¸šà¹„à¸Ÿà¸¥à¹Œà¸ à¸²à¸à¸•à¹‰à¸™à¸—à¸²à¸‡\n",
        "INPUT_DIR = '/content/drive/MyDrive/kmitl_dataset/final-tran-to-img'\n",
        "\n",
        "# à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œà¸ªà¸³à¸«à¸£à¸±à¸šà¹€à¸à¹‡à¸šà¸ à¸²à¸à¸—à¸µà¹ˆà¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¹€à¸ªà¸£à¹‡à¸ˆà¹à¸¥à¹‰à¸§\n",
        "OUTPUT_DIR = '/content/drive/MyDrive/kmitl_dataset/final-preprecessed'\n",
        "\n",
        "\n",
        "# ---. à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¸ à¸²à¸ (à¸™à¸³à¸¡à¸²à¸ˆà¸²à¸à¹‚à¸„à¹‰à¸”à¸‚à¸­à¸‡à¸„à¸¸à¸“) ---\n",
        "\n",
        "def process_image_with_color_mask(image):\n",
        "    \"\"\"\n",
        "    à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¸ à¸²à¸à¹‚à¸”à¸¢à¹ƒà¸Šà¹‰à¹€à¸—à¸„à¸™à¸´à¸„à¸à¸²à¸£à¹à¸¢à¸à¸ªà¸µ (Color Masking)\n",
        "    à¹€à¸à¸·à¹ˆà¸­à¸„à¸‡à¹„à¸§à¹‰à¹€à¸‰à¸à¸²à¸°à¸•à¸±à¸§à¸­à¸±à¸à¸©à¸£à¸ªà¸µà¸”à¸³à¹à¸¥à¸°à¸™à¹‰à¸³à¹€à¸‡à¸´à¸™\n",
        "    \"\"\"\n",
        "    # à¹à¸›à¸¥à¸‡à¸ à¸²à¸à¹€à¸›à¹‡à¸™ HSV\n",
        "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "    # à¸à¸³à¸«à¸™à¸”à¸Šà¹ˆà¸§à¸‡à¸ªà¸µà¸”à¸³à¹ƒà¸™ HSV\n",
        "    lower_black = np.array([0, 0, 0])\n",
        "    upper_black = np.array([255, 255, 110])\n",
        "\n",
        "    # à¸à¸³à¸«à¸™à¸”à¸Šà¹ˆà¸§à¸‡à¸ªà¸µà¸™à¹‰à¸³à¹€à¸‡à¸´à¸™à¹ƒà¸™ HSV\n",
        "    lower_blue = np.array([90, 30, 50])\n",
        "    upper_blue = np.array([130, 255, 255])\n",
        "\n",
        "    # à¸ªà¸£à¹‰à¸²à¸‡ Mask à¸ªà¸³à¸«à¸£à¸±à¸šà¸ªà¸µà¸”à¸³à¹à¸¥à¸°à¸™à¹‰à¸³à¹€à¸‡à¸´à¸™\n",
        "    mask_black = cv2.inRange(hsv, lower_black, upper_black)\n",
        "    mask_blue = cv2.inRange(hsv, lower_blue, upper_blue)\n",
        "\n",
        "    # à¸£à¸§à¸¡ Mask à¸—à¸±à¹‰à¸‡à¸ªà¸­à¸‡à¸ªà¸µà¹€à¸‚à¹‰à¸²à¸”à¹‰à¸§à¸¢à¸à¸±à¸™\n",
        "    mask = cv2.bitwise_or(mask_black, mask_blue)\n",
        "\n",
        "    # à¸ªà¸£à¹‰à¸²à¸‡à¸à¸·à¹‰à¸™à¸«à¸¥à¸±à¸‡à¸ªà¸µà¸‚à¸²à¸§ (à¹€à¸à¸·à¹ˆà¸­à¹ƒà¸«à¹‰ OCR à¸­à¹ˆà¸²à¸™à¸‡à¹ˆà¸²à¸¢)\n",
        "    background = np.full_like(image, (255, 255, 255), dtype=np.uint8)\n",
        "\n",
        "    # à¹ƒà¸Šà¹‰ mask à¹€à¸à¸·à¹ˆà¸­à¸„à¸‡à¹„à¸§à¹‰à¹€à¸‰à¸à¸²à¸°à¸ªà¹ˆà¸§à¸™à¸—à¸µà¹ˆà¹€à¸›à¹‡à¸™à¸•à¸±à¸§à¸­à¸±à¸à¸©à¸£ (à¸ªà¸µà¸”à¸³à¹à¸¥à¸°à¸™à¹‰à¸³à¹€à¸‡à¸´à¸™)\n",
        "    # à¹à¸¥à¸°à¹à¸—à¸™à¸—à¸µà¹ˆà¸ªà¹ˆà¸§à¸™à¸­à¸·à¹ˆà¸™à¸”à¹‰à¸§à¸¢à¸à¸·à¹‰à¸™à¸«à¸¥à¸±à¸‡à¸ªà¸µà¸‚à¸²à¸§\n",
        "    inverted_mask = cv2.bitwise_not(mask)\n",
        "    background_part = cv2.bitwise_and(background, background, mask=inverted_mask)\n",
        "    text_part = cv2.bitwise_and(image, image, mask=mask)\n",
        "\n",
        "    final_result = cv2.add(text_part, background_part)\n",
        "\n",
        "    return final_result\n",
        "\n",
        "# --- 3. à¹‚à¸„à¹‰à¸”à¸«à¸¥à¸±à¸à¸ªà¸³à¸«à¸£à¸±à¸šà¸—à¸³à¸‡à¸²à¸™à¹à¸šà¸šà¸­à¸±à¸•à¹‚à¸™à¸¡à¸±à¸•à¸´ ---\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    à¸§à¸™à¸¥à¸¹à¸›à¸—à¸³à¸‡à¸²à¸™à¸à¸±à¸šà¸—à¸¸à¸à¹„à¸Ÿà¸¥à¹Œà¹ƒà¸™à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œ\n",
        "    \"\"\"\n",
        "    # à¸ªà¸£à¹‰à¸²à¸‡à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œà¸›à¸¥à¸²à¸¢à¸—à¸²à¸‡à¸«à¸²à¸à¸¢à¸±à¸‡à¹„à¸¡à¹ˆà¸¡à¸µ\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "    # à¸„à¹‰à¸™à¸«à¸²à¹„à¸Ÿà¸¥à¹Œà¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”à¹ƒà¸™à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œà¸•à¹‰à¸™à¸—à¸²à¸‡\n",
        "    files_to_process = [f for f in os.listdir(INPUT_DIR) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "    if not files_to_process:\n",
        "        print(f\"à¹„à¸¡à¹ˆà¸à¸šà¹„à¸Ÿà¸¥à¹Œà¸ à¸²à¸à¹ƒà¸™à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œ: {INPUT_DIR}\")\n",
        "        return\n",
        "\n",
        "    print(f\"à¸à¸šà¹„à¸Ÿà¸¥à¹Œà¸—à¸±à¹‰à¸‡à¸«à¸¡à¸” {len(files_to_process)} à¹„à¸Ÿà¸¥à¹Œ à¹€à¸£à¸´à¹ˆà¸¡à¸”à¸³à¹€à¸™à¸´à¸™à¸à¸²à¸£...\")\n",
        "\n",
        "    # à¸§à¸™à¸¥à¸¹à¸›à¸—à¸³à¸‡à¸²à¸™à¸—à¸µà¸¥à¸°à¹„à¸Ÿà¸¥à¹Œ\n",
        "    for filename in files_to_process:\n",
        "        try:\n",
        "            filepath = os.path.join(INPUT_DIR, filename)\n",
        "\n",
        "            # à¸­à¹ˆà¸²à¸™à¹„à¸Ÿà¸¥à¹Œà¸ à¸²à¸\n",
        "            original_image = cv2.imread(filepath)\n",
        "\n",
        "            if original_image is None:\n",
        "                print(f\"  -> à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¸­à¹ˆà¸²à¸™à¹„à¸Ÿà¸¥à¹Œ: {filename}, à¸‚à¸­à¸‚à¹‰à¸²à¸¡à¹„à¸Ÿà¸¥à¹Œà¸™à¸µà¹‰\")\n",
        "                continue\n",
        "\n",
        "            # à¸ªà¹ˆà¸‡à¸ à¸²à¸à¹„à¸›à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¸”à¹‰à¸§à¸¢à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¸‚à¸­à¸‡à¸„à¸¸à¸“\n",
        "            processed_image = process_image_with_color_mask(original_image)\n",
        "\n",
        "            # à¸šà¸±à¸™à¸—à¸¶à¸à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œ\n",
        "            save_path = os.path.join(OUTPUT_DIR, filename)\n",
        "            cv2.imwrite(save_path, processed_image)\n",
        "\n",
        "            print(f\"  -> à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¹„à¸Ÿà¸¥à¹Œ '{filename}' à¹€à¸£à¸µà¸¢à¸šà¸£à¹‰à¸­à¸¢à¹à¸¥à¹‰à¸§\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  !!!! à¹€à¸à¸´à¸”à¸‚à¹‰à¸­à¸œà¸´à¸”à¸à¸¥à¸²à¸”à¸à¸±à¸šà¹„à¸Ÿà¸¥à¹Œ {filename}: {e}\")\n",
        "\n",
        "    print(\"-\" * 50)\n",
        "    print(\"à¸à¸²à¸£à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”à¹€à¸ªà¸£à¹‡à¸ˆà¸ªà¸´à¹‰à¸™!\")\n",
        "    print(f\"à¹„à¸Ÿà¸¥à¹Œà¸—à¸µà¹ˆà¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¹à¸¥à¹‰à¸§à¸–à¸¹à¸à¸šà¸±à¸™à¸—à¸¶à¸à¹„à¸§à¹‰à¸—à¸µà¹ˆ: {OUTPUT_DIR}\")\n",
        "\n",
        "\n",
        "# à¸ªà¸±à¹ˆà¸‡à¹ƒà¸«à¹‰à¹‚à¸›à¸£à¹à¸à¸£à¸¡à¹€à¸£à¸´à¹ˆà¸¡à¸—à¸³à¸‡à¸²à¸™\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0pRHBRmjpUV"
      },
      "source": [
        "# OCR AUTOMATE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jm13ks44kY2Y"
      },
      "source": [
        "**pytesseract**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Xzi8fHaGXvAX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2016e3c7-6bf9-4843-f09e-0021a29e9630"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- ğŸ” à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥ ---\n",
            "à¸à¸š ID à¹ƒà¸™ Ground Truth à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”: 120 ID | à¸à¸šà¹„à¸Ÿà¸¥à¹Œà¸£à¸¹à¸›à¸ à¸²à¸: 120 à¹„à¸Ÿà¸¥à¹Œ\n",
            "--- âœ… à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸ªà¸£à¹‡à¸ˆà¸ªà¸´à¹‰à¸™ ---\n",
            "\n",
            "--- ğŸš€ à¹€à¸£à¸´à¹ˆà¸¡à¸à¸²à¸£à¸ªà¸à¸±à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ˆà¸²à¸à¸£à¸¹à¸›à¸ à¸²à¸ 120 à¹„à¸Ÿà¸¥à¹Œ à¸”à¹‰à¸§à¸¢ Pytesseract (Pure OCR) ---\n",
            "  - à¸à¸³à¸¥à¸±à¸‡à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥ ID: 1 (KMITL-TEP PILOT-1.png)\n",
            "  - à¸à¸³à¸¥à¸±à¸‡à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥ ID: 2 (KMITL-TEP PILOT-2.png)\n",
            "  - à¸à¸³à¸¥à¸±à¸‡à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥ ID: 3 (KMITL-TEP PILOT-3.png)\n",
            "  - à¸à¸³à¸¥à¸±à¸‡à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥ ID: 4 (KMITL-TEP PILOT-4.png)\n",
            "  - à¸à¸³à¸¥à¸±à¸‡à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥ ID: 5 (KMITL-TEP PILOT-5.png)\n",
            "  - à¸à¸³à¸¥à¸±à¸‡à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥ ID: 6 (KMITL-TEP PILOT-6.png)\n",
            "  - à¸à¸³à¸¥à¸±à¸‡à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥ ID: 7 (KMITL-TEP PILOT-7.png)\n",
            "  - à¸à¸³à¸¥à¸±à¸‡à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥ ID: 8 (KMITL-TEP PILOT-8.png)\n",
            "  - à¸à¸³à¸¥à¸±à¸‡à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥ ID: 9 (KMITL-TEP PILOT-9.png)\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import jiwer\n",
        "import numpy as np\n",
        "import pytesseract\n",
        "\n",
        "# --- 1. à¸à¸³à¸«à¸™à¸”à¸„à¹ˆà¸²à¹à¸¥à¸° Path à¸—à¸µà¹ˆà¸ªà¸³à¸„à¸±à¸ ---\n",
        "MODEL_NAME = 'Pytesseract'\n",
        "APPROACH_NAME = 'Pure OCR'\n",
        "MASTER_OUTPUT_PATH = \"/content/drive/MyDrive/kmitl_dataset/final-excel/final_comparison_report.xlsx\"\n",
        "INDIVIDUAL_REPORT_PATH = f\"/content/drive/MyDrive/kmitl_dataset/final-excel/{MODEL_NAME.lower()}_{APPROACH_NAME.replace(' ', '_').lower()}_report.xlsx\"\n",
        "\n",
        "IMAGE_DIR = \"/content/drive/MyDrive/kmitl_dataset/final-preprecessed\"\n",
        "GROUND_TRUTH_PATH = \"/content/drive/MyDrive/kmitl_dataset/dataset/à¸„à¸°à¹à¸™à¸™ TEP (Pilot Study)_Total_IT.xlsx\"\n",
        "\n",
        "\n",
        "# --- 2. à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¸Šà¹ˆà¸§à¸¢à¹€à¸«à¸¥à¸·à¸­ ---\n",
        "\n",
        "def parse_score(text):\n",
        "    if not text: return None, None\n",
        "    match = re.search(r'([A-Z][1-2])\\s*\\((\\d+)\\)', text, re.IGNORECASE)\n",
        "    if match: return match.group(1).upper(), match.group(2)\n",
        "    return None, None\n",
        "\n",
        "def get_id_from_filename(filename):\n",
        "    match = re.search(r'(\\d+)\\.(png|jpg|jpeg)$', filename.lower())\n",
        "    if match: return int(match.group(1))\n",
        "    return None\n",
        "\n",
        "def normalize_text(text):\n",
        "    if text is None: return \"\"\n",
        "    text = str(text).strip().lower()\n",
        "    if text.endswith('.0'): text = text[:-2]\n",
        "    return text\n",
        "\n",
        "# â¬‡ï¸â¬‡ï¸ à¸™à¸³à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¸‚à¸­à¸‡à¸„à¸¸à¸“à¸¡à¸²à¸›à¸£à¸±à¸šà¸›à¸£à¸¸à¸‡à¹à¸¥à¸°à¹€à¸à¸´à¹ˆà¸¡à¸à¸²à¸£à¸ªà¸à¸±à¸” Total Score â¬‡ï¸â¬‡ï¸\n",
        "def parse_pytesseract_pure_ocr(text):\n",
        "    result = {}\n",
        "    valid_levels = {\"A1\", \"A2\", \"B1\", \"B2\", \"C1\", \"C2\"}\n",
        "    lines = [l.strip().replace('â€™', \"'\").replace('â€“', '-').replace('!', '1').replace('I', '1').replace('l', '1') for l in text.split('\\n') if l.strip()]\n",
        "    prev = \"\"\n",
        "\n",
        "    # à¹ƒà¸Šà¹‰ Regex à¸à¸±à¸šà¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”à¸à¹ˆà¸­à¸™à¹€à¸à¸·à¹ˆà¸­à¸„à¸§à¸²à¸¡à¹à¸¡à¹ˆà¸™à¸¢à¸³\n",
        "    full_text = \"\\n\".join(lines)\n",
        "\n",
        "    # Name\n",
        "    name_match = re.search(r'Name\\s*:?\\s*(.*?)\\s*Application', full_text, re.DOTALL | re.IGNORECASE)\n",
        "    if name_match: result[\"name\"] = name_match.group(1).strip()\n",
        "\n",
        "    # Application No.\n",
        "    app_no_match = re.search(r'Application\\s*No\\.?\\s*:*\\s*(\\S+)', full_text, re.IGNORECASE)\n",
        "    if app_no_match: result[\"application_no\"] = app_no_match.group(1).strip()\n",
        "\n",
        "    # Date\n",
        "    date_match = re.search(r'Date of test Administration\\s*:?\\s*(.+)', full_text, re.IGNORECASE)\n",
        "    if date_match:\n",
        "        # à¹€à¸­à¸²à¹à¸„à¹ˆà¸šà¸£à¸£à¸—à¸±à¸”à¹à¸£à¸à¸‚à¸­à¸‡à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œà¹€à¸œà¸·à¹ˆà¸­à¸¡à¸µà¸à¸²à¸£à¸‚à¸¶à¹‰à¸™à¸šà¸£à¸£à¸—à¸±à¸”à¹ƒà¸«à¸¡à¹ˆà¸œà¸´à¸”à¸à¸¥à¸²à¸”\n",
        "        result[\"test_date\"] = date_match.group(1).strip().split('\\n')[0]\n",
        "\n",
        "    # Skill Scores (à¹ƒà¸Šà¹‰ Logic à¹€à¸”à¸´à¸¡à¸‚à¸­à¸‡à¸„à¸¸à¸“à¹€à¸›à¹‡à¸™à¸à¸²à¸™ à¹à¸•à¹ˆà¸„à¹‰à¸™à¸«à¸²à¸ˆà¸²à¸ full_text)\n",
        "    score_pattern_full = r'([A-Z][1-2]\\s*\\(\\d+\\))'\n",
        "    for skill in [\"Grammar\", \"Reading\", \"Speaking\", \"Writing\"]:\n",
        "        m = re.search(rf\"{skill}.*?({score_pattern_full})\", full_text, re.IGNORECASE)\n",
        "        if m:\n",
        "            # à¹à¸¢à¸ Level à¹à¸¥à¸° Score à¸ˆà¸²à¸à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œ\n",
        "            score_text = m.group(1)\n",
        "            level, score = parse_score(score_text)\n",
        "            if level in valid_levels:\n",
        "                result[f\"{skill.lower()}_level\"] = level\n",
        "                result[f\"{skill.lower()}_score\"] = score\n",
        "\n",
        "    # Total Score (à¹€à¸à¸´à¹ˆà¸¡à¸•à¸£à¸£à¸à¸°à¹ƒà¸«à¸¡à¹ˆ)\n",
        "    all_scores_text = re.findall(score_pattern_full, full_text, re.IGNORECASE)\n",
        "    used_scores_text = []\n",
        "    for skill in [\"grammar\", \"reading\", \"speaking\", \"writing\"]:\n",
        "        if f\"{skill}_level\" in result:\n",
        "             used_scores_text.append(f\"{result[f'{skill}_level']} ({result[f'{skill}_score']})\")\n",
        "\n",
        "    remaining_scores = [s for s in all_scores_text if s not in used_scores_text]\n",
        "    if remaining_scores:\n",
        "        total_level, total_score = parse_score(remaining_scores[0])\n",
        "        result[\"total_level\"] = total_level\n",
        "        result[\"total_score\"] = total_score\n",
        "\n",
        "    return result\n",
        "\n",
        "# --- 3. à¹‚à¸«à¸¥à¸”à¹à¸¥à¸°à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸à¹ˆà¸­à¸™à¹€à¸£à¸´à¹ˆà¸¡ ---\n",
        "print(\"--- ğŸ” à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥ ---\")\n",
        "gt_df = pd.read_excel(GROUND_TRUTH_PATH)\n",
        "ground_truth_ids = set(gt_df['No.'].unique())\n",
        "image_files = sorted([f for f in os.listdir(IMAGE_DIR) if f.lower().endswith(('.png', '.jpg', '.jpeg'))], key=get_id_from_filename)\n",
        "print(f\"à¸à¸š ID à¹ƒà¸™ Ground Truth à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”: {len(ground_truth_ids)} ID | à¸à¸šà¹„à¸Ÿà¸¥à¹Œà¸£à¸¹à¸›à¸ à¸²à¸: {len(image_files)} à¹„à¸Ÿà¸¥à¹Œ\")\n",
        "print(\"--- âœ… à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸ªà¸£à¹‡à¸ˆà¸ªà¸´à¹‰à¸™ ---\\n\")\n",
        "\n",
        "# --- 4. à¹€à¸£à¸´à¹ˆà¸¡à¸à¸£à¸°à¸šà¸§à¸™à¸à¸²à¸£à¸ªà¸à¸±à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ ---\n",
        "results_list = []\n",
        "print(f\"--- ğŸš€ à¹€à¸£à¸´à¹ˆà¸¡à¸à¸²à¸£à¸ªà¸à¸±à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ˆà¸²à¸à¸£à¸¹à¸›à¸ à¸²à¸ {len(image_files)} à¹„à¸Ÿà¸¥à¹Œ à¸”à¹‰à¸§à¸¢ {MODEL_NAME} ({APPROACH_NAME}) ---\")\n",
        "for filename in image_files:\n",
        "    image_id = get_id_from_filename(filename)\n",
        "    if image_id is None: continue\n",
        "    gt_row = gt_df[gt_df['No.'] == image_id]\n",
        "    if gt_row.empty: continue\n",
        "\n",
        "    print(f\"  - à¸à¸³à¸¥à¸±à¸‡à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥ ID: {image_id} ({filename})\")\n",
        "    image_path = os.path.join(IMAGE_DIR, filename)\n",
        "    img = cv2.imread(image_path)\n",
        "\n",
        "    custom_config = r'--oem 3 --psm 6'\n",
        "    text = pytesseract.image_to_string(img, config=custom_config)\n",
        "\n",
        "    extracted_raw = parse_pytesseract_pure_ocr(text)\n",
        "\n",
        "    # à¸™à¸³à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¸ªà¸à¸±à¸”à¹„à¸”à¹‰à¸¡à¸²à¸ˆà¸±à¸”à¸¥à¸‡ Dictionary\n",
        "    results_list.append({\n",
        "        \"No.\": image_id,\n",
        "        \"Application No. (GT)\": gt_row.iloc[0][\"Application No.\"], \"Application No. (Pred)\": extracted_raw.get('application_no'),\n",
        "        \"Name (GT)\": gt_row.iloc[0][\"Name\"], \"Name (Pred)\": extracted_raw.get('name'),\n",
        "        \"Test Date (GT)\": gt_row.iloc[0][\"Test Date\"], \"Test Date (Pred)\": extracted_raw.get('test_date'),\n",
        "        \"Grammar_Level (GT)\": gt_row.iloc[0][\"Grammar_Level\"], \"Grammar_Level (Pred)\": extracted_raw.get('grammar_level'),\n",
        "        \"Grammar_Score (GT)\": gt_row.iloc[0][\"Grammar_Score\"], \"Grammar_Score (Pred)\": extracted_raw.get('grammar_score'),\n",
        "        \"Reading_Level (GT)\": gt_row.iloc[0][\"Reading_Level\"], \"Reading_Level (Pred)\": extracted_raw.get('reading_level'),\n",
        "        \"Reading_Score (GT)\": gt_row.iloc[0][\"Reading_Score\"], \"Reading_Score (Pred)\": extracted_raw.get('reading_score'),\n",
        "        \"Speaking_Level (GT)\": gt_row.iloc[0][\"Speaking_Level\"], \"Speaking_Level (Pred)\": extracted_raw.get('speaking_level'),\n",
        "        \"Speaking_Score (GT)\": gt_row.iloc[0][\"Speaking_Score\"], \"Speaking_Score (Pred)\": extracted_raw.get('speaking_score'),\n",
        "        \"Writing_Level (GT)\": gt_row.iloc[0][\"Writing_Level\"], \"Writing_Level (Pred)\": extracted_raw.get('writing_level'),\n",
        "        \"Writing_Score (GT)\": gt_row.iloc[0][\"Writing_Score\"], \"Writing_Score (Pred)\": extracted_raw.get('writing_score'),\n",
        "        \"Total_Level (GT)\": gt_row.iloc[0][\"Total_Level\"], \"Total_Level (Pred)\": extracted_raw.get('total_level'),\n",
        "        \"Total_Score (GT)\": gt_row.iloc[0][\"Total_Score\"], \"Total_Score (Pred)\": extracted_raw.get('total_score'),\n",
        "    })\n",
        "\n",
        "ocr_results_df = pd.DataFrame(results_list)\n",
        "print(\"--- âœ… à¸ªà¸à¸±à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸ªà¸£à¹‡à¸ˆà¸ªà¸´à¹‰à¸™ ---\\n\")\n",
        "\n",
        "\n",
        "# --- 5. à¹€à¸£à¸´à¹ˆà¸¡à¸à¸£à¸°à¸šà¸§à¸™à¸à¸²à¸£à¸›à¸£à¸°à¹€à¸¡à¸´à¸™à¸œà¸¥ (Evaluation) ---\n",
        "# ... (à¹‚à¸„à¹‰à¸”à¸ªà¹ˆà¸§à¸™à¸™à¸µà¹‰à¹€à¸«à¸¡à¸·à¸­à¸™à¸à¸±à¸šà¹€à¸§à¸­à¸£à¹Œà¸Šà¸±à¸™à¸à¹ˆà¸­à¸™à¸«à¸™à¹‰à¸²à¸—à¸¸à¸à¸›à¸£à¸°à¸à¸²à¸£) ...\n",
        "print(\"--- ğŸ“Š à¹€à¸£à¸´à¹ˆà¸¡à¸à¸²à¸£à¸›à¸£à¸°à¹€à¸¡à¸´à¸™à¸œà¸¥ ---\")\n",
        "eval_df = ocr_results_df.fillna('')\n",
        "fields_to_evaluate = {\n",
        "    'Name': ('Name (GT)', 'Name (Pred)'), 'Application No.': ('Application No. (GT)', 'Application No. (Pred)'),\n",
        "    'Test Date': ('Test Date (GT)', 'Test Date (Pred)'), 'Grammar_Level': ('Grammar_Level (GT)', 'Grammar_Level (Pred)'),\n",
        "    'Grammar_Score': ('Grammar_Score (GT)', 'Grammar_Score (Pred)'), 'Reading_Level': ('Reading_Level (GT)', 'Reading_Level (Pred)'),\n",
        "    'Reading_Score': ('Reading_Score (GT)', 'Reading_Score (Pred)'), 'Speaking_Level': ('Speaking_Level (GT)', 'Speaking_Level (Pred)'),\n",
        "    'Speaking_Score': ('Speaking_Score (GT)', 'Speaking_Score (Pred)'), 'Writing_Level': ('Writing_Level (GT)', 'Writing_Level (Pred)'),\n",
        "    'Writing_Score': ('Writing_Score (GT)', 'Writing_Score (Pred)'), 'Total_Level': ('Total_Level (GT)', 'Total_Level (Pred)'),\n",
        "    'Total_Score': ('Total_Score (GT)', 'Total_Score (Pred)'),\n",
        "}\n",
        "evaluation_summary_list = []\n",
        "for field, (gt_col, pred_col) in fields_to_evaluate.items():\n",
        "    ground_truth = [normalize_text(t) for t in eval_df[gt_col]]\n",
        "    prediction = [normalize_text(t) for t in eval_df[pred_col]]\n",
        "    accuracy = np.mean([1 if gt == pred else 0 for gt, pred in zip(ground_truth, prediction)]) * 100\n",
        "    error_metrics = jiwer.compute_measures(ground_truth, prediction)\n",
        "    wer = error_metrics.get('wer', 0) * 100\n",
        "    cer = error_metrics.get('cer', 0) * 100\n",
        "    H = error_metrics.get('hits', 0); I = error_metrics.get('insertions', 0); D = error_metrics.get('deletions', 0); S = error_metrics.get('substitutions', 0)\n",
        "    precision = H / (H + I + S) if (H + I + S) > 0 else 0\n",
        "    recall = H / (H + D + S) if (H + D + S) > 0 else 0\n",
        "    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    evaluation_summary_list.append({\n",
        "        'Field': field, 'Accuracy (%)': round(accuracy, 2), 'WER (%)': round(wer, 2),\n",
        "        'CER (%)': round(cer, 2), 'F1-score (%)': round(f1_score * 100, 2)\n",
        "    })\n",
        "eval_summary_df = pd.DataFrame(evaluation_summary_list)\n",
        "eval_summary_df.insert(0, 'Approach', APPROACH_NAME)\n",
        "eval_summary_df.insert(0, 'Model', MODEL_NAME)\n",
        "print(eval_summary_df.to_string(index=False))\n",
        "print(\"--- âœ… à¸›à¸£à¸°à¹€à¸¡à¸´à¸™à¸œà¸¥à¹€à¸ªà¸£à¹‡à¸ˆà¸ªà¸´à¹‰à¸™ ---\\n\")\n",
        "\n",
        "\n",
        "# --- 6. à¸šà¸±à¸™à¸—à¸¶à¸à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œ (à¸—à¸³ 2 à¸­à¸¢à¹ˆà¸²à¸‡) ---\n",
        "# --- 6.A: à¸šà¸±à¸™à¸—à¸¶à¸à¸œà¸¥à¸à¸²à¸£à¸—à¸”à¸¥à¸­à¸‡à¸™à¸µà¹‰à¹à¸¢à¸à¹„à¸Ÿà¸¥à¹Œ ---\n",
        "print(f\"--- ğŸ’¾ à¸à¸³à¸¥à¸±à¸‡à¸šà¸±à¸™à¸—à¸¶à¸à¸£à¸²à¸¢à¸‡à¸²à¸™à¹€à¸‰à¸à¸²à¸°à¸‚à¸­à¸‡ {MODEL_NAME} ({APPROACH_NAME}) à¸¥à¸‡à¹ƒà¸™à¹„à¸Ÿà¸¥à¹Œ: {INDIVIDUAL_REPORT_PATH} ---\")\n",
        "with pd.ExcelWriter(INDIVIDUAL_REPORT_PATH, engine='openpyxl') as writer:\n",
        "    ocr_results_df.to_excel(writer, sheet_name='Detailed_Results', index=False)\n",
        "    eval_summary_df.drop(columns=['Model', 'Approach']).to_excel(writer, sheet_name='Evaluation_Summary', index=False)\n",
        "print(f\"ğŸ‰ à¸šà¸±à¸™à¸—à¸¶à¸à¹„à¸Ÿà¸¥à¹Œ {INDIVIDUAL_REPORT_PATH} à¸ªà¸³à¹€à¸£à¹‡à¸ˆ!\")\n",
        "\n",
        "# --- 6.B: à¸­à¸±à¸›à¹€à¸”à¸•à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸¥à¸‡ Master File ---\n",
        "print(f\"--- ğŸ’¾ à¸à¸³à¸¥à¸±à¸‡à¸­à¸±à¸›à¹€à¸”à¸•à¹„à¸Ÿà¸¥à¹Œ Master Report: {MASTER_OUTPUT_PATH} ---\")\n",
        "SHEET_NAME = 'Master_Evaluation'\n",
        "try:\n",
        "    with pd.ExcelFile(MASTER_OUTPUT_PATH) as xls:\n",
        "        master_df = pd.read_excel(xls, sheet_name=SHEET_NAME)\n",
        "        master_df = master_df[(master_df['Model'] != MODEL_NAME) | (master_df['Approach'] != APPROACH_NAME)]\n",
        "    combined_df = pd.concat([master_df, eval_summary_df], ignore_index=True)\n",
        "except FileNotFoundError:\n",
        "    print(f\"à¹„à¸¡à¹ˆà¸à¸šà¹„à¸Ÿà¸¥à¹Œ Master à¹€à¸”à¸´à¸¡, à¸à¸³à¸¥à¸±à¸‡à¸ªà¸£à¹‰à¸²à¸‡à¹„à¸Ÿà¸¥à¹Œà¹ƒà¸«à¸¡à¹ˆ...\")\n",
        "    combined_df = eval_summary_df\n",
        "with pd.ExcelWriter(MASTER_OUTPUT_PATH, engine='openpyxl') as writer:\n",
        "    combined_df.to_excel(writer, sheet_name=SHEET_NAME, index=False)\n",
        "print(f\"ğŸ‰ à¸­à¸±à¸›à¹€à¸”à¸•à¹„à¸Ÿà¸¥à¹Œ Master Report à¸ªà¸³à¹€à¸£à¹‡à¸ˆ!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdtOu7aSW58l"
      },
      "source": [
        "**à¸›à¸£à¸±à¸šà¹ƒà¸«à¹‰à¸¡à¸±à¸™à¸­à¹ˆà¸²à¸™à¸„à¹ˆà¸²à¹„à¸”à¹‰à¹à¸¡à¹ˆà¸™à¹†à¸à¹ˆà¸­à¸™**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4oFkaAIiTuqf"
      },
      "outputs": [],
      "source": [
        "# 1. IMPORT LIBRARIES\n",
        "import cv2\n",
        "import pytesseract\n",
        "import re\n",
        "import os\n",
        "import pandas as pd\n",
        "import json # à¹€à¸à¸´à¹ˆà¸¡à¹€à¸‚à¹‰à¸²à¸¡à¸²à¹€à¸à¸·à¹ˆà¸­à¸à¸´à¸¡à¸à¹Œ Dictionary à¹ƒà¸«à¹‰à¸­à¹ˆà¸²à¸™à¸‡à¹ˆà¸²à¸¢\n",
        "\n",
        "# 2. à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¸ªà¸à¸±à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸‚à¸­à¸‡à¸„à¸¸à¸“ (à¹„à¸¡à¹ˆà¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™à¹à¸›à¸¥à¸‡)\n",
        "def extract_ktep_key_value(text):\n",
        "    result = {}\n",
        "    valid_levels = {\"A1\", \"A2\", \"B1\", \"B2\", \"C1\", \"C2\"}\n",
        "    lines = [l.strip().replace('â€™', \"'\").replace('â€“', '-').replace('!', '1').replace('I', '1') for l in text.split('\\n') if l.strip()]\n",
        "    prev = \"\"\n",
        "    # à¹€à¸à¸´à¹ˆà¸¡ print(lines) à¹€à¸à¸·à¹ˆà¸­à¸Šà¹ˆà¸§à¸¢à¸”à¸µà¸šà¸±à¸ à¸§à¹ˆà¸² Tesseract à¸­à¹ˆà¸²à¸™à¸­à¸°à¹„à¸£à¸­à¸­à¸à¸¡à¸²à¸šà¹‰à¸²à¸‡\n",
        "    print(\"--- à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸—à¸µà¹ˆ Tesseract à¸­à¹ˆà¸²à¸™à¹„à¸”à¹‰ (à¹à¸šà¹ˆà¸‡à¸šà¸£à¸£à¸—à¸±à¸”à¹à¸¥à¹‰à¸§) ---\")\n",
        "    print(lines)\n",
        "    print(\"-------------------------------------------------\")\n",
        "\n",
        "\n",
        "    for idx, line in enumerate(lines):\n",
        "        # Name & Application No.\n",
        "        if \"Name:\" in line and \"Application No.\" in line:\n",
        "            name = re.search(r'Name:\\s*(.*?)\\s*Application No', line, re.IGNORECASE)\n",
        "            app_no = re.search(r'Application No\\.?:\\s*(\\S+)', line, re.IGNORECASE)\n",
        "            if name:\n",
        "                result[\"name\"] = name.group(1).strip()\n",
        "            if app_no:\n",
        "                result[\"application_no\"] = app_no.group(1).strip()\n",
        "\n",
        "        # fallback à¹à¸¢à¸à¸šà¸£à¸£à¸—à¸±à¸”\n",
        "        if \"Name:\" in line and idx + 1 < len(lines) and \"name\" not in result:\n",
        "            result[\"name\"] = lines[idx + 1].strip()\n",
        "        if \"Application No.\" in line and idx + 1 < len(lines) and \"application_no\" not in result:\n",
        "            app_no_fallback = re.search(r'\\d{3}', lines[idx + 1])\n",
        "            if app_no_fallback:\n",
        "                result[\"application_no\"] = app_no_fallback.group()\n",
        "        # Date\n",
        "        if \"Date of test Administration\" in line:\n",
        "            date_match = re.search(r'Date of test Administration:\\s*(.+)', line, re.IGNORECASE)\n",
        "            if date_match:\n",
        "                result[\"test_date\"] = date_match.group(1).strip()\n",
        "        elif \"June\" in line or \"May\" in line or \"July\" in line:\n",
        "            date_match = re.search(r\"(June|July|May)\\s+\\d{1,2},\\s+\\d{4}\", line)\n",
        "            if date_match and \"test_date\" not in result:\n",
        "                result[\"test_date\"] = date_match.group()\n",
        "\n",
        "        # Skills: à¸šà¸£à¸£à¸—à¸±à¸”à¹€à¸”à¸µà¸¢à¸§\n",
        "        for skill in [\"Grammar\", \"Reading\", \"Speaking\", \"Writing\"]:\n",
        "            if skill.lower() + \"_score\" in result:\n",
        "                continue\n",
        "            m = re.search(rf\"{skill}.*?([A-Z][12])\\s*\\(?(\\d+)\\)?\", line)\n",
        "            if m and m.group(1) in valid_levels:\n",
        "                result[f\"{skill.lower()}_level\"] = m.group(1)\n",
        "                result[f\"{skill.lower()}_score\"] = m.group(2)\n",
        "\n",
        "        # Skills: fallback à¸šà¸£à¸£à¸—à¸±à¸”à¸à¹ˆà¸­à¸™\n",
        "        for skill in [\"Grammar\", \"Reading\", \"Speaking\", \"Writing\"]:\n",
        "            if skill.lower() + \"_score\" not in result and skill.lower() in prev.lower():\n",
        "                m = re.search(r'([A-Z][12])\\s*\\(?(\\d+)\\)?', line)\n",
        "                if m and m.group(1) in valid_levels:\n",
        "                    result[f\"{skill.lower()}_level\"] = m.group(1)\n",
        "                    result[f\"{skill.lower()}_score\"] = m.group(2)\n",
        "\n",
        "        prev = line\n",
        "\n",
        "    return result\n",
        "\n",
        "# 3. MAIN SCRIPT FOR SINGLE IMAGE TEST\n",
        "if __name__ == \"__main__\":\n",
        "    # --- âš™ï¸ à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸² ---\n",
        "    # â—ï¸â—ï¸ à¹à¸à¹‰à¹„à¸‚ Path à¸£à¸¹à¸›à¸ à¸²à¸à¸‚à¸­à¸‡à¸„à¸¸à¸“à¸•à¸£à¸‡à¸™à¸µà¹‰ â—ï¸â—ï¸\n",
        "    single_image_path = \"/content/drive/MyDrive/kmitl_dataset/removebg_images/KMITL-TEP PILOT-1.png\"\n",
        "\n",
        "    # --- à¹‚à¸«à¸¥à¸”à¸£à¸¹à¸›à¸ à¸²à¸ ---\n",
        "    print(f\"à¸à¸³à¸¥à¸±à¸‡à¹‚à¸«à¸¥à¸”à¸£à¸¹à¸›à¸ à¸²à¸à¸ˆà¸²à¸: {single_image_path}\")\n",
        "    img = cv2.imread(single_image_path)\n",
        "\n",
        "    if img is None:\n",
        "        print(f\"âŒ à¹€à¸à¸´à¸”à¸‚à¹‰à¸­à¸œà¸´à¸”à¸à¸¥à¸²à¸”: à¹„à¸¡à¹ˆà¸à¸šà¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¹‚à¸«à¸¥à¸”à¸£à¸¹à¸›à¸ à¸²à¸à¹„à¸”à¹‰à¸ˆà¸²à¸ Path à¸—à¸µà¹ˆà¸£à¸°à¸šà¸¸\")\n",
        "    else:\n",
        "        # --- à¸£à¸±à¸™ Tesseract OCR ---\n",
        "        print(\"à¸à¸³à¸¥à¸±à¸‡à¸£à¸±à¸™ Tesseract OCR...\")\n",
        "        custom_config = r'--oem 3 --psm 6'\n",
        "        text = pytesseract.image_to_string(img, config=custom_config)\n",
        "\n",
        "        # --- à¸ªà¸à¸±à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸”à¹‰à¸§à¸¢à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¸‚à¸­à¸‡à¸„à¸¸à¸“ ---\n",
        "        print(\"à¸à¸³à¸¥à¸±à¸‡à¸ªà¸à¸±à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ Key-Value...\")\n",
        "        extracted_data = extract_ktep_key_value(text)\n",
        "\n",
        "        # --- à¹à¸ªà¸”à¸‡à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œà¸ªà¸¸à¸”à¸—à¹‰à¸²à¸¢ ---\n",
        "        print(\"\\n--- à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œà¸—à¸µà¹ˆà¸ªà¸à¸±à¸”à¹„à¸”à¹‰ ---\")\n",
        "        # à¹ƒà¸Šà¹‰ json.dumps à¹€à¸à¸·à¹ˆà¸­à¸à¸´à¸¡à¸à¹Œ Dictionary à¸­à¸­à¸à¸¡à¸²à¸ªà¸§à¸¢à¸‡à¸²à¸¡ à¸­à¹ˆà¸²à¸™à¸‡à¹ˆà¸²à¸¢\n",
        "        print(json.dumps(extracted_data, indent=4, ensure_ascii=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoAefk8okiUK"
      },
      "source": [
        "**Easy OCR**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KHSF-OtCkliu"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import jiwer\n",
        "import numpy as np\n",
        "import easyocr\n",
        "\n",
        "# --- 1. à¸à¸³à¸«à¸™à¸”à¸„à¹ˆà¸²à¹à¸¥à¸° Path à¸—à¸µà¹ˆà¸ªà¸³à¸„à¸±à¸ ---\n",
        "MODEL_NAME = 'EasyOCR'\n",
        "APPROACH_NAME = 'Pure OCR'\n",
        "MASTER_OUTPUT_PATH = \"/content/drive/MyDrive/kmitl_dataset/final-excel/final_comparison_report.xlsx\"\n",
        "INDIVIDUAL_REPORT_PATH = f\"/content/drive/MyDrive/kmitl_dataset/final-excel/{MODEL_NAME.lower()}_{APPROACH_NAME.replace(' ', '_').lower()}_report.xlsx\"\n",
        "\n",
        "IMAGE_DIR = \"/content/drive/MyDrive/kmitl_dataset/final-preprecessed\"\n",
        "GROUND_TRUTH_PATH = \"/content/drive/MyDrive/kmitl_dataset/dataset/à¸„à¸°à¹à¸™à¸™ TEP (Pilot Study)_Total_IT.xlsx\"\n",
        "\n",
        "# --- 2. à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¸Šà¹ˆà¸§à¸¢à¹€à¸«à¸¥à¸·à¸­ ---\n",
        "\n",
        "def parse_score(text):\n",
        "    if not text: return None, None\n",
        "    match = re.search(r'([A-Z][1-2])\\s*\\((\\d+)\\)', text, re.IGNORECASE)\n",
        "    if match: return match.group(1).upper(), match.group(2)\n",
        "    return None, None\n",
        "\n",
        "def get_id_from_filename(filename):\n",
        "    match = re.search(r'(\\d+)\\.(png|jpg|jpeg)$', filename.lower())\n",
        "    if match: return int(match.group(1))\n",
        "    return None\n",
        "\n",
        "def normalize_text(text):\n",
        "    if text is None: return \"\"\n",
        "    text = str(text).strip().lower()\n",
        "    if text.endswith('.0'): text = text[:-2]\n",
        "    return text\n",
        "\n",
        "def parse_pure_ocr_text(text):\n",
        "    \"\"\"\n",
        "    Parser à¸—à¸µà¹ˆà¸—à¸³à¸‡à¸²à¸™à¸à¸±à¸šà¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸¢à¸²à¸§à¹† à¸—à¸µà¹ˆà¹„à¸”à¹‰à¸ˆà¸²à¸ OCR (à¹ƒà¸Šà¹‰à¹„à¸”à¹‰à¸—à¸±à¹‰à¸‡ Pytesseract à¹à¸¥à¸° EasyOCR)\n",
        "    \"\"\"\n",
        "    result = {}\n",
        "    clean_text = text.replace('â€™', \"'\").replace('â€“', '-').replace('!', '1').replace('l', '1').replace('I', '1')\n",
        "\n",
        "    # Name\n",
        "    name_match = re.search(r'Name\\s*:?\\s*(.*?)\\s*Application', clean_text, re.DOTALL | re.IGNORECASE)\n",
        "    if name_match: result[\"name\"] = name_match.group(1).strip()\n",
        "\n",
        "    # Application No.\n",
        "    app_no_match = re.search(r'Application\\s*No\\.?\\s*:*\\s*(\\S+)', clean_text, re.IGNORECASE)\n",
        "    if app_no_match: result[\"application_no\"] = app_no_match.group(1).strip()\n",
        "\n",
        "    # Date\n",
        "    date_match = re.search(r'Date of test Administration\\s*:?\\s*(.+)', clean_text, re.IGNORECASE)\n",
        "    if date_match:\n",
        "        result[\"test_date\"] = date_match.group(1).strip().split('\\n')[0]\n",
        "\n",
        "    # Skill Scores\n",
        "    score_pattern_full = r'([A-B][1-2]\\s*\\(\\d+\\))'\n",
        "    for skill in [\"Grammar\", \"Reading\", \"Speaking\", \"Writing\"]:\n",
        "        m = re.search(rf\"{skill}.*?({score_pattern_full})\", clean_text, re.IGNORECASE)\n",
        "        if m:\n",
        "            score_text = m.group(1)\n",
        "            level, score = parse_score(score_text)\n",
        "            if level:\n",
        "                result[f\"{skill.lower()}_level\"] = level\n",
        "                result[f\"{skill.lower()}_score\"] = score\n",
        "\n",
        "    # Total Score\n",
        "    all_scores_text = re.findall(score_pattern_full, clean_text, re.IGNORECASE)\n",
        "    used_scores_text = []\n",
        "    for skill in [\"grammar\", \"reading\", \"speaking\", \"writing\"]:\n",
        "        if f\"{skill}_level\" in result:\n",
        "             used_scores_text.append(f\"{result[f'{skill}_level']} ({result[f'{skill}_score']})\")\n",
        "    remaining_scores = [s for s in all_scores_text if s not in used_scores_text]\n",
        "    if remaining_scores:\n",
        "        total_level, total_score = parse_score(remaining_scores[0])\n",
        "        result[\"total_level\"] = total_level\n",
        "        result[\"total_score\"] = total_score\n",
        "\n",
        "    return result\n",
        "\n",
        "# --- 3. à¹‚à¸«à¸¥à¸”à¹à¸¥à¸°à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸à¹ˆà¸­à¸™à¹€à¸£à¸´à¹ˆà¸¡ ---\n",
        "print(\"--- ğŸ” à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥ ---\")\n",
        "gt_df = pd.read_excel(GROUND_TRUTH_PATH)\n",
        "ground_truth_ids = set(gt_df['No.'].unique())\n",
        "image_files = sorted([f for f in os.listdir(IMAGE_DIR) if f.lower().endswith(('.png', '.jpg', '.jpeg'))], key=get_id_from_filename)\n",
        "print(f\"à¸à¸š ID à¹ƒà¸™ Ground Truth à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”: {len(ground_truth_ids)} ID | à¸à¸šà¹„à¸Ÿà¸¥à¹Œà¸£à¸¹à¸›à¸ à¸²à¸: {len(image_files)} à¹„à¸Ÿà¸¥à¹Œ\")\n",
        "print(\"--- âœ… à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸ªà¸£à¹‡à¸ˆà¸ªà¸´à¹‰à¸™ ---\\n\")\n",
        "\n",
        "# --- 4. à¹€à¸£à¸´à¹ˆà¸¡à¸à¸£à¸°à¸šà¸§à¸™à¸à¸²à¸£à¸ªà¸à¸±à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ ---\n",
        "# â¬‡ï¸â¬‡ï¸ à¹‚à¸«à¸¥à¸”à¹‚à¸¡à¹€à¸”à¸¥ EasyOCR (à¸—à¸³à¸„à¸£à¸±à¹‰à¸‡à¹€à¸”à¸µà¸¢à¸§) â¬‡ï¸â¬‡ï¸\n",
        "print(\"à¸à¸³à¸¥à¸±à¸‡à¹‚à¸«à¸¥à¸”à¹‚à¸¡à¹€à¸”à¸¥ EasyOCR...\")\n",
        "reader = easyocr.Reader(['en'])\n",
        "print(\"à¹‚à¸«à¸¥à¸”à¹‚à¸¡à¹€à¸”à¸¥à¸ªà¸³à¹€à¸£à¹‡à¸ˆ!\")\n",
        "\n",
        "results_list = []\n",
        "print(f\"--- ğŸš€ à¹€à¸£à¸´à¹ˆà¸¡à¸à¸²à¸£à¸ªà¸à¸±à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ˆà¸²à¸à¸£à¸¹à¸›à¸ à¸²à¸ {len(image_files)} à¹„à¸Ÿà¸¥à¹Œ à¸”à¹‰à¸§à¸¢ {MODEL_NAME} ({APPROACH_NAME}) ---\")\n",
        "for filename in image_files:\n",
        "    image_id = get_id_from_filename(filename)\n",
        "    if image_id is None: continue\n",
        "    gt_row = gt_df[gt_df['No.'] == image_id]\n",
        "    if gt_row.empty: continue\n",
        "\n",
        "    print(f\"  - à¸à¸³à¸¥à¸±à¸‡à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥ ID: {image_id} ({filename})\")\n",
        "    image_path = os.path.join(IMAGE_DIR, filename)\n",
        "\n",
        "    # â¬‡ï¸â¬‡ï¸ à¹ƒà¸Šà¹‰ EasyOCR à¸­à¹ˆà¸²à¸™à¸ à¸²à¸à¸—à¸±à¹‰à¸‡à¹ƒà¸š â¬‡ï¸â¬‡ï¸\n",
        "    # paragraph=True à¸Šà¹ˆà¸§à¸¢à¸£à¸§à¸¡à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸—à¸µà¹ˆà¸­à¸¢à¸¹à¹ˆà¹ƒà¸à¸¥à¹‰à¸à¸±à¸™ à¸—à¸³à¹ƒà¸«à¹‰à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œà¸ªà¸°à¸­à¸²à¸”à¸‚à¸¶à¹‰à¸™\n",
        "    raw_text_list = reader.readtext(image_path, detail=0, paragraph=True)\n",
        "    text = \"\\n\".join(raw_text_list)\n",
        "\n",
        "    # à¹€à¸£à¸µà¸¢à¸à¹ƒà¸Šà¹‰ Parser à¸•à¸±à¸§à¹€à¸”à¸µà¸¢à¸§à¸à¸±à¸™à¸à¸±à¸šà¸‚à¸­à¸‡ Pytesseract\n",
        "    extracted_raw = parse_pure_ocr_text(text)\n",
        "\n",
        "    # (à¸ªà¹ˆà¸§à¸™à¸—à¸µà¹ˆà¹€à¸«à¸¥à¸·à¸­à¹€à¸«à¸¡à¸·à¸­à¸™à¹€à¸”à¸´à¸¡à¸—à¸¸à¸à¸›à¸£à¸°à¸à¸²à¸£)\n",
        "    # ...\n",
        "    # à¸™à¸³à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¸ªà¸à¸±à¸”à¹„à¸”à¹‰à¸¡à¸²à¸ˆà¸±à¸”à¸¥à¸‡ Dictionary\n",
        "    results_list.append({\n",
        "        \"No.\": image_id,\n",
        "        \"Application No. (GT)\": gt_row.iloc[0][\"Application No.\"], \"Application No. (Pred)\": extracted_raw.get('application_no'),\n",
        "        \"Name (GT)\": gt_row.iloc[0][\"Name\"], \"Name (Pred)\": extracted_raw.get('name'),\n",
        "        \"Test Date (GT)\": gt_row.iloc[0][\"Test Date\"], \"Test Date (Pred)\": extracted_raw.get('test_date'),\n",
        "        \"Grammar_Level (GT)\": gt_row.iloc[0][\"Grammar_Level\"], \"Grammar_Level (Pred)\": extracted_raw.get('grammar_level'),\n",
        "        \"Grammar_Score (GT)\": gt_row.iloc[0][\"Grammar_Score\"], \"Grammar_Score (Pred)\": extracted_raw.get('grammar_score'),\n",
        "        \"Reading_Level (GT)\": gt_row.iloc[0][\"Reading_Level\"], \"Reading_Level (Pred)\": extracted_raw.get('reading_level'),\n",
        "        \"Reading_Score (GT)\": gt_row.iloc[0][\"Reading_Score\"], \"Reading_Score (Pred)\": extracted_raw.get('reading_score'),\n",
        "        \"Speaking_Level (GT)\": gt_row.iloc[0][\"Speaking_Level\"], \"Speaking_Level (Pred)\": extracted_raw.get('speaking_level'),\n",
        "        \"Speaking_Score (GT)\": gt_row.iloc[0][\"Speaking_Score\"], \"Speaking_Score (Pred)\": extracted_raw.get('speaking_score'),\n",
        "        \"Writing_Level (GT)\": gt_row.iloc[0][\"Writing_Level\"], \"Writing_Level (Pred)\": extracted_raw.get('writing_level'),\n",
        "        \"Writing_Score (GT)\": gt_row.iloc[0][\"Writing_Score\"], \"Writing_Score (Pred)\": extracted_raw.get('writing_score'),\n",
        "        \"Total_Level (GT)\": gt_row.iloc[0][\"Total_Level\"], \"Total_Level (Pred)\": extracted_raw.get('total_level'),\n",
        "        \"Total_Score (GT)\": gt_row.iloc[0][\"Total_Score\"], \"Total_Score (Pred)\": extracted_raw.get('total_score'),\n",
        "    })\n",
        "\n",
        "ocr_results_df = pd.DataFrame(results_list)\n",
        "print(\"--- âœ… à¸ªà¸à¸±à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸ªà¸£à¹‡à¸ˆà¸ªà¸´à¹‰à¸™ ---\\n\")\n",
        "\n",
        "\n",
        "# --- 5. à¹€à¸£à¸´à¹ˆà¸¡à¸à¸£à¸°à¸šà¸§à¸™à¸à¸²à¸£à¸›à¸£à¸°à¹€à¸¡à¸´à¸™à¸œà¸¥ (Evaluation) ---\n",
        "# ... (à¹‚à¸„à¹‰à¸”à¸ªà¹ˆà¸§à¸™à¸™à¸µà¹‰à¹€à¸«à¸¡à¸·à¸­à¸™à¸à¸±à¸šà¹€à¸§à¸­à¸£à¹Œà¸Šà¸±à¸™à¸à¹ˆà¸­à¸™à¸«à¸™à¹‰à¸²à¸—à¸¸à¸à¸›à¸£à¸°à¸à¸²à¸£) ...\n",
        "print(\"--- ğŸ“Š à¹€à¸£à¸´à¹ˆà¸¡à¸à¸²à¸£à¸›à¸£à¸°à¹€à¸¡à¸´à¸™à¸œà¸¥ ---\")\n",
        "eval_df = ocr_results_df.fillna('')\n",
        "fields_to_evaluate = {\n",
        "    'Name': ('Name (GT)', 'Name (Pred)'), 'Application No.': ('Application No. (GT)', 'Application No. (Pred)'),\n",
        "    'Test Date': ('Test Date (GT)', 'Test Date (Pred)'), 'Grammar_Level': ('Grammar_Level (GT)', 'Grammar_Level (Pred)'),\n",
        "    'Grammar_Score': ('Grammar_Score (GT)', 'Grammar_Score (Pred)'), 'Reading_Level': ('Reading_Level (GT)', 'Reading_Level (Pred)'),\n",
        "    'Reading_Score': ('Reading_Score (GT)', 'Reading_Score (Pred)'), 'Speaking_Level': ('Speaking_Level (GT)', 'Speaking_Level (Pred)'),\n",
        "    'Speaking_Score': ('Speaking_Score (GT)', 'Speaking_Score (Pred)'), 'Writing_Level': ('Writing_Level (GT)', 'Writing_Level (Pred)'),\n",
        "    'Writing_Score': ('Writing_Score (GT)', 'Writing_Score (Pred)'), 'Total_Level': ('Total_Level (GT)', 'Total_Level (Pred)'),\n",
        "    'Total_Score': ('Total_Score (GT)', 'Total_Score (Pred)'),\n",
        "}\n",
        "evaluation_summary_list = []\n",
        "for field, (gt_col, pred_col) in fields_to_evaluate.items():\n",
        "    ground_truth = [normalize_text(t) for t in eval_df[gt_col]]\n",
        "    prediction = [normalize_text(t) for t in eval_df[pred_col]]\n",
        "    accuracy = np.mean([1 if gt == pred else 0 for gt, pred in zip(ground_truth, prediction)]) * 100\n",
        "    error_metrics = jiwer.compute_measures(ground_truth, prediction)\n",
        "    wer = error_metrics.get('wer', 0) * 100\n",
        "    cer = error_metrics.get('cer', 0) * 100\n",
        "    H = error_metrics.get('hits', 0); I = error_metrics.get('insertions', 0); D = error_metrics.get('deletions', 0); S = error_metrics.get('substitutions', 0)\n",
        "    precision = H / (H + I + S) if (H + I + S) > 0 else 0\n",
        "    recall = H / (H + D + S) if (H + D + S) > 0 else 0\n",
        "    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    evaluation_summary_list.append({\n",
        "        'Field': field, 'Accuracy (%)': round(accuracy, 2), 'WER (%)': round(wer, 2),\n",
        "        'CER (%)': round(cer, 2), 'F1-score (%)': round(f1_score * 100, 2)\n",
        "    })\n",
        "eval_summary_df = pd.DataFrame(evaluation_summary_list)\n",
        "eval_summary_df.insert(0, 'Approach', APPROACH_NAME)\n",
        "eval_summary_df.insert(0, 'Model', MODEL_NAME)\n",
        "print(eval_summary_df.to_string(index=False))\n",
        "print(\"--- âœ… à¸›à¸£à¸°à¹€à¸¡à¸´à¸™à¸œà¸¥à¹€à¸ªà¸£à¹‡à¸ˆà¸ªà¸´à¹‰à¸™ ---\\n\")\n",
        "\n",
        "\n",
        "# --- 6. à¸šà¸±à¸™à¸—à¸¶à¸à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œ (à¸—à¸³ 2 à¸­à¸¢à¹ˆà¸²à¸‡) ---\n",
        "# --- 6.A: à¸šà¸±à¸™à¸—à¸¶à¸à¸œà¸¥à¸à¸²à¸£à¸—à¸”à¸¥à¸­à¸‡à¸™à¸µà¹‰à¹à¸¢à¸à¹„à¸Ÿà¸¥à¹Œ ---\n",
        "print(f\"--- ğŸ’¾ à¸à¸³à¸¥à¸±à¸‡à¸šà¸±à¸™à¸—à¸¶à¸à¸£à¸²à¸¢à¸‡à¸²à¸™à¹€à¸‰à¸à¸²à¸°à¸‚à¸­à¸‡ {MODEL_NAME} ({APPROACH_NAME}) à¸¥à¸‡à¹ƒà¸™à¹„à¸Ÿà¸¥à¹Œ: {INDIVIDUAL_REPORT_PATH} ---\")\n",
        "with pd.ExcelWriter(INDIVIDUAL_REPORT_PATH, engine='openpyxl') as writer:\n",
        "    ocr_results_df.to_excel(writer, sheet_name='Detailed_Results', index=False)\n",
        "    eval_summary_df.drop(columns=['Model', 'Approach']).to_excel(writer, sheet_name='Evaluation_Summary', index=False)\n",
        "print(f\"ğŸ‰ à¸šà¸±à¸™à¸—à¸¶à¸à¹„à¸Ÿà¸¥à¹Œ {INDIVIDUAL_REPORT_PATH} à¸ªà¸³à¹€à¸£à¹‡à¸ˆ!\")\n",
        "\n",
        "# --- 6.B: à¸­à¸±à¸›à¹€à¸”à¸•à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸¥à¸‡ Master File ---\n",
        "print(f\"--- ğŸ’¾ à¸à¸³à¸¥à¸±à¸‡à¸­à¸±à¸›à¹€à¸”à¸•à¹„à¸Ÿà¸¥à¹Œ Master Report: {MASTER_OUTPUT_PATH} ---\")\n",
        "SHEET_NAME = 'Master_Evaluation'\n",
        "try:\n",
        "    with pd.ExcelFile(MASTER_OUTPUT_PATH) as xls:\n",
        "        master_df = pd.read_excel(xls, sheet_name=SHEET_NAME)\n",
        "        master_df = master_df[(master_df['Model'] != MODEL_NAME) | (master_df['Approach'] != APPROACH_NAME)]\n",
        "    combined_df = pd.concat([master_df, eval_summary_df], ignore_index=True)\n",
        "except FileNotFoundError:\n",
        "    print(f\"à¹„à¸¡à¹ˆà¸à¸šà¹„à¸Ÿà¸¥à¹Œ Master à¹€à¸”à¸´à¸¡, à¸à¸³à¸¥à¸±à¸‡à¸ªà¸£à¹‰à¸²à¸‡à¹„à¸Ÿà¸¥à¹Œà¹ƒà¸«à¸¡à¹ˆ...\")\n",
        "    combined_df = eval_summary_df\n",
        "with pd.ExcelWriter(MASTER_OUTPUT_PATH, engine='openpyxl') as writer:\n",
        "    combined_df.to_excel(writer, sheet_name=SHEET_NAME, index=False)\n",
        "print(f\"ğŸ‰ à¸­à¸±à¸›à¹€à¸”à¸•à¹„à¸Ÿà¸¥à¹Œ Master Report à¸ªà¸³à¹€à¸£à¹‡à¸ˆ!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wc4xvsKakl_p"
      },
      "source": [
        "**TRocr**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc7pYdeFf9pj"
      },
      "source": [
        " TrOCR à¸–à¸¹à¸à¸­à¸­à¸à¹à¸šà¸šà¸¡à¸²à¹ƒà¸«à¹‰à¸­à¹ˆà¸²à¸™à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸—à¸µà¸¥à¸° \"à¸šà¸£à¸£à¸—à¸±à¸”\" à¹„à¸”à¹‰à¸”à¸µà¹€à¸¢à¸µà¹ˆà¸¢à¸¡ à¹à¸•à¹ˆà¹„à¸¡à¹ˆà¹€à¸à¹ˆà¸‡à¹ƒà¸™à¸à¸²à¸£à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¹‚à¸„à¸£à¸‡à¸ªà¸£à¹‰à¸²à¸‡à¸‚à¸­à¸‡à¹€à¸­à¸à¸ªà¸²à¸£à¸—à¸±à¹‰à¸‡à¸«à¸™à¹‰à¸²à¹ƒà¸™à¸„à¸£à¸²à¸§à¹€à¸”à¸µà¸¢à¸§ à¸«à¸²à¸à¹€à¸£à¸²à¸ªà¹ˆà¸‡à¸ à¸²à¸à¸—à¸±à¹‰à¸‡à¹ƒà¸šà¹ƒà¸«à¹‰ TrOCR à¸­à¹ˆà¸²à¸™à¹‚à¸”à¸¢à¸•à¸£à¸‡ à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œà¸—à¸µà¹ˆà¹„à¸”à¹‰à¸¡à¸±à¸à¸ˆà¸°à¹€à¸›à¹‡à¸™à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸¢à¸²à¸§à¹† à¸—à¸µà¹ˆà¹„à¸¡à¹ˆà¸¡à¸µà¸à¸²à¸£à¸ˆà¸±à¸”à¹€à¸£à¸µà¸¢à¸‡ à¸—à¸³à¹ƒà¸«à¹‰à¹à¸¢à¸à¹à¸¢à¸°à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹„à¸”à¹‰à¸¢à¸²à¸à¸¡à¸²à¸\n",
        "\n",
        "à¸”à¸±à¸‡à¸™à¸±à¹‰à¸™ à¹€à¸à¸·à¹ˆà¸­à¸”à¸¶à¸‡à¸›à¸£à¸°à¸ªà¸´à¸—à¸˜à¸´à¸ à¸²à¸à¸ªà¸¹à¸‡à¸ªà¸¸à¸”à¸‚à¸­à¸‡ TrOCR à¸­à¸­à¸à¸¡à¸² à¹€à¸£à¸²à¸ˆà¸°à¹ƒà¸Šà¹‰ à¹à¸™à¸§à¸—à¸²à¸‡à¹„à¸®à¸šà¸£à¸´à¸” (Hybrid Approach) à¸—à¸µà¹ˆà¸Šà¸²à¸à¸‰à¸¥à¸²à¸”à¸à¸§à¹ˆà¸² à¸”à¸±à¸‡à¸™à¸µà¹‰:\n",
        "\n",
        "1. à¹ƒà¸Šà¹‰ Pytesseract à¸«à¸²à¸•à¸³à¹à¸«à¸™à¹ˆà¸‡à¸šà¸£à¸£à¸—à¸±à¸” (Layout Detection): à¹€à¸£à¸²à¸ˆà¸°à¹ƒà¸Šà¹‰ pytesseract.image_to_data à¹€à¸à¸·à¹ˆà¸­à¸ªà¹à¸à¸™à¸«à¸² \"à¸•à¸³à¹à¸«à¸™à¹ˆà¸‡\" à¸‚à¸­à¸‡à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¹à¸•à¹ˆà¸¥à¸°à¸šà¸£à¸£à¸—à¸±à¸”à¸šà¸™à¸«à¸™à¹‰à¸²à¹‚à¸”à¸¢à¸­à¸±à¸•à¹‚à¸™à¸¡à¸±à¸•à¸´ (à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™à¸™à¸µà¹‰à¹€à¸«à¸¡à¸·à¸­à¸™à¸à¸²à¸£à¸—à¸³ Bounding Box à¸­à¸±à¸•à¹‚à¸™à¸¡à¸±à¸•à¸´)\n",
        "2. à¹ƒà¸Šà¹‰ TrOCR à¸­à¹ˆà¸²à¸™à¸—à¸µà¸¥à¸°à¸šà¸£à¸£à¸—à¸±à¸” (Line-by-Line OCR): à¹€à¸¡à¸·à¹ˆà¸­à¹€à¸£à¸²à¸£à¸¹à¹‰à¸•à¸³à¹à¸«à¸™à¹ˆà¸‡à¸‚à¸­à¸‡à¹à¸•à¹ˆà¸¥à¸°à¸šà¸£à¸£à¸—à¸±à¸”à¹à¸¥à¹‰à¸§ à¹€à¸£à¸²à¸ˆà¸°à¸•à¸±à¸”à¸ à¸²à¸à¹€à¸‰à¸à¸²à¸°à¸šà¸£à¸£à¸—à¸±à¸”à¸™à¸±à¹‰à¸™à¹† à¹à¸¥à¹‰à¸§à¸ªà¹ˆà¸‡à¹ƒà¸«à¹‰ TrOCR à¸­à¹ˆà¸²à¸™ à¸‹à¸¶à¹ˆà¸‡à¹€à¸›à¹‡à¸™à¸‡à¸²à¸™à¸—à¸µà¹ˆ TrOCR à¸–à¸™à¸±à¸”à¸—à¸µà¹ˆà¸ªà¸¸à¸” à¸ˆà¸°à¹„à¸”à¹‰à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œà¸—à¸µà¹ˆà¹à¸¡à¹ˆà¸™à¸¢à¸³à¸¡à¸²à¸\n",
        "3. à¹ƒà¸Šà¹‰ Parser à¸ªà¸à¸±à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥: à¸™à¸³à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸—à¸¸à¸à¸šà¸£à¸£à¸—à¸±à¸”à¸—à¸µà¹ˆ TrOCR à¸­à¹ˆà¸²à¸™à¹„à¸”à¹‰à¸¡à¸²à¸£à¸§à¸¡à¸à¸±à¸™ à¹à¸¥à¹‰à¸§à¹ƒà¸Šà¹‰ Parser à¸—à¸µà¹ˆà¹€à¸£à¸²à¸à¸±à¸’à¸™à¸²à¹„à¸§à¹‰ (à¹à¸šà¸šà¹€à¸”à¸µà¸¢à¸§à¸à¸±à¸š Pytesseract Pure OCR) à¹€à¸à¸·à¹ˆà¸­à¸ªà¸à¸±à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ Key-Value à¸­à¸­à¸à¸¡à¸²"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "8aeJD-FbkppT"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import jiwer\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
        "import pytesseract # â¬…ï¸ à¹€à¸à¸´à¹ˆà¸¡à¹€à¸‚à¹‰à¸²à¸¡à¸²à¹€à¸à¸·à¹ˆà¸­à¹ƒà¸Šà¹‰à¸«à¸² Layout\n",
        "\n",
        "# --- 1. à¸à¸³à¸«à¸™à¸”à¸„à¹ˆà¸²à¹à¸¥à¸° Path à¸—à¸µà¹ˆà¸ªà¸³à¸„à¸±à¸ ---\n",
        "MODEL_NAME = 'TrOCR'\n",
        "APPROACH_NAME = 'Pure OCR (Hybrid)' # â¬…ï¸ à¸£à¸°à¸šà¸¸à¸§à¹ˆà¸²à¹€à¸›à¹‡à¸™à¹à¸™à¸§à¸—à¸²à¸‡à¹„à¸®à¸šà¸£à¸´à¸”\n",
        "INDIVIDUAL_REPORT_PATH = f\"/content/drive/MyDrive/kmitl_dataset/final-excel/{MODEL_NAME.lower()}_{APPROACH_NAME.replace(' ', '_').lower()}_report.xlsx\"\n",
        "MASTER_OUTPUT_PATH = \"/content/drive/MyDrive/kmitl_dataset/final-excel/final_comparison_report.xlsx\"\n",
        "\n",
        "IMAGE_DIR = \"/content/drive/MyDrive/kmitl_dataset/final-preprecessed\"\n",
        "GROUND_TRUTH_PATH = \"/content/drive/MyDrive/kmitl_dataset/dataset/à¸„à¸°à¹à¸™à¸™ TEP (Pilot Study)_Total_IT.xlsx\"\n",
        "\n",
        "# --- 2. à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¸Šà¹ˆà¸§à¸¢à¹€à¸«à¸¥à¸·à¸­ (à¹€à¸«à¸¡à¸·à¸­à¸™à¹€à¸”à¸´à¸¡) ---\n",
        "def parse_score(text):\n",
        "    if not text: return None, None\n",
        "    processed_text = text.replace('l', '1').replace('I', '1')\n",
        "    match = re.search(r'([A-Z][1-2])\\s*\\((\\d+)\\)', processed_text, re.IGNORECASE)\n",
        "    if match: return match.group(1).upper(), match.group(2)\n",
        "    return None, None\n",
        "\n",
        "def get_id_from_filename(filename):\n",
        "    match = re.search(r'(\\d+)\\.(png|jpg|jpeg)$', filename.lower())\n",
        "    if match: return int(match.group(1))\n",
        "    return None\n",
        "\n",
        "def normalize_text(text):\n",
        "    if text is None: return \"\"\n",
        "    text = str(text).strip().lower()\n",
        "    if text.endswith('.0'): text = text[:-2]\n",
        "    return text\n",
        "\n",
        "def parse_pytesseract_pure_ocr(text): # â¬…ï¸ à¹€à¸£à¸²à¸ªà¸²à¸¡à¸²à¸£à¸–à¹ƒà¸Šà¹‰ Parser à¹€à¸”à¸´à¸¡à¸‚à¸­à¸‡ Pytesseract à¹„à¸”à¹‰à¹€à¸¥à¸¢\n",
        "    result = {}\n",
        "    clean_text = text.replace('â€™', \"'\").replace('â€“', '-').replace('!', '1').replace('l', '1').replace('I', '1')\n",
        "    name_match = re.search(r'Name\\s*:?\\s*(.*?)\\s*Application', clean_text, re.DOTALL | re.IGNORECASE)\n",
        "    if name_match: result[\"name\"] = name_match.group(1).strip()\n",
        "    app_no_match = re.search(r'Application\\s*No\\.?\\s*:*\\s*(\\S+)', clean_text, re.IGNORECASE)\n",
        "    if app_no_match: result[\"application_no\"] = app_no_match.group(1).strip()\n",
        "    date_match = re.search(r'Date of test Administration\\s*:?\\s*(.+)', clean_text, re.IGNORECASE)\n",
        "    if date_match: result[\"test_date\"] = date_match.group(1).strip().split('\\n')[0]\n",
        "    score_pattern_full = r'([A-B][1-2]\\s*\\(\\d+\\))'\n",
        "    for skill in [\"Grammar\", \"Reading\", \"Speaking\", \"Writing\"]:\n",
        "        m = re.search(rf\"{skill}.*?({score_pattern_full})\", clean_text, re.IGNORECASE)\n",
        "        if m:\n",
        "            level, score = parse_score(m.group(1))\n",
        "            if level:\n",
        "                result[f\"{skill.lower()}_level\"] = level\n",
        "                result[f\"{skill.lower()}_score\"] = score\n",
        "    all_scores_text = re.findall(score_pattern_full, clean_text, re.IGNORECASE)\n",
        "    used_scores_text = []\n",
        "    for skill in [\"grammar\", \"reading\", \"speaking\", \"writing\"]:\n",
        "        if f\"{skill}_level\" in result:\n",
        "             used_scores_text.append(f\"{result[f'{skill}_level']} ({result[f'{skill}_score']})\")\n",
        "    remaining_scores = [s for s in all_scores_text if s not in used_scores_text]\n",
        "    if remaining_scores:\n",
        "        total_level, total_score = parse_score(remaining_scores[0])\n",
        "        result[\"total_level\"] = total_level\n",
        "        result[\"total_score\"] = total_score\n",
        "    return result\n",
        "\n",
        "# --- 3. à¹‚à¸«à¸¥à¸”à¹à¸¥à¸°à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸à¹ˆà¸­à¸™à¹€à¸£à¸´à¹ˆà¸¡ ---\n",
        "print(\"--- ğŸ” à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥ ---\")\n",
        "gt_df = pd.read_excel(GROUND_TRUTH_PATH)\n",
        "ground_truth_ids = set(gt_df['No.'].unique())\n",
        "image_files = sorted([f for f in os.listdir(IMAGE_DIR) if f.lower().endswith(('.png', '.jpg', '.jpeg'))], key=get_id_from_filename)\n",
        "print(f\"à¸à¸š ID à¹ƒà¸™ Ground Truth à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”: {len(ground_truth_ids)} ID | à¸à¸šà¹„à¸Ÿà¸¥à¹Œà¸£à¸¹à¸›à¸ à¸²à¸: {len(image_files)} à¹„à¸Ÿà¸¥à¹Œ\")\n",
        "print(\"--- âœ… à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸ªà¸£à¹‡à¸ˆà¸ªà¸´à¹‰à¸™ ---\\n\")\n",
        "\n",
        "# --- 4. à¹€à¸£à¸´à¹ˆà¸¡à¸à¸£à¸°à¸šà¸§à¸™à¸à¸²à¸£à¸ªà¸à¸±à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ (TrOCR Hybrid) ---\n",
        "# â¬‡ï¸ à¹‚à¸«à¸¥à¸”à¹‚à¸¡à¹€à¸”à¸¥ TrOCR (à¸—à¸³à¸„à¸£à¸±à¹‰à¸‡à¹€à¸”à¸µà¸¢à¸§)\n",
        "print(\"--- ğŸ¤– à¸à¸³à¸¥à¸±à¸‡à¹‚à¸«à¸¥à¸”à¹‚à¸¡à¹€à¸”à¸¥ TrOCR ---\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"à¸ˆà¸°à¹ƒà¸Šà¹‰ Device: {device}\")\n",
        "processor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-printed')\n",
        "model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed').to(device)\n",
        "print(\"--- âœ… à¹‚à¸«à¸¥à¸”à¹‚à¸¡à¹€à¸”à¸¥à¸ªà¸³à¹€à¸£à¹‡à¸ˆ! ---\\n\")\n",
        "\n",
        "results_list = []\n",
        "print(f\"--- ğŸš€ à¹€à¸£à¸´à¹ˆà¸¡à¸à¸²à¸£à¸ªà¸à¸±à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ˆà¸²à¸à¸£à¸¹à¸›à¸ à¸²à¸ {len(image_files)} à¹„à¸Ÿà¸¥à¹Œ à¸”à¹‰à¸§à¸¢ {MODEL_NAME} ({APPROACH_NAME}) ---\")\n",
        "for filename in image_files:\n",
        "    image_id = get_id_from_filename(filename)\n",
        "    if image_id is None: continue\n",
        "    gt_row = gt_df[gt_df['No.'] == image_id]\n",
        "    if gt_row.empty: continue\n",
        "\n",
        "    print(f\"  - à¸à¸³à¸¥à¸±à¸‡à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥ ID: {image_id} ({filename})\")\n",
        "    image_path = os.path.join(IMAGE_DIR, filename)\n",
        "    img = cv2.imread(image_path)\n",
        "\n",
        "    # â¬‡ï¸â¬‡ï¸â¬‡ï¸ à¸ªà¹ˆà¸§à¸™à¸‚à¸­à¸‡ Hybrid Approach â¬‡ï¸â¬‡ï¸â¬‡ï¸\n",
        "    # 1. à¹ƒà¸Šà¹‰ Pytesseract à¸«à¸²à¸•à¸³à¹à¸«à¸™à¹ˆà¸‡à¸‚à¸­à¸‡à¸—à¸¸à¸à¸šà¸£à¸£à¸—à¸±à¸” (Layout Detection)\n",
        "    line_data = pytesseract.image_to_data(img, output_type=pytesseract.Output.DATAFRAME)\n",
        "    line_data = line_data[line_data.conf > 30].dropna(subset=['text'])\n",
        "\n",
        "    recognized_lines = []\n",
        "    # 2. à¸­à¹ˆà¸²à¸™à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸—à¸µà¸¥à¸°à¸šà¸£à¸£à¸—à¸±à¸”à¸”à¹‰à¸§à¸¢ TrOCR\n",
        "    for line_num in line_data['line_num'].unique():\n",
        "        line_df = line_data[line_data['line_num'] == line_num]\n",
        "        if not line_df.empty:\n",
        "            # à¸«à¸² Bounding Box à¸‚à¸­à¸‡à¸—à¸±à¹‰à¸‡à¸šà¸£à¸£à¸—à¸±à¸”\n",
        "            x, y, w, h = (line_df.left.min(), line_df.top.min(),\n",
        "                          line_df.left.max() + line_df.width.max() - line_df.left.min(),\n",
        "                          line_df.top.max() + line_df.height.max() - line_df.top.min())\n",
        "\n",
        "            # à¸•à¸±à¸”à¸ à¸²à¸à¹€à¸‰à¸à¸²à¸°à¸šà¸£à¸£à¸—à¸±à¸”\n",
        "            line_image = img[y:y+h, x:x+w]\n",
        "\n",
        "            # à¸ªà¹ˆà¸‡à¸ à¸²à¸à¸šà¸£à¸£à¸—à¸±à¸”à¹ƒà¸«à¹‰ TrOCR à¸­à¹ˆà¸²à¸™\n",
        "            if line_image.size > 0:\n",
        "                line_rgb = cv2.cvtColor(line_image, cv2.COLOR_BGR2RGB)\n",
        "                pixel_values = processor(images=line_rgb, return_tensors=\"pt\").pixel_values.to(device)\n",
        "                generated_ids = model.generate(pixel_values)\n",
        "                generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "                recognized_lines.append(generated_text)\n",
        "\n",
        "    # 3. à¸£à¸§à¸¡à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸—à¸µà¹ˆà¹„à¸”à¹‰à¸ˆà¸²à¸ TrOCR à¹à¸¥à¹‰à¸§à¸ªà¹ˆà¸‡à¹ƒà¸«à¹‰ Parser\n",
        "    full_text = \"\\n\".join(recognized_lines)\n",
        "    extracted_raw = parse_pytesseract_pure_ocr(full_text)\n",
        "\n",
        "    # (à¸ªà¹ˆà¸§à¸™à¸—à¸µà¹ˆà¹€à¸«à¸¥à¸·à¸­à¹€à¸«à¸¡à¸·à¸­à¸™à¹€à¸”à¸´à¸¡à¸—à¸¸à¸à¸›à¸£à¸°à¸à¸²à¸£)\n",
        "    # ...\n",
        "    # (à¸ªà¹ˆà¸§à¸™ Parse à¹à¸¥à¸° Append to results_list à¹€à¸«à¸¡à¸·à¸­à¸™à¹€à¸”à¸´à¸¡)\n",
        "    g_level, g_score = parse_score(extracted_raw.get('grammar'))\n",
        "    r_level, r_score = parse_score(extracted_raw.get('reading'))\n",
        "    s_level, s_score = parse_score(extracted_raw.get('speaking'))\n",
        "    w_level, w_score = parse_score(extracted_raw.get('writing'))\n",
        "    t_level, t_score = parse_score(extracted_raw.get('total'))\n",
        "    results_list.append({\n",
        "        \"No.\": image_id,\n",
        "        \"Application No. (GT)\": gt_row.iloc[0][\"Application No.\"], \"Application No. (Pred)\": extracted_raw.get('application_no'),\n",
        "        \"Name (GT)\": gt_row.iloc[0][\"Name\"], \"Name (Pred)\": extracted_raw.get('name'),\n",
        "        \"Test Date (GT)\": gt_row.iloc[0][\"Test Date\"], \"Test Date (Pred)\": extracted_raw.get('test_date'),\n",
        "        \"Grammar_Level (GT)\": gt_row.iloc[0][\"Grammar_Level\"], \"Grammar_Level (Pred)\": g_level,\n",
        "        \"Grammar_Score (GT)\": gt_row.iloc[0][\"Grammar_Score\"], \"Grammar_Score (Pred)\": g_score,\n",
        "        \"Reading_Level (GT)\": gt_row.iloc[0][\"Reading_Level\"], \"Reading_Level (Pred)\": r_level,\n",
        "        \"Reading_Score (GT)\": gt_row.iloc[0][\"Reading_Score\"], \"Reading_Score (Pred)\": r_score,\n",
        "        \"Speaking_Level (GT)\": gt_row.iloc[0][\"Speaking_Level\"], \"Speaking_Level (Pred)\": s_level,\n",
        "        \"Speaking_Score (GT)\": gt_row.iloc[0][\"Speaking_Score\"], \"Speaking_Score (Pred)\": s_score,\n",
        "        \"Writing_Level (GT)\": gt_row.iloc[0][\"Writing_Level\"], \"Writing_Level (Pred)\": w_level,\n",
        "        \"Writing_Score (GT)\": gt_row.iloc[0][\"Writing_Score\"], \"Writing_Score (Pred)\": w_score,\n",
        "        \"Total_Level (GT)\": gt_row.iloc[0][\"Total_Level\"], \"Total_Level (Pred)\": t_level,\n",
        "        \"Total_Score (GT)\": gt_row.iloc[0][\"Total_Score\"], \"Total_Score (Pred)\": t_score,\n",
        "    })\n",
        "\n",
        "\n",
        "ocr_results_df = pd.DataFrame(results_list)\n",
        "print(\"--- âœ… à¸ªà¸à¸±à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸ªà¸£à¹‡à¸ˆà¸ªà¸´à¹‰à¸™ ---\\n\")\n",
        "\n",
        "\n",
        "# --- 5. à¹€à¸£à¸´à¹ˆà¸¡à¸à¸£à¸°à¸šà¸§à¸™à¸à¸²à¸£à¸›à¸£à¸°à¹€à¸¡à¸´à¸™à¸œà¸¥ (Evaluation) ---\n",
        "# ... (à¹‚à¸„à¹‰à¸”à¸ªà¹ˆà¸§à¸™à¸™à¸µà¹‰à¹€à¸«à¸¡à¸·à¸­à¸™à¸à¸±à¸šà¹€à¸§à¸­à¸£à¹Œà¸Šà¸±à¸™à¸à¹ˆà¸­à¸™à¸«à¸™à¹‰à¸²à¸—à¸¸à¸à¸›à¸£à¸°à¸à¸²à¸£) ...\n",
        "print(\"--- ğŸ“Š à¹€à¸£à¸´à¹ˆà¸¡à¸à¸²à¸£à¸›à¸£à¸°à¹€à¸¡à¸´à¸™à¸œà¸¥ ---\")\n",
        "eval_df = ocr_results_df.fillna('')\n",
        "fields_to_evaluate = {\n",
        "    'Name': ('Name (GT)', 'Name (Pred)'), 'Application No.': ('Application No. (GT)', 'Application No. (Pred)'),\n",
        "    'Test Date': ('Test Date (GT)', 'Test Date (Pred)'), 'Grammar_Level': ('Grammar_Level (GT)', 'Grammar_Level (Pred)'),\n",
        "    'Grammar_Score': ('Grammar_Score (GT)', 'Grammar_Score (Pred)'), 'Reading_Level': ('Reading_Level (GT)', 'Reading_Level (Pred)'),\n",
        "    'Reading_Score': ('Reading_Score (GT)', 'Reading_Score (Pred)'), 'Speaking_Level': ('Speaking_Level (GT)', 'Speaking_Level (Pred)'),\n",
        "    'Speaking_Score': ('Speaking_Score (GT)', 'Speaking_Score (Pred)'), 'Writing_Level': ('Writing_Level (GT)', 'Writing_Level (Pred)'),\n",
        "    'Writing_Score': ('Writing_Score (GT)', 'Writing_Score (Pred)'), 'Total_Level': ('Total_Level (GT)', 'Total_Level (Pred)'),\n",
        "    'Total_Score': ('Total_Score (GT)', 'Total_Score (Pred)'),\n",
        "}\n",
        "evaluation_summary_list = []\n",
        "for field, (gt_col, pred_col) in fields_to_evaluate.items():\n",
        "    ground_truth = [normalize_text(t) for t in eval_df[gt_col]]\n",
        "    prediction = [normalize_text(t) for t in eval_df[pred_col]]\n",
        "    accuracy = np.mean([1 if gt == pred else 0 for gt, pred in zip(ground_truth, prediction)]) * 100\n",
        "    error_metrics = jiwer.compute_measures(ground_truth, prediction)\n",
        "    wer = error_metrics.get('wer', 0) * 100\n",
        "    cer = error_metrics.get('cer', 0) * 100\n",
        "    H = error_metrics.get('hits', 0); I = error_metrics.get('insertions', 0); D = error_metrics.get('deletions', 0); S = error_metrics.get('substitutions', 0)\n",
        "    precision = H / (H + I + S) if (H + I + S) > 0 else 0\n",
        "    recall = H / (H + D + S) if (H + D + S) > 0 else 0\n",
        "    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    evaluation_summary_list.append({\n",
        "        'Field': field, 'Accuracy (%)': round(accuracy, 2), 'WER (%)': round(wer, 2),\n",
        "        'CER (%)': round(cer, 2), 'F1-score (%)': round(f1_score * 100, 2)\n",
        "    })\n",
        "eval_summary_df = pd.DataFrame(evaluation_summary_list)\n",
        "eval_summary_df.insert(0, 'Approach', APPROACH_NAME)\n",
        "eval_summary_df.insert(0, 'Model', MODEL_NAME)\n",
        "print(eval_summary_df.to_string(index=False))\n",
        "print(\"--- âœ… à¸›à¸£à¸°à¹€à¸¡à¸´à¸™à¸œà¸¥à¹€à¸ªà¸£à¹‡à¸ˆà¸ªà¸´à¹‰à¸™ ---\\n\")\n",
        "\n",
        "\n",
        "# --- 6. à¸šà¸±à¸™à¸—à¸¶à¸à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œ (à¸—à¸³ 2 à¸­à¸¢à¹ˆà¸²à¸‡) ---\n",
        "# --- 6.A: à¸šà¸±à¸™à¸—à¸¶à¸à¸œà¸¥à¸à¸²à¸£à¸—à¸”à¸¥à¸­à¸‡à¸™à¸µà¹‰à¹à¸¢à¸à¹„à¸Ÿà¸¥à¹Œ ---\n",
        "print(f\"--- ğŸ’¾ à¸à¸³à¸¥à¸±à¸‡à¸šà¸±à¸™à¸—à¸¶à¸à¸£à¸²à¸¢à¸‡à¸²à¸™à¹€à¸‰à¸à¸²à¸°à¸‚à¸­à¸‡ {MODEL_NAME} ({APPROACH_NAME}) à¸¥à¸‡à¹ƒà¸™à¹„à¸Ÿà¸¥à¹Œ: {INDIVIDUAL_REPORT_PATH} ---\")\n",
        "with pd.ExcelWriter(INDIVIDUAL_REPORT_PATH, engine='openpyxl') as writer:\n",
        "    ocr_results_df.to_excel(writer, sheet_name='Detailed_Results', index=False)\n",
        "    eval_summary_df.drop(columns=['Model', 'Approach']).to_excel(writer, sheet_name='Evaluation_Summary', index=False)\n",
        "print(f\"ğŸ‰ à¸šà¸±à¸™à¸—à¸¶à¸à¹„à¸Ÿà¸¥à¹Œ {INDIVIDUAL_REPORT_PATH} à¸ªà¸³à¹€à¸£à¹‡à¸ˆ!\")\n",
        "\n",
        "# --- 6.B: à¸­à¸±à¸›à¹€à¸”à¸•à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸¥à¸‡ Master File ---\n",
        "print(f\"--- ğŸ’¾ à¸à¸³à¸¥à¸±à¸‡à¸­à¸±à¸›à¹€à¸”à¸•à¹„à¸Ÿà¸¥à¹Œ Master Report: {MASTER_OUTPUT_PATH} ---\")\n",
        "SHEET_NAME = 'Master_Evaluation'\n",
        "try:\n",
        "    with pd.ExcelFile(MASTER_OUTPUT_PATH) as xls:\n",
        "        master_df = pd.read_excel(xls, sheet_name=SHEET_NAME)\n",
        "        master_df = master_df[(master_df['Model'] != MODEL_NAME) | (master_df['Approach'] != APPROACH_NAME)]\n",
        "    combined_df = pd.concat([master_df, eval_summary_df], ignore_index=True)\n",
        "except FileNotFoundError:\n",
        "    print(f\"à¹„à¸¡à¹ˆà¸à¸šà¹„à¸Ÿà¸¥à¹Œ Master à¹€à¸”à¸´à¸¡, à¸à¸³à¸¥à¸±à¸‡à¸ªà¸£à¹‰à¸²à¸‡à¹„à¸Ÿà¸¥à¹Œà¹ƒà¸«à¸¡à¹ˆ...\")\n",
        "    combined_df = eval_summary_df\n",
        "with pd.ExcelWriter(MASTER_OUTPUT_PATH, engine='openpyxl') as writer:\n",
        "    combined_df.to_excel(writer, sheet_name=SHEET_NAME, index=False)\n",
        "print(f\"ğŸ‰ à¸­à¸±à¸›à¹€à¸”à¸•à¹„à¸Ÿà¸¥à¹Œ Master Report à¸ªà¸³à¹€à¸£à¹‡à¸ˆ!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niL2R_vzDxgX"
      },
      "source": [
        "# **Pytessecrac Boxing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "RHNGGu6ARflT"
      },
      "outputs": [],
      "source": [
        "!pip install pytesseract\n",
        "import cv2\n",
        "import pytesseract\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "S7CnfpilLP30"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import pytesseract\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- 1. à¸à¸³à¸«à¸™à¸”à¸„à¹ˆà¸²à¹à¸¥à¸°à¹‚à¸«à¸¥à¸”à¸£à¸¹à¸›à¸ à¸²à¸ ---\n",
        "# à¸ªà¸³à¸«à¸£à¸±à¸š Windows, à¸«à¸²à¸à¸ˆà¸³à¹€à¸›à¹‡à¸™à¸•à¹‰à¸­à¸‡à¸£à¸°à¸šà¸¸ Path à¸‚à¸­à¸‡ Tesseract\n",
        "# pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
        "\n",
        "# à¸£à¸°à¸šà¸¸ Path à¸‚à¸­à¸‡à¸£à¸¹à¸›à¸ à¸²à¸à¸—à¸µà¹ˆà¸„à¸¸à¸“à¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¸—à¸”à¸ªà¸­à¸š\n",
        "IMAGE_PATH = \"/content/drive/MyDrive/kmitl_dataset/final-preprecessed/KMITL-TEP PILOT-10.png\"\n",
        "\n",
        "# à¸à¸³à¸«à¸™à¸”à¸à¸´à¸à¸±à¸”à¸‚à¸­à¸‡à¹à¸•à¹ˆà¸¥à¸°à¸ªà¹ˆà¸§à¸™à¸—à¸µà¹ˆà¸ªà¸™à¹ƒà¸ˆ (Region of Interest)\n",
        "# à¸£à¸¹à¸›à¹à¸šà¸šà¸„à¸·à¸­: (x_start, y_start, width, height)\n",
        "ROI_CONFIG = {\n",
        "    \"name\":          (200, 485, 600, 65),\n",
        "    \"application_no\": (1100, 485, 500, 60),\n",
        "    \"test_date\":     (1310, 545, 400, 60),\n",
        "    \"overall_score\": (820, 625, 800, 100),\n",
        "    \"grammar\":       (320, 785, 500, 80),\n",
        "    \"reading\":       (1050, 785, 500, 80),\n",
        "    \"speaking\":      (320, 900, 500, 80),\n",
        "    \"writing\":       (1050, 900, 500, 80),\n",
        "}\n",
        "\n",
        "# --- 2. à¹‚à¸«à¸¥à¸”à¹à¸¥à¸°à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¸£à¸¹à¸›à¸ à¸²à¸ ---\n",
        "image = cv2.imread(IMAGE_PATH)\n",
        "if image is None:\n",
        "    print(f\"à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¹‚à¸«à¸¥à¸”à¸£à¸¹à¸›à¸ à¸²à¸à¹„à¸”à¹‰à¸ˆà¸²à¸: {IMAGE_PATH}\")\n",
        "else:\n",
        "    # à¸ªà¸£à¹‰à¸²à¸‡ Dictionary à¸§à¹ˆà¸²à¸‡à¸ªà¸³à¸«à¸£à¸±à¸šà¹€à¸à¹‡à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¸ªà¸à¸±à¸”à¹„à¸”à¹‰\n",
        "    extracted_data = {}\n",
        "\n",
        "    # à¸ªà¸£à¹‰à¸²à¸‡à¸ªà¸³à¹€à¸™à¸²à¸‚à¸­à¸‡à¸£à¸¹à¸›à¸ à¸²à¸à¹€à¸à¸·à¹ˆà¸­à¸§à¸²à¸”à¸à¸£à¸­à¸šà¸ªà¸µà¹ˆà¹€à¸«à¸¥à¸µà¹ˆà¸¢à¸¡ (à¸ªà¸³à¸«à¸£à¸±à¸šà¹à¸ªà¸”à¸‡à¸œà¸¥)\n",
        "    image_with_boxes = image.copy()\n",
        "\n",
        "    # --- 3. à¸§à¸™à¸¥à¸¹à¸›à¹€à¸à¸·à¹ˆà¸­à¸ªà¸à¸±à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸•à¸²à¸¡ ROI_CONFIG ---\n",
        "    print(\"à¸à¸³à¸¥à¸±à¸‡à¸ªà¸à¸±à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸•à¸²à¸¡à¸à¸´à¸à¸±à¸” ROI...\")\n",
        "    for field_name, (x, y, w, h) in ROI_CONFIG.items():\n",
        "\n",
        "        # 3.1 à¸•à¸±à¸”à¸ à¸²à¸ (Crop) à¸•à¸²à¸¡à¸à¸´à¸à¸±à¸”à¸—à¸µà¹ˆà¸à¸³à¸«à¸™à¸”\n",
        "        roi_image = image[y:y+h, x:x+w]\n",
        "\n",
        "        # 3.2 à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¸ à¸²à¸à¸Šà¸´à¹‰à¸™à¹€à¸¥à¹‡à¸à¹† (ROI) à¹€à¸à¸·à¹ˆà¸­à¹€à¸à¸´à¹ˆà¸¡à¸„à¸§à¸²à¸¡à¹à¸¡à¹ˆà¸™à¸¢à¸³\n",
        "        # à¹à¸›à¸¥à¸‡à¹€à¸›à¹‡à¸™ Grayscale à¹à¸¥à¸°à¸—à¸³ Binarization\n",
        "        gray_roi = cv2.cvtColor(roi_image, cv2.COLOR_BGR2GRAY)\n",
        "        # à¹€à¸à¸´à¹ˆà¸¡à¸„à¸§à¸²à¸¡à¸¥à¸°à¹€à¸­à¸µà¸¢à¸”à¸‚à¸­à¸‡à¸ à¸²à¸ ROI à¹€à¸à¸·à¹ˆà¸­à¹ƒà¸«à¹‰ Tesseract à¸­à¹ˆà¸²à¸™à¹„à¸”à¹‰à¸”à¸µà¸‚à¸¶à¹‰à¸™\n",
        "        upscaled_roi = cv2.resize(gray_roi, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)\n",
        "        # à¹ƒà¸Šà¹‰ Thresholding à¹€à¸à¸·à¹ˆà¸­à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™à¹€à¸›à¹‡à¸™à¸ à¸²à¸à¸‚à¸²à¸§-à¸”à¸³à¸—à¸µà¹ˆà¸„à¸¡à¸Šà¸±à¸”\n",
        "        _, binary_roi = cv2.threshold(upscaled_roi, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "\n",
        "        # 3.3 à¸—à¸³ OCR à¸à¸±à¸šà¸ à¸²à¸ ROI à¸—à¸µà¹ˆà¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¹à¸¥à¹‰à¸§\n",
        "        # à¹ƒà¸Šà¹‰ Page Segmentation Mode (PSM) 7 à¸‹à¸¶à¹ˆà¸‡à¹€à¸«à¸¡à¸²à¸°à¸ªà¸³à¸«à¸£à¸±à¸šà¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸šà¸£à¸£à¸—à¸±à¸”à¹€à¸”à¸µà¸¢à¸§\n",
        "        custom_config = r'--oem 3 --psm 7 -l eng'\n",
        "        text = pytesseract.image_to_string(binary_roi, config=custom_config)\n",
        "\n",
        "        # 3.4 à¸—à¸³à¸„à¸§à¸²à¸¡à¸ªà¸°à¸­à¸²à¸”à¹à¸¥à¸°à¹€à¸à¹‡à¸šà¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œ\n",
        "        cleaned_text = text.strip()\n",
        "        extracted_data[field_name] = cleaned_text\n",
        "        print(f\"  - {field_name}: '{cleaned_text}'\")\n",
        "\n",
        "        # 3.5 à¸§à¸²à¸”à¸à¸£à¸­à¸šà¸ªà¸µà¹ˆà¹€à¸«à¸¥à¸µà¹ˆà¸¢à¸¡à¸šà¸™à¸ à¸²à¸à¸ªà¸³à¹€à¸™à¸²à¹€à¸à¸·à¹ˆà¸­à¹à¸ªà¸”à¸‡à¸œà¸¥\n",
        "        cv2.rectangle(image_with_boxes, (x, y), (x + w, y + h), (0, 255, 0), 3)\n",
        "        cv2.putText(image_with_boxes, field_name, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (36, 255, 12), 3)\n",
        "\n",
        "\n",
        "    # --- 4. à¸ªà¸£à¸¸à¸›à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œ ---\n",
        "    print(\"\\n--- à¸ªà¸£à¸¸à¸›à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¸ªà¸à¸±à¸”à¹„à¸”à¹‰ ---\")\n",
        "    print(extracted_data)\n",
        "\n",
        "    # --- 5. à¹à¸ªà¸”à¸‡à¸ à¸²à¸à¸à¸£à¹‰à¸­à¸¡à¸à¸£à¸­à¸š ROI ---\n",
        "    # à¹à¸›à¸¥à¸‡ BGR à¹€à¸›à¹‡à¸™ RGB à¹€à¸à¸·à¹ˆà¸­à¹ƒà¸«à¹‰ Matplotlib à¹à¸ªà¸”à¸‡à¸ªà¸µà¸–à¸¹à¸à¸•à¹‰à¸­à¸‡\n",
        "    plt.figure(figsize=(20, 15))\n",
        "    plt.imshow(cv2.cvtColor(image_with_boxes, cv2.COLOR_BGR2RGB))\n",
        "    plt.title('Image with ROIs')\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nq2wtP3RFn4z"
      },
      "source": [
        "**Final auto pytes update to master file**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5S2ntXP-R4Gc"
      },
      "outputs": [],
      "source": [
        "!pip install jiwer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KdP_fdY3Axhj"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import jiwer\n",
        "import numpy as np\n",
        "import pytesseract\n",
        "\n",
        "# --- 1. à¸à¸³à¸«à¸™à¸”à¸„à¹ˆà¸²à¹à¸¥à¸° Path à¸—à¸µà¹ˆà¸ªà¸³à¸„à¸±à¸ ---\n",
        "# â¬‡ï¸ à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™à¹à¸„à¹ˆ 2 à¸šà¸£à¸£à¸—à¸±à¸”à¸™à¸µà¹‰\n",
        "MODEL_NAME = 'Pytesseract'\n",
        "INDIVIDUAL_REPORT_PATH = \"/content/drive/MyDrive/kmitl_dataset/final-excel/pytesseract_boxing_report.xlsx\"\n",
        "\n",
        "# Path à¸ªà¸³à¸«à¸£à¸±à¸šà¹„à¸Ÿà¸¥à¹Œ Master à¸ˆà¸°à¹ƒà¸Šà¹‰à¸Šà¸·à¹ˆà¸­à¹€à¸”à¸´à¸¡à¹€à¸à¸·à¹ˆà¸­à¸£à¸§à¸šà¸£à¸§à¸¡à¸œà¸¥\n",
        "APPROACH_NAME = 'Boxing'\n",
        "MASTER_OUTPUT_PATH = \"/content/drive/MyDrive/kmitl_dataset/final-excel/final_comparison_report.xlsx\"\n",
        "\n",
        "IMAGE_DIR = \"/content/drive/MyDrive/kmitl_dataset/final-preprecessed\"\n",
        "GROUND_TRUTH_PATH = \"/content/drive/MyDrive/kmitl_dataset/dataset/à¸„à¸°à¹à¸™à¸™ TEP (Pilot Study)_Total_IT.xlsx\"\n",
        "\n",
        "ROI_CONFIG = {\n",
        "    \"name\":           (200, 485, 600, 65),\n",
        "    \"application_no\": (1100, 485, 500, 60),\n",
        "    \"test_date\":      (1310, 545, 400, 60),\n",
        "    \"total\":          (820, 625, 800, 100),\n",
        "    \"grammar\":        (320, 785, 500, 80),\n",
        "    \"reading\":        (1050, 785, 500, 80),\n",
        "    \"speaking\":       (320, 900, 500, 80),\n",
        "    \"writing\":        (1050, 900, 500, 80),\n",
        "}\n",
        "\n",
        "# --- 2. à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¸Šà¹ˆà¸§à¸¢à¹€à¸«à¸¥à¸·à¸­ (à¸‰à¸šà¸±à¸šà¸ªà¸¡à¸šà¸¹à¸£à¸“à¹Œ) ---\n",
        "def parse_score(text):\n",
        "    if not text: return None, None\n",
        "    processed_text = text.replace('l', '1').replace('I', '1')\n",
        "    match = re.search(r'([A-Z][1-2])\\s*\\((\\d+)\\)', processed_text, re.IGNORECASE)\n",
        "    if match:\n",
        "        return match.group(1).upper(), match.group(2)\n",
        "    return None, None\n",
        "\n",
        "def get_id_from_filename(filename):\n",
        "    match = re.search(r'(\\d+)\\.(png|jpg|jpeg)$', filename.lower())\n",
        "    if match: return int(match.group(1))\n",
        "    return None\n",
        "\n",
        "def normalize_text(text):\n",
        "    if text is None: return \"\"\n",
        "    text = str(text).strip().lower()\n",
        "    if text.endswith('.0'): text = text[:-2]\n",
        "    return text\n",
        "\n",
        "# --- 3. à¹‚à¸«à¸¥à¸”à¹à¸¥à¸°à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸à¹ˆà¸­à¸™à¹€à¸£à¸´à¹ˆà¸¡ (à¹€à¸«à¸¡à¸·à¸­à¸™à¹€à¸”à¸´à¸¡) ---\n",
        "print(\"--- ğŸ” à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥ ---\")\n",
        "gt_df = pd.read_excel(GROUND_TRUTH_PATH)\n",
        "ground_truth_ids = set(gt_df['No.'].unique())\n",
        "print(f\"à¸à¸š ID à¹ƒà¸™ Ground Truth à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”: {len(ground_truth_ids)} ID\")\n",
        "allowed_extensions = ['.png', '.jpg', '.jpeg']\n",
        "all_files_in_dir = os.listdir(IMAGE_DIR)\n",
        "image_files = [f for f in all_files_in_dir if f.lower().endswith(tuple(allowed_extensions))]\n",
        "image_files.sort(key=get_id_from_filename)\n",
        "print(f\"à¸à¸šà¹„à¸Ÿà¸¥à¹Œà¸£à¸¹à¸›à¸ à¸²à¸à¹ƒà¸™à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œ: {len(image_files)} à¹„à¸Ÿà¸¥à¹Œ\")\n",
        "print(\"--- âœ… à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸ªà¸£à¹‡à¸ˆà¸ªà¸´à¹‰à¸™ ---\\n\")\n",
        "\n",
        "# --- 4. à¹€à¸£à¸´à¹ˆà¸¡à¸à¸£à¸°à¸šà¸§à¸™à¸à¸²à¸£à¸ªà¸à¸±à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸”à¹‰à¸§à¸¢ Pytesseract ---\n",
        "results_list = []\n",
        "print(f\"--- ğŸš€ à¹€à¸£à¸´à¹ˆà¸¡à¸à¸²à¸£à¸ªà¸à¸±à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ˆà¸²à¸à¸£à¸¹à¸›à¸ à¸²à¸ {len(image_files)} à¹„à¸Ÿà¸¥à¹Œ à¸”à¹‰à¸§à¸¢ {MODEL_NAME} ({APPROACH_NAME}) ---\")\n",
        "for filename in image_files:\n",
        "    image_id = get_id_from_filename(filename)\n",
        "    if image_id is None: continue\n",
        "    gt_row = gt_df[gt_df['No.'] == image_id]\n",
        "    if gt_row.empty: continue\n",
        "\n",
        "    print(f\"  - à¸à¸³à¸¥à¸±à¸‡à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥ ID: {image_id} ({filename})\")\n",
        "    image_path = os.path.join(IMAGE_DIR, filename)\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    extracted_raw = {}\n",
        "    for field_name, (x, y, w, h) in ROI_CONFIG.items():\n",
        "        roi_image = image[y:y+h, x:x+w]\n",
        "\n",
        "        # â¬‡ï¸ Preprocessing à¹à¸¥à¸° OCR à¸ªà¸³à¸«à¸£à¸±à¸š Pytesseract\n",
        "        gray_roi = cv2.cvtColor(roi_image, cv2.COLOR_BGR2GRAY)\n",
        "        upscaled_roi = cv2.resize(gray_roi, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)\n",
        "        _, binary_roi = cv2.threshold(upscaled_roi, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "\n",
        "        custom_config = r'--oem 3 --psm 7 -l eng'\n",
        "        text = pytesseract.image_to_string(binary_roi, config=custom_config).strip()\n",
        "\n",
        "        extracted_raw[field_name] = text\n",
        "\n",
        "    # (à¸ªà¹ˆà¸§à¸™ Parse à¹à¸¥à¸° Append to results_list à¹€à¸«à¸¡à¸·à¸­à¸™à¹€à¸”à¸´à¸¡)\n",
        "    g_level, g_score = parse_score(extracted_raw.get('grammar'))\n",
        "    r_level, r_score = parse_score(extracted_raw.get('reading'))\n",
        "    s_level, s_score = parse_score(extracted_raw.get('speaking'))\n",
        "    w_level, w_score = parse_score(extracted_raw.get('writing'))\n",
        "    t_level, t_score = parse_score(extracted_raw.get('total'))\n",
        "    results_list.append({\n",
        "        \"No.\": image_id,\n",
        "        \"Application No. (GT)\": gt_row.iloc[0][\"Application No.\"], \"Application No. (Pred)\": extracted_raw.get('application_no'),\n",
        "        \"Name (GT)\": gt_row.iloc[0][\"Name\"], \"Name (Pred)\": extracted_raw.get('name'),\n",
        "        \"Test Date (GT)\": gt_row.iloc[0][\"Test Date\"], \"Test Date (Pred)\": extracted_raw.get('test_date'),\n",
        "        \"Grammar_Level (GT)\": gt_row.iloc[0][\"Grammar_Level\"], \"Grammar_Level (Pred)\": g_level,\n",
        "        \"Grammar_Score (GT)\": gt_row.iloc[0][\"Grammar_Score\"], \"Grammar_Score (Pred)\": g_score,\n",
        "        \"Reading_Level (GT)\": gt_row.iloc[0][\"Reading_Level\"], \"Reading_Level (Pred)\": r_level,\n",
        "        \"Reading_Score (GT)\": gt_row.iloc[0][\"Reading_Score\"], \"Reading_Score (Pred)\": r_score,\n",
        "        \"Speaking_Level (GT)\": gt_row.iloc[0][\"Speaking_Level\"], \"Speaking_Level (Pred)\": s_level,\n",
        "        \"Speaking_Score (GT)\": gt_row.iloc[0][\"Speaking_Score\"], \"Speaking_Score (Pred)\": s_score,\n",
        "        \"Writing_Level (GT)\": gt_row.iloc[0][\"Writing_Level\"], \"Writing_Level (Pred)\": w_level,\n",
        "        \"Writing_Score (GT)\": gt_row.iloc[0][\"Writing_Score\"], \"Writing_Score (Pred)\": w_score,\n",
        "        \"Total_Level (GT)\": gt_row.iloc[0][\"Total_Level\"], \"Total_Level (Pred)\": t_level,\n",
        "        \"Total_Score (GT)\": gt_row.iloc[0][\"Total_Score\"], \"Total_Score (Pred)\": t_score,\n",
        "    })\n",
        "\n",
        "ocr_results_df = pd.DataFrame(results_list)\n",
        "print(\"--- âœ… à¸ªà¸à¸±à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸ªà¸£à¹‡à¸ˆà¸ªà¸´à¹‰à¸™ ---\\n\")\n",
        "\n",
        "# --- 5. à¹€à¸£à¸´à¹ˆà¸¡à¸à¸£à¸°à¸šà¸§à¸™à¸à¸²à¸£à¸›à¸£à¸°à¹€à¸¡à¸´à¸™à¸œà¸¥ (Evaluation) ---\n",
        "# (à¸ªà¹ˆà¸§à¸™à¸™à¸µà¹‰à¹€à¸«à¸¡à¸·à¸­à¸™à¹€à¸”à¸´à¸¡à¸—à¸¸à¸à¸›à¸£à¸°à¸à¸²à¸£)\n",
        "print(\"--- ğŸ“Š à¹€à¸£à¸´à¹ˆà¸¡à¸à¸²à¸£à¸›à¸£à¸°à¹€à¸¡à¸´à¸™à¸œà¸¥ ---\")\n",
        "eval_df = ocr_results_df.fillna('')\n",
        "fields_to_evaluate = {\n",
        "    'Name': ('Name (GT)', 'Name (Pred)'), 'Application No.': ('Application No. (GT)', 'Application No. (Pred)'),\n",
        "    'Test Date': ('Test Date (GT)', 'Test Date (Pred)'), 'Grammar_Level': ('Grammar_Level (GT)', 'Grammar_Level (Pred)'),\n",
        "    'Grammar_Score': ('Grammar_Score (GT)', 'Grammar_Score (Pred)'), 'Reading_Level': ('Reading_Level (GT)', 'Reading_Level (Pred)'),\n",
        "    'Reading_Score': ('Reading_Score (GT)', 'Reading_Score (Pred)'), 'Speaking_Level': ('Speaking_Level (GT)', 'Speaking_Level (Pred)'),\n",
        "    'Speaking_Score': ('Speaking_Score (GT)', 'Speaking_Score (Pred)'), 'Writing_Level': ('Writing_Level (GT)', 'Writing_Level (Pred)'),\n",
        "    'Writing_Score': ('Writing_Score (GT)', 'Writing_Score (Pred)'), 'Total_Level': ('Total_Level (GT)', 'Total_Level (Pred)'),\n",
        "    'Total_Score': ('Total_Score (GT)', 'Total_Score (Pred)'),\n",
        "}\n",
        "evaluation_summary_list = []\n",
        "for field, (gt_col, pred_col) in fields_to_evaluate.items():\n",
        "    ground_truth = [normalize_text(t) for t in eval_df[gt_col]]\n",
        "    prediction = [normalize_text(t) for t in eval_df[pred_col]]\n",
        "    accuracy = np.mean([1 if gt == pred else 0 for gt, pred in zip(ground_truth, prediction)]) * 100\n",
        "    error_metrics = jiwer.compute_measures(ground_truth, prediction)\n",
        "    wer = error_metrics.get('wer', 0) * 100\n",
        "    cer = error_metrics.get('cer', 0) * 100\n",
        "    H = error_metrics.get('hits', 0); I = error_metrics.get('insertions', 0); D = error_metrics.get('deletions', 0); S = error_metrics.get('substitutions', 0)\n",
        "    precision = H / (H + I + S) if (H + I + S) > 0 else 0\n",
        "    recall = H / (H + D + S) if (H + D + S) > 0 else 0\n",
        "    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    evaluation_summary_list.append({\n",
        "        'Field': field, 'Accuracy (%)': round(accuracy, 2), 'WER (%)': round(wer, 2),\n",
        "        'CER (%)': round(cer, 2), 'F1-score (%)': round(f1_score * 100, 2)\n",
        "    })\n",
        "eval_summary_df = pd.DataFrame(evaluation_summary_list)\n",
        "eval_summary_df.insert(0, 'Approach', APPROACH_NAME)\n",
        "eval_summary_df.insert(0, 'Model', MODEL_NAME)\n",
        "print(eval_summary_df.to_string(index=False))\n",
        "print(\"--- âœ… à¸›à¸£à¸°à¹€à¸¡à¸´à¸™à¸œà¸¥à¹€à¸ªà¸£à¹‡à¸ˆà¸ªà¸´à¹‰à¸™ ---\\n\")\n",
        "\n",
        "\n",
        "# --- 6. à¸šà¸±à¸™à¸—à¸¶à¸à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œ (à¸—à¸³ 2 à¸­à¸¢à¹ˆà¸²à¸‡) ---\n",
        "# --- 6.A: à¸šà¸±à¸™à¸—à¸¶à¸à¸œà¸¥à¸à¸²à¸£à¸—à¸”à¸¥à¸­à¸‡à¸™à¸µà¹‰à¹à¸¢à¸à¹„à¸Ÿà¸¥à¹Œ (2 à¸Šà¸µà¸•) ---\n",
        "print(f\"--- ğŸ’¾ à¸à¸³à¸¥à¸±à¸‡à¸šà¸±à¸™à¸—à¸¶à¸à¸£à¸²à¸¢à¸‡à¸²à¸™à¹€à¸‰à¸à¸²à¸°à¸‚à¸­à¸‡ {MODEL_NAME} à¸¥à¸‡à¹ƒà¸™à¹„à¸Ÿà¸¥à¹Œ: {INDIVIDUAL_REPORT_PATH} ---\")\n",
        "with pd.ExcelWriter(INDIVIDUAL_REPORT_PATH, engine='openpyxl') as writer:\n",
        "    ocr_results_df.to_excel(writer, sheet_name='Detailed_Results', index=False)\n",
        "    eval_summary_df.drop(columns=['Model', 'Approach']).to_excel(writer, sheet_name='Evaluation_Summary', index=False)\n",
        "print(f\"ğŸ‰ à¸šà¸±à¸™à¸—à¸¶à¸à¹„à¸Ÿà¸¥à¹Œ {INDIVIDUAL_REPORT_PATH} à¸ªà¸³à¹€à¸£à¹‡à¸ˆ!\")\n",
        "\n",
        "# --- 6.B: à¸­à¸±à¸›à¹€à¸”à¸•à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸¥à¸‡ Master File à¸ªà¸³à¸«à¸£à¸±à¸šà¸ªà¸£à¹‰à¸²à¸‡à¸à¸£à¸²à¸Ÿ ---\n",
        "print(f\"--- ğŸ’¾ à¸à¸³à¸¥à¸±à¸‡à¸­à¸±à¸›à¹€à¸”à¸•à¹„à¸Ÿà¸¥à¹Œ Master Report: {MASTER_OUTPUT_PATH} ---\")\n",
        "SHEET_NAME = 'Master_Evaluation'\n",
        "try:\n",
        "    with pd.ExcelFile(MASTER_OUTPUT_PATH) as xls:\n",
        "        master_df = pd.read_excel(xls, sheet_name=SHEET_NAME)\n",
        "        master_df = master_df[(master_df['Model'] != MODEL_NAME) | (master_df['Approach'] != APPROACH_NAME)]\n",
        "    combined_df = pd.concat([master_df, eval_summary_df], ignore_index=True)\n",
        "except FileNotFoundError:\n",
        "    print(f\"à¹„à¸¡à¹ˆà¸à¸šà¹„à¸Ÿà¸¥à¹Œ Master à¹€à¸”à¸´à¸¡, à¸à¸³à¸¥à¸±à¸‡à¸ªà¸£à¹‰à¸²à¸‡à¹„à¸Ÿà¸¥à¹Œà¹ƒà¸«à¸¡à¹ˆ...\")\n",
        "    combined_df = eval_summary_df\n",
        "with pd.ExcelWriter(MASTER_OUTPUT_PATH, engine='openpyxl') as writer:\n",
        "    combined_df.to_excel(writer, sheet_name=SHEET_NAME, index=False)\n",
        "\n",
        "print(f\"ğŸ‰ à¸­à¸±à¸›à¹€à¸”à¸•à¹„à¸Ÿà¸¥à¹Œ Master Report à¸ªà¸³à¹€à¸£à¹‡à¸ˆ!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAoPk0hwh5PN"
      },
      "source": [
        "# **Easy OCR Boxing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ahhYkSvSBeGm"
      },
      "outputs": [],
      "source": [
        "!pip install easyocr\n",
        "!pip install jiwer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "GkXXektfnj0P"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import jiwer\n",
        "import numpy as np\n",
        "import easyocr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zwsSLHQ7XD1"
      },
      "source": [
        "Easy read B1 => Bl (L-lower) or BI (I)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "gvfYxh7V0Cvb"
      },
      "outputs": [],
      "source": [
        "# --- à¹‚à¸„à¹‰à¸”à¸ªà¸³à¸«à¸£à¸±à¸šà¸”à¸µà¸šà¸±à¸à¹à¸¥à¸°à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š ROI ---\n",
        "import cv2\n",
        "import easyocr\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. à¸à¸³à¸«à¸™à¸” Path à¸£à¸¹à¸›à¸ à¸²à¸à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡à¸—à¸µà¹ˆà¸„à¸¸à¸“à¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š\n",
        "IMAGE_PATH_TO_DEBUG = \"/content/drive/MyDrive/kmitl_dataset/final-preprecessed/KMITL-TEP PILOT-1.png\" # â¬…ï¸ à¸¥à¸­à¸‡à¹ƒà¸Šà¹‰à¸£à¸¹à¸›à¸—à¸µà¹ˆ 10 à¹€à¸›à¹‡à¸™à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡\n",
        "\n",
        "# 2. à¸à¸³à¸«à¸™à¸”à¸à¸´à¸à¸±à¸” ROI à¸—à¸µà¹ˆà¸„à¸¸à¸“à¹ƒà¸Šà¹‰à¸­à¸¢à¸¹à¹ˆà¸›à¸±à¸ˆà¸ˆà¸¸à¸šà¸±à¸™\n",
        "ROI_CONFIG = {\n",
        "    \"name\":           (200, 485, 600, 65),\n",
        "    \"application_no\": (1100, 485, 500, 60),\n",
        "    \"test_date\":      (1310, 545, 400, 60),\n",
        "    \"total\":          (820, 625, 800, 100),\n",
        "    \"grammar\":        (320, 785, 500, 80),\n",
        "    \"reading\":        (1050, 785, 500, 80),\n",
        "    \"speaking\":       (320, 900, 500, 80),\n",
        "    \"writing\":        (1050, 900, 500, 80),\n",
        "}\n",
        "\n",
        "# 3. à¹‚à¸«à¸¥à¸”à¹‚à¸¡à¹€à¸”à¸¥à¹à¸¥à¸°à¸£à¸¹à¸›à¸ à¸²à¸\n",
        "reader = easyocr.Reader(['en'])\n",
        "image = cv2.imread(IMAGE_PATH_TO_DEBUG)\n",
        "image_with_boxes = image.copy()\n",
        "\n",
        "print(\"--- ğŸ•µï¸â€â™‚ï¸ à¸œà¸¥à¸à¸²à¸£à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š ROI à¹à¸•à¹ˆà¸¥à¸°à¸Šà¹ˆà¸­à¸‡ ---\")\n",
        "\n",
        "# 4. à¸§à¸™à¸¥à¸¹à¸›à¹€à¸à¸·à¹ˆà¸­à¸§à¸²à¸”à¸à¸¥à¹ˆà¸­à¸‡à¹à¸¥à¸°à¸ªà¸à¸±à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¸¥à¸°à¸Šà¹ˆà¸­à¸‡\n",
        "for field_name, (x, y, w, h) in ROI_CONFIG.items():\n",
        "    # à¸•à¸±à¸”à¸ à¸²à¸à¸•à¸²à¸¡à¸à¸´à¸à¸±à¸”\n",
        "    roi_image = image[y:y+h, x:x+w]\n",
        "\n",
        "    # à¸¥à¸­à¸‡à¸­à¹ˆà¸²à¸™à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸ˆà¸²à¸à¸Šà¹ˆà¸­à¸‡à¸™à¸µà¹‰à¸”à¹‰à¸§à¸¢ EasyOCR\n",
        "    ocr_result = reader.readtext(roi_image, detail=0, paragraph=True)\n",
        "    text = \" \".join(ocr_result).strip()\n",
        "\n",
        "    # à¸à¸´à¸¡à¸à¹Œà¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œà¸—à¸µà¹ˆà¹„à¸”à¹‰à¸ˆà¸²à¸à¸à¸¥à¹ˆà¸­à¸‡à¸™à¸µà¹‰\n",
        "    print(f\"  - à¸à¸¥à¹ˆà¸­à¸‡ '{field_name}': à¸­à¹ˆà¸²à¸™à¹„à¸”à¹‰ -> '{text}'\")\n",
        "\n",
        "    # à¸§à¸²à¸”à¸à¸¥à¹ˆà¸­à¸‡à¸ªà¸µà¹€à¸‚à¸µà¸¢à¸§à¸¥à¸‡à¸šà¸™à¸ à¸²à¸à¹€à¸à¸·à¹ˆà¸­à¸”à¸¹à¸•à¸³à¹à¸«à¸™à¹ˆà¸‡\n",
        "    cv2.rectangle(image_with_boxes, (x, y), (x + w, y + h), (0, 255, 0), 3)\n",
        "    cv2.putText(image_with_boxes, field_name, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (36, 255, 12), 3)\n",
        "\n",
        "# 5. à¹à¸ªà¸”à¸‡à¸ à¸²à¸à¸à¸£à¹‰à¸­à¸¡à¸à¸¥à¹ˆà¸­à¸‡à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”\n",
        "plt.figure(figsize=(20, 15))\n",
        "plt.imshow(cv2.cvtColor(image_with_boxes, cv2.COLOR_BGR2RGB))\n",
        "plt.title('à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸•à¸³à¹à¸«à¸™à¹ˆà¸‡ Bounding Boxes')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "NwZhlpZph4jm"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import jiwer\n",
        "import numpy as np\n",
        "import easyocr\n",
        "\n",
        "# --- 1. à¸à¸³à¸«à¸™à¸”à¸„à¹ˆà¸²à¹à¸¥à¸° Path à¸—à¸µà¹ˆà¸ªà¸³à¸„à¸±à¸ ---\n",
        "IMAGE_DIR = \"/content/drive/MyDrive/kmitl_dataset/final-preprecessed\"\n",
        "GROUND_TRUTH_PATH = \"/content/drive/MyDrive/kmitl_dataset/dataset/à¸„à¸°à¹à¸™à¸™ TEP (Pilot Study)_Total_IT.xlsx\"\n",
        "OUTPUT_EXCEL_PATH = \"/content/drive/MyDrive/kmitl_dataset/final-excel/easyocr_full_report_with_f1.xlsx\"\n",
        "\n",
        "ROI_CONFIG = {\n",
        "    \"name\":           (200, 485, 600, 65),\n",
        "    \"application_no\": (1100, 485, 500, 60),\n",
        "    \"test_date\":      (1310, 545, 400, 60),\n",
        "    \"total\":          (820, 625, 800, 100),\n",
        "    \"grammar\":        (320, 785, 500, 80),\n",
        "    \"reading\":        (1050, 785, 500, 80),\n",
        "    \"speaking\":       (320, 900, 500, 80),\n",
        "    \"writing\":        (1050, 900, 500, 80),\n",
        "}\n",
        "\n",
        "# --- 2. à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¸Šà¹ˆà¸§à¸¢à¹€à¸«à¸¥à¸·à¸­ (à¹€à¸«à¸¡à¸·à¸­à¸™à¹€à¸”à¸´à¸¡) ---\n",
        "def parse_score(text):\n",
        "    match = re.search(r'([A-Z][1-2])\\s*\\((\\d+)\\)', text)\n",
        "    if match:\n",
        "        return match.group(1), match.group(2)\n",
        "    return None, None\n",
        "\n",
        "def get_id_from_filename(filename):\n",
        "    match = re.search(r'(\\d+)\\.(png|jpg|jpeg)$', filename.lower())\n",
        "    if match:\n",
        "        return int(match.group(1))\n",
        "    return None\n",
        "\n",
        "# --- 3. à¹‚à¸«à¸¥à¸”à¹à¸¥à¸°à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸à¹ˆà¸­à¸™à¹€à¸£à¸´à¹ˆà¸¡ (à¹€à¸«à¸¡à¸·à¸­à¸™à¹€à¸”à¸´à¸¡) ---\n",
        "print(\"--- ğŸ” à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥ ---\")\n",
        "gt_df = pd.read_excel(GROUND_TRUTH_PATH)\n",
        "ground_truth_ids = set(gt_df['No.'].unique())\n",
        "print(f\"à¸à¸š ID à¹ƒà¸™ Ground Truth à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”: {len(ground_truth_ids)} ID\")\n",
        "allowed_extensions = ['.png', '.jpg', '.jpeg']\n",
        "all_files_in_dir = os.listdir(IMAGE_DIR)\n",
        "image_files = [f for f in all_files_in_dir if f.lower().endswith(tuple(allowed_extensions))]\n",
        "image_files.sort(key=get_id_from_filename)\n",
        "# (à¸ªà¹ˆà¸§à¸™à¹‚à¸„à¹‰à¸”à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¹„à¸Ÿà¸¥à¹Œà¸—à¸µà¹ˆà¹€à¸«à¸¥à¸·à¸­à¹€à¸«à¸¡à¸·à¸­à¸™à¹€à¸”à¸´à¸¡)\n",
        "print(f\"à¸à¸šà¹„à¸Ÿà¸¥à¹Œà¸£à¸¹à¸›à¸ à¸²à¸à¹ƒà¸™à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œ: {len(image_files)} à¹„à¸Ÿà¸¥à¹Œ\")\n",
        "print(\"--- âœ… à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸ªà¸£à¹‡à¸ˆà¸ªà¸´à¹‰à¸™ ---\\n\")\n",
        "\n",
        "\n",
        "# --- 4. à¹€à¸£à¸´à¹ˆà¸¡à¸à¸£à¸°à¸šà¸§à¸™à¸à¸²à¸£à¸ªà¸à¸±à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸”à¹‰à¸§à¸¢ EasyOCR (à¹€à¸«à¸¡à¸·à¸­à¸™à¹€à¸”à¸´à¸¡) ---\n",
        "print(\"à¸à¸³à¸¥à¸±à¸‡à¹‚à¸«à¸¥à¸”à¹‚à¸¡à¹€à¸”à¸¥ EasyOCR...\")\n",
        "reader = easyocr.Reader(['en'])\n",
        "print(\"à¹‚à¸«à¸¥à¸”à¹‚à¸¡à¹€à¸”à¸¥à¸ªà¸³à¹€à¸£à¹‡à¸ˆ!\")\n",
        "\n",
        "results_list = []\n",
        "print(f\"--- ğŸš€ à¹€à¸£à¸´à¹ˆà¸¡à¸à¸²à¸£à¸ªà¸à¸±à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ˆà¸²à¸à¸£à¸¹à¸›à¸ à¸²à¸ {len(image_files)} à¹„à¸Ÿà¸¥à¹Œ à¸”à¹‰à¸§à¸¢ EasyOCR ---\")\n",
        "for filename in image_files:\n",
        "    image_id = get_id_from_filename(filename)\n",
        "    if image_id is None: continue\n",
        "    gt_row = gt_df[gt_df['No.'] == image_id]\n",
        "    if gt_row.empty: continue\n",
        "\n",
        "    print(f\"  - à¸à¸³à¸¥à¸±à¸‡à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥ ID: {image_id} ({filename})\")\n",
        "    image_path = os.path.join(IMAGE_DIR, filename)\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    extracted_raw = {}\n",
        "    for field_name, (x, y, w, h) in ROI_CONFIG.items():\n",
        "        roi_image = image[y:y+h, x:x+w]\n",
        "        gray_roi = cv2.cvtColor(roi_image, cv2.COLOR_BGR2GRAY)\n",
        "        ocr_result = reader.readtext(gray_roi, detail=0, paragraph=True)\n",
        "        text = \" \".join(ocr_result).strip()\n",
        "        extracted_raw[field_name] = text\n",
        "\n",
        "    g_level, g_score = parse_score(extracted_raw.get('grammar', ''))\n",
        "    r_level, r_score = parse_score(extracted_raw.get('reading', ''))\n",
        "    s_level, s_score = parse_score(extracted_raw.get('speaking', ''))\n",
        "    w_level, w_score = parse_score(extracted_raw.get('writing', ''))\n",
        "    t_level, t_score = parse_score(extracted_raw.get('total', ''))\n",
        "    results_list.append({\n",
        "        \"No.\": image_id,\n",
        "        \"Application No. (GT)\": gt_row.iloc[0][\"Application No.\"], \"Application No. (Pred)\": extracted_raw.get('application_no'),\n",
        "        \"Name (GT)\": gt_row.iloc[0][\"Name\"], \"Name (Pred)\": extracted_raw.get('name'),\n",
        "        \"Grammar_Level (GT)\": gt_row.iloc[0][\"Grammar_Level\"], \"Grammar_Level (Pred)\": g_level,\n",
        "        \"Grammar_Score (GT)\": gt_row.iloc[0][\"Grammar_Score\"], \"Grammar_Score (Pred)\": g_score,\n",
        "        \"Reading_Level (GT)\": gt_row.iloc[0][\"Reading_Level\"], \"Reading_Level (Pred)\": r_level,\n",
        "        \"Reading_Score (GT)\": gt_row.iloc[0][\"Reading_Score\"], \"Reading_Score (Pred)\": r_score,\n",
        "        \"Speaking_Level (GT)\": gt_row.iloc[0][\"Speaking_Level\"], \"Speaking_Level (Pred)\": s_level,\n",
        "        \"Speaking_Score (GT)\": gt_row.iloc[0][\"Speaking_Score\"], \"Speaking_Score (Pred)\": s_score,\n",
        "        \"Writing_Level (GT)\": gt_row.iloc[0][\"Writing_Level\"], \"Writing_Level (Pred)\": w_level,\n",
        "        \"Writing_Score (GT)\": gt_row.iloc[0][\"Writing_Score\"], \"Writing_Score (Pred)\": w_score,\n",
        "        \"Total_Level (GT)\": gt_row.iloc[0][\"Total_Level\"], \"Total_Level (Pred)\": t_level,\n",
        "        \"Total_Score (GT)\": gt_row.iloc[0][\"Total_Score\"], \"Total_Score (Pred)\": t_score,\n",
        "        \"Test Date (GT)\": gt_row.iloc[0][\"Test Date\"], \"Test Date (Pred)\": extracted_raw.get('test_date'),\n",
        "    })\n",
        "\n",
        "ocr_results_df = pd.DataFrame(results_list)\n",
        "print(\"--- âœ… à¸ªà¸à¸±à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸ªà¸£à¹‡à¸ˆà¸ªà¸´à¹‰à¸™ ---\\n\")\n",
        "\n",
        "\n",
        "# --- 5. à¹€à¸£à¸´à¹ˆà¸¡à¸à¸£à¸°à¸šà¸§à¸™à¸à¸²à¸£à¸›à¸£à¸°à¹€à¸¡à¸´à¸™à¸œà¸¥ (Evaluation) ---\n",
        "print(\"--- ğŸ“Š à¹€à¸£à¸´à¹ˆà¸¡à¸à¸²à¸£à¸›à¸£à¸°à¹€à¸¡à¸´à¸™à¸œà¸¥ ---\")\n",
        "eval_df = ocr_results_df.fillna('')\n",
        "\n",
        "# â¬‡ï¸ 1. à¸­à¸±à¸›à¹€à¸”à¸• fields_to_evaluate à¹ƒà¸«à¹‰à¸„à¸£à¸­à¸šà¸„à¸¥à¸¸à¸¡à¸—à¸¸à¸à¸Ÿà¸´à¸¥à¸”à¹Œ\n",
        "fields_to_evaluate = {\n",
        "    'Name': ('Name (GT)', 'Name (Pred)'),\n",
        "    'Application No.': ('Application No. (GT)', 'Application No. (Pred)'),\n",
        "    'Test Date': ('Test Date (GT)', 'Test Date (Pred)'),\n",
        "    'Grammar_Level': ('Grammar_Level (GT)', 'Grammar_Level (Pred)'),\n",
        "    'Grammar_Score': ('Grammar_Score (GT)', 'Grammar_Score (Pred)'),\n",
        "    'Reading_Level': ('Reading_Level (GT)', 'Reading_Level (Pred)'),\n",
        "    'Reading_Score': ('Reading_Score (GT)', 'Reading_Score (Pred)'),\n",
        "    'Speaking_Level': ('Speaking_Level (GT)', 'Speaking_Level (Pred)'),\n",
        "    'Speaking_Score': ('Speaking_Score (GT)', 'Speaking_Score (Pred)'),\n",
        "    'Writing_Level': ('Writing_Level (GT)', 'Writing_Level (Pred)'),\n",
        "    'Writing_Score': ('Writing_Score (GT)', 'Writing_Score (Pred)'),\n",
        "    'Total_Level': ('Total_Level (GT)', 'Total_Level (Pred)'),\n",
        "    'Total_Score': ('Total_Score (GT)', 'Total_Score (Pred)'),\n",
        "}\n",
        "\n",
        "evaluation_summary_list = []\n",
        "for field, (gt_col, pred_col) in fields_to_evaluate.items():\n",
        "    ground_truth = eval_df[gt_col].astype(str).tolist()\n",
        "    prediction = eval_df[pred_col].astype(str).tolist()\n",
        "\n",
        "    accuracy = np.mean([1 if gt == pred else 0 for gt, pred in zip(ground_truth, prediction)]) * 100\n",
        "\n",
        "    # â¬‡ï¸ 2. à¸„à¸³à¸™à¸§à¸“ Metrics à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸” à¸£à¸§à¸¡à¸–à¸¶à¸‡ F1-Score\n",
        "    error_metrics = jiwer.compute_measures(ground_truth, prediction)\n",
        "\n",
        "    # à¸”à¸¶à¸‡à¸„à¹ˆà¸²à¸•à¹ˆà¸²à¸‡à¹† à¸­à¸­à¸à¸¡à¸²à¸­à¸¢à¹ˆà¸²à¸‡à¸›à¸¥à¸­à¸”à¸ à¸±à¸¢à¸”à¹‰à¸§à¸¢ .get() à¹€à¸à¸·à¹ˆà¸­à¸›à¹‰à¸­à¸‡à¸à¸±à¸™ KeyError\n",
        "    wer = error_metrics.get('wer', 0) * 100\n",
        "    cer = error_metrics.get('cer', 0) * 100\n",
        "\n",
        "    # à¸„à¸³à¸™à¸§à¸“ Precision, Recall, à¹à¸¥à¸° F1-score\n",
        "    H = error_metrics.get('hits', 0)\n",
        "    I = error_metrics.get('insertions', 0)\n",
        "    D = error_metrics.get('deletions', 0)\n",
        "\n",
        "    precision = H / (H + I) if (H + I) > 0 else 0\n",
        "    recall = H / (H + D) if (H + D) > 0 else 0\n",
        "    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    evaluation_summary_list.append({\n",
        "        'Field': field,\n",
        "        'Accuracy (%)': round(accuracy, 2),\n",
        "        'WER (%)': round(wer, 2),\n",
        "        'CER (%)': round(cer, 2),\n",
        "        'F1-score (%)': round(f1_score * 100, 2) # â¬…ï¸ 3. à¹€à¸à¸´à¹ˆà¸¡ F1-score à¹ƒà¸™à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œ\n",
        "    })\n",
        "\n",
        "eval_summary_df = pd.DataFrame(evaluation_summary_list)\n",
        "print(eval_summary_df.to_string(index=False))\n",
        "print(\"--- âœ… à¸›à¸£à¸°à¹€à¸¡à¸´à¸™à¸œà¸¥à¹€à¸ªà¸£à¹‡à¸ˆà¸ªà¸´à¹‰à¸™ ---\\n\")\n",
        "\n",
        "\n",
        "# --- 6. à¸šà¸±à¸™à¸—à¸¶à¸à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œà¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”à¸¥à¸‡à¹„à¸Ÿà¸¥à¹Œ Excel à¹€à¸”à¸µà¸¢à¸§ (2 à¸Šà¸µà¸•) ---\n",
        "print(f\"--- ğŸ’¾ à¸à¸³à¸¥à¸±à¸‡à¸šà¸±à¸™à¸—à¸¶à¸à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œà¸¥à¸‡à¹ƒà¸™à¹„à¸Ÿà¸¥à¹Œ: {OUTPUT_EXCEL_PATH} ---\")\n",
        "with pd.ExcelWriter(OUTPUT_EXCEL_PATH, engine='openpyxl') as writer:\n",
        "    ocr_results_df.to_excel(writer, sheet_name='Detailed_Results', index=False)\n",
        "    eval_summary_df.to_excel(writer, sheet_name='Evaluation_Summary', index=False)\n",
        "print(f\"ğŸ‰ à¸šà¸±à¸™à¸—à¸¶à¸à¹„à¸Ÿà¸¥à¹Œà¸ªà¸³à¹€à¸£à¹‡à¸ˆ! à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œà¹„à¸”à¹‰à¸—à¸µà¹ˆ '{OUTPUT_EXCEL_PATH}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8FJSit88A_r"
      },
      "source": [
        "à¸™à¸µà¹ˆà¸„à¸·à¸­à¸ˆà¸¸à¸”à¸—à¸µà¹ˆà¸œà¸´à¸”à¸à¸¥à¸²à¸”:\n",
        "\n",
        "1. EasyOCR à¸­à¹ˆà¸²à¸™à¸œà¸´à¸”: à¹‚à¸¡à¹€à¸”à¸¥ OCR à¸¡à¸±à¸à¸ˆà¸°à¸ªà¸±à¸šà¸ªà¸™à¸£à¸°à¸«à¸§à¹ˆà¸²à¸‡à¸•à¸±à¸§à¹€à¸¥à¸‚ 1 à¸à¸±à¸šà¸•à¸±à¸§à¸­à¸±à¸à¸©à¸£ l (L à¸à¸´à¸¡à¸à¹Œà¹€à¸¥à¹‡à¸) à¸«à¸£à¸·à¸­ I (i à¸à¸´à¸¡à¸à¹Œà¹ƒà¸«à¸à¹ˆ) à¹ƒà¸™à¸à¸£à¸“à¸µà¸™à¸µà¹‰ à¸¡à¸±à¸™à¸­à¹ˆà¸²à¸™ B1 à¹€à¸›à¹‡à¸™ Bl\n",
        "2. à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™ parse_score à¹€à¸‚à¹‰à¸¡à¸‡à¸§à¸”à¹€à¸à¸´à¸™à¹„à¸›: à¹‚à¸„à¹‰à¸”à¹ƒà¸™à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™ parse_score à¸‚à¸­à¸‡à¹€à¸£à¸²à¹ƒà¸Šà¹‰\n",
        "Regular Expression r'([A-Z][1-2])' à¸‹à¸¶à¹ˆà¸‡à¸«à¸¡à¸²à¸¢à¸„à¸§à¸²à¸¡à¸§à¹ˆà¸²à¸¡à¸±à¸™à¸¡à¸­à¸‡à¸«à¸² \"à¸•à¸±à¸§à¸­à¸±à¸à¸©à¸£à¸à¸´à¸¡à¸à¹Œà¹ƒà¸«à¸à¹ˆ A-Z\" à¸•à¸²à¸¡à¸”à¹‰à¸§à¸¢ \"à¸•à¸±à¸§à¹€à¸¥à¸‚ 1 à¸«à¸£à¸·à¸­ 2\" à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™\n",
        "3. à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œ: à¹€à¸¡à¸·à¹ˆà¸­ parse_score à¹„à¸”à¹‰à¸£à¸±à¸šà¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡ 'Bl (17)' à¹€à¸‚à¹‰à¸²à¸¡à¸² à¸¡à¸±à¸™à¹„à¸¡à¹ˆà¸•à¸£à¸‡à¸à¸±à¸šà¹à¸à¸—à¹€à¸—à¸´à¸£à¹Œà¸™à¸—à¸µà¹ˆà¸à¸³à¸«à¸™à¸” (à¹€à¸à¸£à¸²à¸° l à¹„à¸¡à¹ˆà¹ƒà¸Šà¹ˆ 1 à¸«à¸£à¸·à¸­ 2) à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¸ˆà¸¶à¸‡à¸„à¸·à¸™à¸„à¹ˆà¸² (None, None) à¸­à¸­à¸à¹„à¸› à¸—à¸³à¹ƒà¸«à¹‰à¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œ (Pred) à¹ƒà¸™ Excel à¸‚à¸­à¸‡à¸„à¸¸à¸“à¸§à¹ˆà¸²à¸‡à¹€à¸›à¸¥à¹ˆà¸² à¹à¸¥à¸°à¸—à¸³à¹ƒà¸«à¹‰à¸à¸²à¸£à¸›à¸£à¸°à¹€à¸¡à¸´à¸™à¸œà¸¥à¸£à¸§à¸™à¹„à¸›à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”à¸„à¸£à¸±à¸š"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "0wdnOwGd8ba5"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import jiwer\n",
        "import numpy as np\n",
        "import easyocr\n",
        "\n",
        "# --- 1. à¸à¸³à¸«à¸™à¸”à¸„à¹ˆà¸²à¹à¸¥à¸° Path à¸—à¸µà¹ˆà¸ªà¸³à¸„à¸±à¸ ---\n",
        "IMAGE_DIR = \"/content/drive/MyDrive/kmitl_dataset/final-preprecessed\"\n",
        "GROUND_TRUTH_PATH = \"/content/drive/MyDrive/kmitl_dataset/dataset/à¸„à¸°à¹à¸™à¸™ TEP (Pilot Study)_Total_IT.xlsx\"\n",
        "OUTPUT_EXCEL_PATH = \"/content/drive/MyDrive/kmitl_dataset/final-excel/easyocr_final_report_single_sheet.xlsx\"\n",
        "\n",
        "ROI_CONFIG = {\n",
        "    \"name\":           (200, 485, 600, 65),\n",
        "    \"application_no\": (1100, 485, 500, 60),\n",
        "    \"test_date\":      (1310, 545, 400, 60),\n",
        "    \"total\":          (820, 625, 800, 100),\n",
        "    \"grammar\":        (320, 785, 500, 80),\n",
        "    \"reading\":        (1050, 785, 500, 80),\n",
        "    \"speaking\":       (320, 900, 500, 80),\n",
        "    \"writing\":        (1050, 900, 500, 80),\n",
        "}\n",
        "\n",
        "# --- 2. à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¸Šà¹ˆà¸§à¸¢à¹€à¸«à¸¥à¸·à¸­ (à¸‰à¸šà¸±à¸šà¹à¸à¹‰à¹„à¸‚) ---\n",
        "def parse_score(text):\n",
        "    if not text: return None, None\n",
        "    processed_text = text.replace('l', '1').replace('I', '1')\n",
        "    match = re.search(r'([A-Z][1-2])\\s*\\((\\d+)\\)', processed_text, re.IGNORECASE)\n",
        "    if match:\n",
        "        level = match.group(1).upper()\n",
        "        score = match.group(2)\n",
        "        return level, score\n",
        "    return None, None\n",
        "\n",
        "def get_id_from_filename(filename):\n",
        "    match = re.search(r'(\\d+)\\.(png|jpg|jpeg)$', filename.lower())\n",
        "    if match: return int(match.group(1))\n",
        "    return None\n",
        "\n",
        "# --- 3. à¹‚à¸«à¸¥à¸”à¹à¸¥à¸°à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸à¹ˆà¸­à¸™à¹€à¸£à¸´à¹ˆà¸¡ (à¹€à¸«à¸¡à¸·à¸­à¸™à¹€à¸”à¸´à¸¡) ---\n",
        "print(\"--- ğŸ” à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥ ---\")\n",
        "gt_df = pd.read_excel(GROUND_TRUTH_PATH)\n",
        "ground_truth_ids = set(gt_df['No.'].unique())\n",
        "print(f\"à¸à¸š ID à¹ƒà¸™ Ground Truth à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”: {len(ground_truth_ids)} ID\")\n",
        "allowed_extensions = ['.png', '.jpg', '.jpeg']\n",
        "all_files_in_dir = os.listdir(IMAGE_DIR)\n",
        "image_files = [f for f in all_files_in_dir if f.lower().endswith(tuple(allowed_extensions))]\n",
        "image_files.sort(key=get_id_from_filename)\n",
        "print(f\"à¸à¸šà¹„à¸Ÿà¸¥à¹Œà¸£à¸¹à¸›à¸ à¸²à¸à¹ƒà¸™à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œ: {len(image_files)} à¹„à¸Ÿà¸¥à¹Œ\")\n",
        "print(\"--- âœ… à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸ªà¸£à¹‡à¸ˆà¸ªà¸´à¹‰à¸™ ---\\n\")\n",
        "\n",
        "# --- 4. à¹€à¸£à¸´à¹ˆà¸¡à¸à¸£à¸°à¸šà¸§à¸™à¸à¸²à¸£à¸ªà¸à¸±à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸”à¹‰à¸§à¸¢ EasyOCR (à¹€à¸«à¸¡à¸·à¸­à¸™à¹€à¸”à¸´à¸¡) ---\n",
        "print(\"à¸à¸³à¸¥à¸±à¸‡à¹‚à¸«à¸¥à¸”à¹‚à¸¡à¹€à¸”à¸¥ EasyOCR...\")\n",
        "reader = easyocr.Reader(['en'])\n",
        "print(\"à¹‚à¸«à¸¥à¸”à¹‚à¸¡à¹€à¸”à¸¥à¸ªà¸³à¹€à¸£à¹‡à¸ˆ!\")\n",
        "\n",
        "results_list = []\n",
        "print(f\"--- ğŸš€ à¹€à¸£à¸´à¹ˆà¸¡à¸à¸²à¸£à¸ªà¸à¸±à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ˆà¸²à¸à¸£à¸¹à¸›à¸ à¸²à¸ {len(image_files)} à¹„à¸Ÿà¸¥à¹Œ à¸”à¹‰à¸§à¸¢ EasyOCR ---\")\n",
        "for filename in image_files:\n",
        "    image_id = get_id_from_filename(filename)\n",
        "    if image_id is None: continue\n",
        "    gt_row = gt_df[gt_df['No.'] == image_id]\n",
        "    if gt_row.empty: continue\n",
        "\n",
        "    print(f\"  - à¸à¸³à¸¥à¸±à¸‡à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥ ID: {image_id} ({filename})\")\n",
        "    image_path = os.path.join(IMAGE_DIR, filename)\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    extracted_raw = {}\n",
        "    for field_name, (x, y, w, h) in ROI_CONFIG.items():\n",
        "        roi_image = image[y:y+h, x:x+w]\n",
        "        gray_roi = cv2.cvtColor(roi_image, cv2.COLOR_BGR2GRAY)\n",
        "        ocr_result = reader.readtext(gray_roi, detail=0, paragraph=True)\n",
        "        text = \" \".join(ocr_result).strip()\n",
        "        extracted_raw[field_name] = text\n",
        "\n",
        "    g_level, g_score = parse_score(extracted_raw.get('grammar'))\n",
        "    r_level, r_score = parse_score(extracted_raw.get('reading'))\n",
        "    s_level, s_score = parse_score(extracted_raw.get('speaking'))\n",
        "    w_level, w_score = parse_score(extracted_raw.get('writing'))\n",
        "    t_level, t_score = parse_score(extracted_raw.get('total'))\n",
        "\n",
        "    results_list.append({\n",
        "        \"No.\": image_id,\n",
        "        \"Application No. (GT)\": gt_row.iloc[0][\"Application No.\"], \"Application No. (Pred)\": extracted_raw.get('application_no'),\n",
        "        \"Name (GT)\": gt_row.iloc[0][\"Name\"], \"Name (Pred)\": extracted_raw.get('name'),\n",
        "        \"Grammar_Level (GT)\": gt_row.iloc[0][\"Grammar_Level\"], \"Grammar_Level (Pred)\": g_level,\n",
        "        \"Grammar_Score (GT)\": gt_row.iloc[0][\"Grammar_Score\"], \"Grammar_Score (Pred)\": g_score,\n",
        "        \"Reading_Level (GT)\": gt_row.iloc[0][\"Reading_Level\"], \"Reading_Level (Pred)\": r_level,\n",
        "        \"Reading_Score (GT)\": gt_row.iloc[0][\"Reading_Score\"], \"Reading_Score (Pred)\": r_score,\n",
        "        \"Speaking_Level (GT)\": gt_row.iloc[0][\"Speaking_Level\"], \"Speaking_Level (Pred)\": s_level,\n",
        "        \"Speaking_Score (GT)\": gt_row.iloc[0][\"Speaking_Score\"], \"Speaking_Score (Pred)\": s_score,\n",
        "        \"Writing_Level (GT)\": gt_row.iloc[0][\"Writing_Level\"], \"Writing_Level (Pred)\": w_level,\n",
        "        \"Writing_Score (GT)\": gt_row.iloc[0][\"Writing_Score\"], \"Writing_Score (Pred)\": w_score,\n",
        "        \"Total_Level (GT)\": gt_row.iloc[0][\"Total_Level\"], \"Total_Level (Pred)\": t_level,\n",
        "        \"Total_Score (GT)\": gt_row.iloc[0][\"Total_Score\"], \"Total_Score (Pred)\": t_score,\n",
        "        \"Test Date (GT)\": gt_row.iloc[0][\"Test Date\"], \"Test Date (Pred)\": extracted_raw.get('test_date'),\n",
        "    })\n",
        "\n",
        "ocr_results_df = pd.DataFrame(results_list)\n",
        "print(\"--- âœ… à¸ªà¸à¸±à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸ªà¸£à¹‡à¸ˆà¸ªà¸´à¹‰à¸™ ---\\n\")\n",
        "\n",
        "\n",
        "# --- 5. à¹€à¸£à¸´à¹ˆà¸¡à¸à¸£à¸°à¸šà¸§à¸™à¸à¸²à¸£à¸›à¸£à¸°à¹€à¸¡à¸´à¸™à¸œà¸¥ (Evaluation) - (à¸™à¸³ F1-Score à¸à¸¥à¸±à¸šà¸¡à¸²) ---\n",
        "print(\"--- ğŸ“Š à¹€à¸£à¸´à¹ˆà¸¡à¸à¸²à¸£à¸›à¸£à¸°à¹€à¸¡à¸´à¸™à¸œà¸¥ ---\")\n",
        "\n",
        "def normalize_text(text):\n",
        "    if text is None: return \"\"\n",
        "    text = str(text).strip().lower()\n",
        "    if text.endswith('.0'): text = text[:-2]\n",
        "    return text\n",
        "\n",
        "eval_df = ocr_results_df.fillna('')\n",
        "\n",
        "fields_to_evaluate = {\n",
        "    'Name': ('Name (GT)', 'Name (Pred)'), 'Application No.': ('Application No. (GT)', 'Application No. (Pred)'),\n",
        "    'Test Date': ('Test Date (GT)', 'Test Date (Pred)'), 'Grammar_Level': ('Grammar_Level (GT)', 'Grammar_Level (Pred)'),\n",
        "    'Grammar_Score': ('Grammar_Score (GT)', 'Grammar_Score (Pred)'), 'Reading_Level': ('Reading_Level (GT)', 'Reading_Level (Pred)'),\n",
        "    'Reading_Score': ('Reading_Score (GT)', 'Reading_Score (Pred)'), 'Speaking_Level': ('Speaking_Level (GT)', 'Speaking_Level (Pred)'),\n",
        "    'Speaking_Score': ('Speaking_Score (GT)', 'Speaking_Score (Pred)'), 'Writing_Level': ('Writing_Level (GT)', 'Writing_Level (Pred)'),\n",
        "    'Writing_Score': ('Writing_Score (GT)', 'Writing_Score (Pred)'), 'Total_Level': ('Total_Level (GT)', 'Total_Level (Pred)'),\n",
        "    'Total_Score': ('Total_Score (GT)', 'Total_Score (Pred)'),\n",
        "}\n",
        "\n",
        "evaluation_summary_list = []\n",
        "for field, (gt_col, pred_col) in fields_to_evaluate.items():\n",
        "    ground_truth = [normalize_text(t) for t in eval_df[gt_col]]\n",
        "    prediction = [normalize_text(t) for t in eval_df[pred_col]]\n",
        "\n",
        "    accuracy = np.mean([1 if gt == pred else 0 for gt, pred in zip(ground_truth, prediction)]) * 100\n",
        "\n",
        "    # à¹ƒà¸Šà¹‰ compute_measures à¹€à¸à¸·à¹ˆà¸­à¸„à¸³à¸™à¸§à¸“à¸—à¸¸à¸à¸­à¸¢à¹ˆà¸²à¸‡à¹ƒà¸™à¸„à¸£à¸±à¹‰à¸‡à¹€à¸”à¸µà¸¢à¸§\n",
        "    error_metrics = jiwer.compute_measures(ground_truth, prediction)\n",
        "    wer = error_metrics.get('wer', 0) * 100\n",
        "    cer = error_metrics.get('cer', 0) * 100\n",
        "\n",
        "    # à¸„à¸³à¸™à¸§à¸“ F1-score à¸ˆà¸²à¸à¸­à¸‡à¸„à¹Œà¸›à¸£à¸°à¸à¸­à¸š\n",
        "    H = error_metrics.get('hits', 0)\n",
        "    I = error_metrics.get('insertions', 0)\n",
        "    D = error_metrics.get('deletions', 0)\n",
        "    S = error_metrics.get('substitutions', 0)\n",
        "\n",
        "    precision = H / (H + I + S) if (H + I + S) > 0 else 0\n",
        "    recall = H / (H + D + S) if (H + D + S) > 0 else 0\n",
        "    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    evaluation_summary_list.append({\n",
        "        'Field': field,\n",
        "        'Accuracy (%)': round(accuracy, 2),\n",
        "        'WER (%)': round(wer, 2),\n",
        "        'CER (%)': round(cer, 2),\n",
        "        'F1-score (%)': round(f1_score * 100, 2)\n",
        "    })\n",
        "\n",
        "eval_summary_df = pd.DataFrame(evaluation_summary_list)\n",
        "print(eval_summary_df.to_string(index=False))\n",
        "print(\"--- âœ… à¸›à¸£à¸°à¹€à¸¡à¸´à¸™à¸œà¸¥à¹€à¸ªà¸£à¹‡à¸ˆà¸ªà¸´à¹‰à¸™ ---\\n\")\n",
        "\n",
        "\n",
        "# â¬‡ï¸â¬‡ï¸â¬‡ï¸ --- 6. à¸šà¸±à¸™à¸—à¸¶à¸à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œà¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”à¸¥à¸‡à¹„à¸Ÿà¸¥à¹Œ Excel à¹€à¸”à¸µà¸¢à¸§ (à¸Šà¸µà¸•à¹€à¸”à¸µà¸¢à¸§) --- â¬‡ï¸â¬‡ï¸â¬‡ï¸\n",
        "print(f\"--- ğŸ’¾ à¸à¸³à¸¥à¸±à¸‡à¸šà¸±à¸™à¸—à¸¶à¸à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œà¸¥à¸‡à¹ƒà¸™à¹„à¸Ÿà¸¥à¹Œ: {OUTPUT_EXCEL_PATH} ---\")\n",
        "SHEET_NAME = 'EasyOCR_Result'\n",
        "\n",
        "with pd.ExcelWriter(OUTPUT_EXCEL_PATH, engine='openpyxl') as writer:\n",
        "    # à¹€à¸‚à¸µà¸¢à¸™à¸•à¸²à¸£à¸²à¸‡à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œà¹à¸šà¸šà¸¥à¸°à¹€à¸­à¸µà¸¢à¸”à¸à¹ˆà¸­à¸™\n",
        "    ocr_results_df.to_excel(writer, sheet_name=SHEET_NAME, index=False)\n",
        "\n",
        "    # à¸„à¸³à¸™à¸§à¸“à¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œà¹€à¸£à¸´à¹ˆà¸¡à¸•à¹‰à¸™à¸ªà¸³à¸«à¸£à¸±à¸šà¸•à¸²à¸£à¸²à¸‡à¸ªà¸£à¸¸à¸› (à¹€à¸§à¹‰à¸™ 2 à¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œ)\n",
        "    start_col_for_summary = ocr_results_df.shape[1] + 2\n",
        "\n",
        "    # à¹€à¸‚à¸µà¸¢à¸™à¸•à¸²à¸£à¸²à¸‡à¸ªà¸£à¸¸à¸›à¸œà¸¥à¸¥à¸‡à¹ƒà¸™à¸Šà¸µà¸•à¹€à¸”à¸µà¸¢à¸§à¸à¸±à¸™\n",
        "    eval_summary_df.to_excel(writer, sheet_name=SHEET_NAME, index=False, startcol=start_col_for_summary)\n",
        "\n",
        "print(f\"ğŸ‰ à¸šà¸±à¸™à¸—à¸¶à¸à¹„à¸Ÿà¸¥à¹Œà¸ªà¸³à¹€à¸£à¹‡à¸ˆ! à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œà¹„à¸”à¹‰à¸—à¸µà¹ˆ '{OUTPUT_EXCEL_PATH}' à¹ƒà¸™à¸Šà¸µà¸• '{SHEET_NAME}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aH8k_NW1Ay5S"
      },
      "source": [
        "**Final Update to master file**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "CAEhhLZtL0xs"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import jiwer\n",
        "import numpy as np\n",
        "import easyocr\n",
        "\n",
        "# --- 1. à¸à¸³à¸«à¸™à¸”à¸„à¹ˆà¸²à¹à¸¥à¸° Path à¸—à¸µà¹ˆà¸ªà¸³à¸„à¸±à¸ ---\n",
        "MODEL_NAME = 'EasyOCR'\n",
        "APPROACH_NAME = 'Boxing'\n",
        "\n",
        "# â¬‡ï¸ Path à¸ªà¸³à¸«à¸£à¸±à¸šà¸šà¸±à¸™à¸—à¸¶à¸à¹„à¸Ÿà¸¥à¹Œà¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œà¸‚à¸­à¸‡à¹‚à¸¡à¹€à¸”à¸¥à¸™à¸µà¹‰à¹‚à¸”à¸¢à¹€à¸‰à¸à¸²à¸° (2 à¸Šà¸µà¸•)\n",
        "INDIVIDUAL_REPORT_PATH = \"/content/drive/MyDrive/kmitl_dataset/final-excel/easyocr_boxing_report.xlsx\"\n",
        "# â¬‡ï¸ Path à¸ªà¸³à¸«à¸£à¸±à¸šà¹„à¸Ÿà¸¥à¹Œ Master à¸—à¸µà¹ˆà¸ˆà¸°à¸£à¸§à¸šà¸£à¸§à¸¡à¸œà¸¥à¸ˆà¸²à¸à¸—à¸¸à¸à¹‚à¸¡à¹€à¸”à¸¥\n",
        "MASTER_OUTPUT_PATH = \"/content/drive/MyDrive/kmitl_dataset/final-excel/final_comparison_report.xlsx\"\n",
        "\n",
        "IMAGE_DIR = \"/content/drive/MyDrive/kmitl_dataset/final-preprecessed\"\n",
        "GROUND_TRUTH_PATH = \"/content/drive/MyDrive/kmitl_dataset/dataset/à¸„à¸°à¹à¸™à¸™ TEP (Pilot Study)_Total_IT.xlsx\"\n",
        "\n",
        "ROI_CONFIG = {\n",
        "    \"name\":           (200, 485, 600, 65),\n",
        "    \"application_no\": (1100, 485, 500, 60),\n",
        "    \"test_date\":      (1310, 545, 400, 60),\n",
        "    \"total\":          (820, 625, 800, 100),\n",
        "    \"grammar\":        (320, 785, 500, 80),\n",
        "    \"reading\":        (1050, 785, 500, 80),\n",
        "    \"speaking\":       (320, 900, 500, 80),\n",
        "    \"writing\":        (1050, 900, 500, 80),\n",
        "}\n",
        "\n",
        "# --- 2. à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¸Šà¹ˆà¸§à¸¢à¹€à¸«à¸¥à¸·à¸­ (à¸‰à¸šà¸±à¸šà¸ªà¸¡à¸šà¸¹à¸£à¸“à¹Œ) ---\n",
        "def parse_score(text):\n",
        "    if not text: return None, None\n",
        "    processed_text = text.replace('l', '1').replace('I', '1')\n",
        "    match = re.search(r'([A-Z][1-2])\\s*\\((\\d+)\\)', processed_text, re.IGNORECASE)\n",
        "    if match: return match.group(1).upper(), match.group(2)\n",
        "    return None, None\n",
        "\n",
        "def get_id_from_filename(filename):\n",
        "    match = re.search(r'(\\d+)\\.(png|jpg|jpeg)$', filename.lower())\n",
        "    if match: return int(match.group(1))\n",
        "    return None\n",
        "\n",
        "def normalize_text(text):\n",
        "    if text is None: return \"\"\n",
        "    text = str(text).strip().lower()\n",
        "    if text.endswith('.0'): text = text[:-2]\n",
        "    return text\n",
        "\n",
        "# --- 3. & 4. à¹‚à¸«à¸¥à¸”à¹à¸¥à¸°à¸ªà¸à¸±à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ (à¹€à¸«à¸¡à¸·à¸­à¸™à¹€à¸”à¸´à¸¡) ---\n",
        "# ... (à¹‚à¸„à¹‰à¸”à¸ªà¹ˆà¸§à¸™à¸™à¸µà¹‰à¹„à¸¡à¹ˆà¸¡à¸µà¸à¸²à¸£à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™à¹à¸›à¸¥à¸‡) ...\n",
        "print(\"--- ğŸ” à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥ ---\")\n",
        "gt_df = pd.read_excel(GROUND_TRUTH_PATH)\n",
        "ground_truth_ids = set(gt_df['No.'].unique())\n",
        "print(f\"à¸à¸š ID à¹ƒà¸™ Ground Truth à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”: {len(ground_truth_ids)} ID\")\n",
        "allowed_extensions = ['.png', '.jpg', '.jpeg']\n",
        "all_files_in_dir = os.listdir(IMAGE_DIR)\n",
        "image_files = [f for f in all_files_in_dir if f.lower().endswith(tuple(allowed_extensions))]\n",
        "image_files.sort(key=get_id_from_filename)\n",
        "print(f\"à¸à¸šà¹„à¸Ÿà¸¥à¹Œà¸£à¸¹à¸›à¸ à¸²à¸à¹ƒà¸™à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œ: {len(image_files)} à¹„à¸Ÿà¸¥à¹Œ\")\n",
        "print(\"--- âœ… à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸ªà¸£à¹‡à¸ˆà¸ªà¸´à¹‰à¸™ ---\\n\")\n",
        "print(\"à¸à¸³à¸¥à¸±à¸‡à¹‚à¸«à¸¥à¸”à¹‚à¸¡à¹€à¸”à¸¥ EasyOCR...\")\n",
        "reader = easyocr.Reader(['en'])\n",
        "print(\"à¹‚à¸«à¸¥à¸”à¹‚à¸¡à¹€à¸”à¸¥à¸ªà¸³à¹€à¸£à¹‡à¸ˆ!\")\n",
        "results_list = []\n",
        "print(f\"--- ğŸš€ à¹€à¸£à¸´à¹ˆà¸¡à¸à¸²à¸£à¸ªà¸à¸±à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ˆà¸²à¸à¸£à¸¹à¸›à¸ à¸²à¸ {len(image_files)} à¹„à¸Ÿà¸¥à¹Œ à¸”à¹‰à¸§à¸¢ {MODEL_NAME} ({APPROACH_NAME}) ---\")\n",
        "# (à¸ªà¹ˆà¸§à¸™ for loop à¸ªà¸à¸±à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸«à¸¡à¸·à¸­à¸™à¹€à¸”à¸´à¸¡)\n",
        "for filename in image_files:\n",
        "    image_id = get_id_from_filename(filename)\n",
        "    if image_id is None: continue\n",
        "    gt_row = gt_df[gt_df['No.'] == image_id]\n",
        "    if gt_row.empty: continue\n",
        "    print(f\"  - à¸à¸³à¸¥à¸±à¸‡à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥ ID: {image_id} ({filename})\")\n",
        "    image_path = os.path.join(IMAGE_DIR, filename)\n",
        "    image = cv2.imread(image_path)\n",
        "    extracted_raw = {}\n",
        "    for field_name, (x, y, w, h) in ROI_CONFIG.items():\n",
        "        roi_image = image[y:y+h, x:x+w]\n",
        "        gray_roi = cv2.cvtColor(roi_image, cv2.COLOR_BGR2GRAY)\n",
        "        ocr_result = reader.readtext(gray_roi, detail=0, paragraph=True)\n",
        "        text = \" \".join(ocr_result).strip()\n",
        "        extracted_raw[field_name] = text\n",
        "    g_level, g_score = parse_score(extracted_raw.get('grammar'))\n",
        "    r_level, r_score = parse_score(extracted_raw.get('reading'))\n",
        "    s_level, s_score = parse_score(extracted_raw.get('speaking'))\n",
        "    w_level, w_score = parse_score(extracted_raw.get('writing'))\n",
        "    t_level, t_score = parse_score(extracted_raw.get('total'))\n",
        "    results_list.append({\n",
        "        \"No.\": image_id,\n",
        "        \"Application No. (GT)\": gt_row.iloc[0][\"Application No.\"], \"Application No. (Pred)\": extracted_raw.get('application_no'),\n",
        "        \"Name (GT)\": gt_row.iloc[0][\"Name\"], \"Name (Pred)\": extracted_raw.get('name'),\n",
        "        \"Test Date (GT)\": gt_row.iloc[0][\"Test Date\"], \"Test Date (Pred)\": extracted_raw.get('test_date'),\n",
        "        \"Grammar_Level (GT)\": gt_row.iloc[0][\"Grammar_Level\"], \"Grammar_Level (Pred)\": g_level,\n",
        "        \"Grammar_Score (GT)\": gt_row.iloc[0][\"Grammar_Score\"], \"Grammar_Score (Pred)\": g_score,\n",
        "        \"Reading_Level (GT)\": gt_row.iloc[0][\"Reading_Level\"], \"Reading_Level (Pred)\": r_level,\n",
        "        \"Reading_Score (GT)\": gt_row.iloc[0][\"Reading_Score\"], \"Reading_Score (Pred)\": r_score,\n",
        "        \"Speaking_Level (GT)\": gt_row.iloc[0][\"Speaking_Level\"], \"Speaking_Level (Pred)\": s_level,\n",
        "        \"Speaking_Score (GT)\": gt_row.iloc[0][\"Speaking_Score\"], \"Speaking_Score (Pred)\": s_score,\n",
        "        \"Writing_Level (GT)\": gt_row.iloc[0][\"Writing_Level\"], \"Writing_Level (Pred)\": w_level,\n",
        "        \"Writing_Score (GT)\": gt_row.iloc[0][\"Writing_Score\"], \"Writing_Score (Pred)\": w_score,\n",
        "        \"Total_Level (GT)\": gt_row.iloc[0][\"Total_Level\"], \"Total_Level (Pred)\": t_level,\n",
        "        \"Total_Score (GT)\": gt_row.iloc[0][\"Total_Score\"], \"Total_Score (Pred)\": t_score,\n",
        "    })\n",
        "ocr_results_df = pd.DataFrame(results_list)\n",
        "print(\"--- âœ… à¸ªà¸à¸±à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸ªà¸£à¹‡à¸ˆà¸ªà¸´à¹‰à¸™ ---\\n\")\n",
        "\n",
        "\n",
        "# --- 5. à¹€à¸£à¸´à¹ˆà¸¡à¸à¸£à¸°à¸šà¸§à¸™à¸à¸²à¸£à¸›à¸£à¸°à¹€à¸¡à¸´à¸™à¸œà¸¥ (Evaluation) ---\n",
        "print(\"--- ğŸ“Š à¹€à¸£à¸´à¹ˆà¸¡à¸à¸²à¸£à¸›à¸£à¸°à¹€à¸¡à¸´à¸™à¸œà¸¥ ---\")\n",
        "eval_df = ocr_results_df.fillna('')\n",
        "fields_to_evaluate = {\n",
        "    'Name': ('Name (GT)', 'Name (Pred)'), 'Application No.': ('Application No. (GT)', 'Application No. (Pred)'),\n",
        "    'Test Date': ('Test Date (GT)', 'Test Date (Pred)'), 'Grammar_Level': ('Grammar_Level (GT)', 'Grammar_Level (Pred)'),\n",
        "    'Grammar_Score': ('Grammar_Score (GT)', 'Grammar_Score (Pred)'), 'Reading_Level': ('Reading_Level (GT)', 'Reading_Level (Pred)'),\n",
        "    'Reading_Score': ('Reading_Score (GT)', 'Reading_Score (Pred)'), 'Speaking_Level': ('Speaking_Level (GT)', 'Speaking_Level (Pred)'),\n",
        "    'Speaking_Score': ('Speaking_Score (GT)', 'Speaking_Score (Pred)'), 'Writing_Level': ('Writing_Level (GT)', 'Writing_Level (Pred)'),\n",
        "    'Writing_Score': ('Writing_Score (GT)', 'Writing_Score (Pred)'), 'Total_Level': ('Total_Level (GT)', 'Total_Level (Pred)'),\n",
        "    'Total_Score': ('Total_Score (GT)', 'Total_Score (Pred)'),\n",
        "}\n",
        "\n",
        "evaluation_summary_list = []\n",
        "for field, (gt_col, pred_col) in fields_to_evaluate.items():\n",
        "    ground_truth = [normalize_text(t) for t in eval_df[gt_col]]\n",
        "    prediction = [normalize_text(t) for t in eval_df[pred_col]]\n",
        "    accuracy = np.mean([1 if gt == pred else 0 for gt, pred in zip(ground_truth, prediction)]) * 100\n",
        "    error_metrics = jiwer.compute_measures(ground_truth, prediction)\n",
        "    wer = error_metrics.get('wer', 0) * 100\n",
        "    cer = error_metrics.get('cer', 0) * 100\n",
        "    H = error_metrics.get('hits', 0); I = error_metrics.get('insertions', 0); D = error_metrics.get('deletions', 0); S = error_metrics.get('substitutions', 0)\n",
        "    precision = H / (H + I + S) if (H + I + S) > 0 else 0\n",
        "    recall = H / (H + D + S) if (H + D + S) > 0 else 0\n",
        "    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    evaluation_summary_list.append({\n",
        "        'Field': field, 'Accuracy (%)': round(accuracy, 2), 'WER (%)': round(wer, 2),\n",
        "        'CER (%)': round(cer, 2), 'F1-score (%)': round(f1_score * 100, 2)\n",
        "    })\n",
        "\n",
        "eval_summary_df = pd.DataFrame(evaluation_summary_list)\n",
        "eval_summary_df.insert(0, 'Approach', APPROACH_NAME)\n",
        "eval_summary_df.insert(0, 'Model', MODEL_NAME)\n",
        "print(eval_summary_df.to_string(index=False))\n",
        "print(\"--- âœ… à¸›à¸£à¸°à¹€à¸¡à¸´à¸™à¸œà¸¥à¹€à¸ªà¸£à¹‡à¸ˆà¸ªà¸´à¹‰à¸™ ---\\n\")\n",
        "\n",
        "\n",
        "# â¬‡ï¸â¬‡ï¸â¬‡ï¸ --- 6. à¸šà¸±à¸™à¸—à¸¶à¸à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œ (à¸—à¸³ 2 à¸­à¸¢à¹ˆà¸²à¸‡) --- â¬‡ï¸â¬‡ï¸â¬‡ï¸\n",
        "\n",
        "# --- 6.A: à¸šà¸±à¸™à¸—à¸¶à¸à¸œà¸¥à¸à¸²à¸£à¸—à¸”à¸¥à¸­à¸‡à¸™à¸µà¹‰à¹à¸¢à¸à¹„à¸Ÿà¸¥à¹Œ (2 à¸Šà¸µà¸•) ---\n",
        "print(f\"--- ğŸ’¾ à¸à¸³à¸¥à¸±à¸‡à¸šà¸±à¸™à¸—à¸¶à¸à¸£à¸²à¸¢à¸‡à¸²à¸™à¹€à¸‰à¸à¸²à¸°à¸‚à¸­à¸‡ {MODEL_NAME} à¸¥à¸‡à¹ƒà¸™à¹„à¸Ÿà¸¥à¹Œ: {INDIVIDUAL_REPORT_PATH} ---\")\n",
        "with pd.ExcelWriter(INDIVIDUAL_REPORT_PATH, engine='openpyxl') as writer:\n",
        "    ocr_results_df.to_excel(writer, sheet_name='Detailed_Results', index=False)\n",
        "    # à¸™à¸³à¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œ Model à¹à¸¥à¸° Approach à¸­à¸­à¸à¸à¹ˆà¸­à¸™à¸šà¸±à¸™à¸—à¸¶à¸à¸Šà¸µà¸•à¸ªà¸£à¸¸à¸›à¸‚à¸­à¸‡à¹„à¸Ÿà¸¥à¹Œà¸™à¸µà¹‰\n",
        "    eval_summary_df.drop(columns=['Model', 'Approach']).to_excel(writer, sheet_name='Evaluation_Summary', index=False)\n",
        "print(f\"ğŸ‰ à¸šà¸±à¸™à¸—à¸¶à¸à¹„à¸Ÿà¸¥à¹Œ {INDIVIDUAL_REPORT_PATH} à¸ªà¸³à¹€à¸£à¹‡à¸ˆ!\")\n",
        "\n",
        "# --- 6.B: à¸­à¸±à¸›à¹€à¸”à¸•à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸¥à¸‡ Master File à¸ªà¸³à¸«à¸£à¸±à¸šà¸ªà¸£à¹‰à¸²à¸‡à¸à¸£à¸²à¸Ÿ ---\n",
        "print(f\"--- ğŸ’¾ à¸à¸³à¸¥à¸±à¸‡à¸­à¸±à¸›à¹€à¸”à¸•à¹„à¸Ÿà¸¥à¹Œ Master Report: {MASTER_OUTPUT_PATH} ---\")\n",
        "SHEET_NAME = 'Master_Evaluation'\n",
        "try:\n",
        "    with pd.ExcelFile(MASTER_OUTPUT_PATH) as xls:\n",
        "        master_df = pd.read_excel(xls, sheet_name=SHEET_NAME)\n",
        "        master_df = master_df[(master_df['Model'] != MODEL_NAME) | (master_df['Approach'] != APPROACH_NAME)]\n",
        "    combined_df = pd.concat([master_df, eval_summary_df], ignore_index=True)\n",
        "except FileNotFoundError:\n",
        "    print(f\"à¹„à¸¡à¹ˆà¸à¸šà¹„à¸Ÿà¸¥à¹Œ Master à¹€à¸”à¸´à¸¡, à¸à¸³à¸¥à¸±à¸‡à¸ªà¸£à¹‰à¸²à¸‡à¹„à¸Ÿà¸¥à¹Œà¹ƒà¸«à¸¡à¹ˆ...\")\n",
        "    combined_df = eval_summary_df\n",
        "\n",
        "with pd.ExcelWriter(MASTER_OUTPUT_PATH, engine='openpyxl') as writer:\n",
        "    combined_df.to_excel(writer, sheet_name=SHEET_NAME, index=False)\n",
        "\n",
        "print(f\"ğŸ‰ à¸­à¸±à¸›à¹€à¸”à¸•à¹„à¸Ÿà¸¥à¹Œ Master Report à¸ªà¸³à¹€à¸£à¹‡à¸ˆ!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bplEBKnXiEid"
      },
      "source": [
        "# **TRocr Boxing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "0dWgsHdaNlSV"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "s6CmWx3Siyb0"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import jiwer\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
        "\n",
        "# --- 1. à¸à¸³à¸«à¸™à¸”à¸„à¹ˆà¸²à¹à¸¥à¸° Path à¸—à¸µà¹ˆà¸ªà¸³à¸„à¸±à¸ ---\n",
        "MODEL_NAME = 'TrOCR'\n",
        "APPROACH_NAME = 'Boxing'\n",
        "INDIVIDUAL_REPORT_PATH = \"/content/drive/MyDrive/kmitl_dataset/final-excel/trocr_boxing_report.xlsx\"\n",
        "MASTER_OUTPUT_PATH = \"/content/drive/MyDrive/kmitl_dataset/final-excel/final_comparison_report.xlsx\"\n",
        "\n",
        "IMAGE_DIR = \"/content/drive/MyDrive/kmitl_dataset/final-preprecessed\"\n",
        "GROUND_TRUTH_PATH = \"/content/drive/MyDrive/kmitl_dataset/dataset/à¸„à¸°à¹à¸™à¸™ TEP (Pilot Study)_Total_IT.xlsx\"\n",
        "\n",
        "ROI_CONFIG = {\n",
        "    \"name\":           (200, 485, 600, 65),\n",
        "    \"application_no\": (1100, 485, 500, 60),\n",
        "    \"test_date\":      (1310, 545, 400, 60),\n",
        "    \"total\":          (820, 625, 800, 100),\n",
        "    \"grammar\":        (320, 785, 500, 80),\n",
        "    \"reading\":        (1050, 785, 500, 80),\n",
        "    \"speaking\":       (320, 900, 500, 80),\n",
        "    \"writing\":        (1050, 900, 500, 80),\n",
        "}\n",
        "\n",
        "# --- 2. à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¸Šà¹ˆà¸§à¸¢à¹€à¸«à¸¥à¸·à¸­ (à¸‰à¸šà¸±à¸šà¸ªà¸¡à¸šà¸¹à¸£à¸“à¹Œ) ---\n",
        "def parse_score(text):\n",
        "    if not text: return None, None\n",
        "    processed_text = text.replace('l', '1').replace('I', '1')\n",
        "    match = re.search(r'([A-Z][1-2])\\s*\\((\\d+)\\)', processed_text, re.IGNORECASE)\n",
        "    if match:\n",
        "        return match.group(1).upper(), match.group(2)\n",
        "    return None, None\n",
        "\n",
        "def get_id_from_filename(filename):\n",
        "    match = re.search(r'(\\d+)\\.(png|jpg|jpeg)$', filename.lower())\n",
        "    if match: return int(match.group(1))\n",
        "    return None\n",
        "\n",
        "def normalize_text(text):\n",
        "    if text is None: return \"\"\n",
        "    text = str(text).strip().lower()\n",
        "    if text.endswith('.0'): text = text[:-2]\n",
        "    return text\n",
        "\n",
        "# --- 3. à¹‚à¸«à¸¥à¸”à¹à¸¥à¸°à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸à¹ˆà¸­à¸™à¹€à¸£à¸´à¹ˆà¸¡ ---\n",
        "# ... (à¸ªà¹ˆà¸§à¸™à¸™à¸µà¹‰à¹€à¸«à¸¡à¸·à¸­à¸™à¹€à¸”à¸´à¸¡à¸—à¸¸à¸à¸›à¸£à¸°à¸à¸²à¸£) ...\n",
        "print(\"--- ğŸ” à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥ ---\")\n",
        "gt_df = pd.read_excel(GROUND_TRUTH_PATH)\n",
        "ground_truth_ids = set(gt_df['No.'].unique())\n",
        "print(f\"à¸à¸š ID à¹ƒà¸™ Ground Truth à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”: {len(ground_truth_ids)} ID\")\n",
        "allowed_extensions = ['.png', '.jpg', '.jpeg']\n",
        "all_files_in_dir = os.listdir(IMAGE_DIR)\n",
        "image_files = [f for f in all_files_in_dir if f.lower().endswith(tuple(allowed_extensions))]\n",
        "image_files.sort(key=get_id_from_filename)\n",
        "print(f\"à¸à¸šà¹„à¸Ÿà¸¥à¹Œà¸£à¸¹à¸›à¸ à¸²à¸à¹ƒà¸™à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œ: {len(image_files)} à¹„à¸Ÿà¸¥à¹Œ\")\n",
        "print(\"--- âœ… à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸ªà¸£à¹‡à¸ˆà¸ªà¸´à¹‰à¸™ ---\\n\")\n",
        "\n",
        "\n",
        "# --- 4. à¹€à¸£à¸´à¹ˆà¸¡à¸à¸£à¸°à¸šà¸§à¸™à¸à¸²à¸£à¸ªà¸à¸±à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸”à¹‰à¸§à¸¢ TrOCR ---\n",
        "# â¬‡ï¸ à¹‚à¸«à¸¥à¸”à¹‚à¸¡à¹€à¸”à¸¥à¹à¸¥à¸° Processor à¸‚à¸­à¸‡ TrOCR (à¸—à¸³à¸„à¸£à¸±à¹‰à¸‡à¹€à¸”à¸µà¸¢à¸§)\n",
        "print(\"--- ğŸ¤– à¸à¸³à¸¥à¸±à¸‡à¹‚à¸«à¸¥à¸”à¹‚à¸¡à¹€à¸”à¸¥ TrOCR ---\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"à¸ˆà¸°à¹ƒà¸Šà¹‰ Device: {device}\")\n",
        "\n",
        "processor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-printed')\n",
        "model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed').to(device)\n",
        "print(\"--- âœ… à¹‚à¸«à¸¥à¸”à¹‚à¸¡à¹€à¸”à¸¥à¸ªà¸³à¹€à¸£à¹‡à¸ˆ! ---\\n\")\n",
        "\n",
        "results_list = []\n",
        "print(f\"--- ğŸš€ à¹€à¸£à¸´à¹ˆà¸¡à¸à¸²à¸£à¸ªà¸à¸±à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ˆà¸²à¸à¸£à¸¹à¸›à¸ à¸²à¸ {len(image_files)} à¹„à¸Ÿà¸¥à¹Œ à¸”à¹‰à¸§à¸¢ {MODEL_NAME} ({APPROACH_NAME}) ---\")\n",
        "for filename in image_files:\n",
        "    image_id = get_id_from_filename(filename)\n",
        "    if image_id is None: continue\n",
        "    gt_row = gt_df[gt_df['No.'] == image_id]\n",
        "    if gt_row.empty: continue\n",
        "\n",
        "    print(f\"  - à¸à¸³à¸¥à¸±à¸‡à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥ ID: {image_id} ({filename})\")\n",
        "    image_path = os.path.join(IMAGE_DIR, filename)\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    extracted_raw = {}\n",
        "    for field_name, (x, y, w, h) in ROI_CONFIG.items():\n",
        "        roi_image = image[y:y+h, x:x+w]\n",
        "\n",
        "        # â¬‡ï¸ à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™à¸à¸²à¸£à¸—à¸³à¸™à¸²à¸¢à¸œà¸¥à¸”à¹‰à¸§à¸¢ TrOCR\n",
        "        # TrOCR à¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡à¸à¸²à¸£ preprocessing à¹à¸šà¸šà¹€à¸à¹ˆà¸² à¹à¸•à¹ˆà¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¸ à¸²à¸à¹à¸šà¸š RGB\n",
        "        roi_rgb = cv2.cvtColor(roi_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        pixel_values = processor(images=roi_rgb, return_tensors=\"pt\").pixel_values.to(device)\n",
        "        generated_ids = model.generate(pixel_values)\n",
        "        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "        text = generated_text.strip()\n",
        "\n",
        "        extracted_raw[field_name] = text\n",
        "\n",
        "    # (à¸ªà¹ˆà¸§à¸™ Parse à¹à¸¥à¸° Append to results_list à¹€à¸«à¸¡à¸·à¸­à¸™à¹€à¸”à¸´à¸¡)\n",
        "    # ...\n",
        "    g_level, g_score = parse_score(extracted_raw.get('grammar'))\n",
        "    r_level, r_score = parse_score(extracted_raw.get('reading'))\n",
        "    s_level, s_score = parse_score(extracted_raw.get('speaking'))\n",
        "    w_level, w_score = parse_score(extracted_raw.get('writing'))\n",
        "    t_level, t_score = parse_score(extracted_raw.get('total'))\n",
        "    results_list.append({\n",
        "        \"No.\": image_id,\n",
        "        \"Application No. (GT)\": gt_row.iloc[0][\"Application No.\"], \"Application No. (Pred)\": extracted_raw.get('application_no'),\n",
        "        \"Name (GT)\": gt_row.iloc[0][\"Name\"], \"Name (Pred)\": extracted_raw.get('name'),\n",
        "        \"Test Date (GT)\": gt_row.iloc[0][\"Test Date\"], \"Test Date (Pred)\": extracted_raw.get('test_date'),\n",
        "        \"Grammar_Level (GT)\": gt_row.iloc[0][\"Grammar_Level\"], \"Grammar_Level (Pred)\": g_level,\n",
        "        \"Grammar_Score (GT)\": gt_row.iloc[0][\"Grammar_Score\"], \"Grammar_Score (Pred)\": g_score,\n",
        "        \"Reading_Level (GT)\": gt_row.iloc[0][\"Reading_Level\"], \"Reading_Level (Pred)\": r_level,\n",
        "        \"Reading_Score (GT)\": gt_row.iloc[0][\"Reading_Score\"], \"Reading_Score (Pred)\": r_score,\n",
        "        \"Speaking_Level (GT)\": gt_row.iloc[0][\"Speaking_Level\"], \"Speaking_Level (Pred)\": s_level,\n",
        "        \"Speaking_Score (GT)\": gt_row.iloc[0][\"Speaking_Score\"], \"Speaking_Score (Pred)\": s_score,\n",
        "        \"Writing_Level (GT)\": gt_row.iloc[0][\"Writing_Level\"], \"Writing_Level (Pred)\": w_level,\n",
        "        \"Writing_Score (GT)\": gt_row.iloc[0][\"Writing_Score\"], \"Writing_Score (Pred)\": w_score,\n",
        "        \"Total_Level (GT)\": gt_row.iloc[0][\"Total_Level\"], \"Total_Level (Pred)\": t_level,\n",
        "        \"Total_Score (GT)\": gt_row.iloc[0][\"Total_Score\"], \"Total_Score (Pred)\": t_score,\n",
        "    })\n",
        "\n",
        "ocr_results_df = pd.DataFrame(results_list)\n",
        "print(\"--- âœ… à¸ªà¸à¸±à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸ªà¸£à¹‡à¸ˆà¸ªà¸´à¹‰à¸™ ---\\n\")\n",
        "\n",
        "# --- 5. à¹€à¸£à¸´à¹ˆà¸¡à¸à¸£à¸°à¸šà¸§à¸™à¸à¸²à¸£à¸›à¸£à¸°à¹€à¸¡à¸´à¸™à¸œà¸¥ (Evaluation) ---\n",
        "# ... (à¸ªà¹ˆà¸§à¸™à¸™à¸µà¹‰à¹€à¸«à¸¡à¸·à¸­à¸™à¹€à¸”à¸´à¸¡à¸—à¸¸à¸à¸›à¸£à¸°à¸à¸²à¸£) ...\n",
        "print(\"--- ğŸ“Š à¹€à¸£à¸´à¹ˆà¸¡à¸à¸²à¸£à¸›à¸£à¸°à¹€à¸¡à¸´à¸™à¸œà¸¥ ---\")\n",
        "eval_df = ocr_results_df.fillna('')\n",
        "fields_to_evaluate = {\n",
        "    'Name': ('Name (GT)', 'Name (Pred)'), 'Application No.': ('Application No. (GT)', 'Application No. (Pred)'),\n",
        "    'Test Date': ('Test Date (GT)', 'Test Date (Pred)'), 'Grammar_Level': ('Grammar_Level (GT)', 'Grammar_Level (Pred)'),\n",
        "    'Grammar_Score': ('Grammar_Score (GT)', 'Grammar_Score (Pred)'), 'Reading_Level': ('Reading_Level (GT)', 'Reading_Level (Pred)'),\n",
        "    'Reading_Score': ('Reading_Score (GT)', 'Reading_Score (Pred)'), 'Speaking_Level': ('Speaking_Level (GT)', 'Speaking_Level (Pred)'),\n",
        "    'Speaking_Score': ('Speaking_Score (GT)', 'Speaking_Score (Pred)'), 'Writing_Level': ('Writing_Level (GT)', 'Writing_Level (Pred)'),\n",
        "    'Writing_Score': ('Writing_Score (GT)', 'Writing_Score (Pred)'), 'Total_Level': ('Total_Level (GT)', 'Total_Level (Pred)'),\n",
        "    'Total_Score': ('Total_Score (GT)', 'Total_Score (Pred)'),\n",
        "}\n",
        "evaluation_summary_list = []\n",
        "for field, (gt_col, pred_col) in fields_to_evaluate.items():\n",
        "    ground_truth = [normalize_text(t) for t in eval_df[gt_col]]\n",
        "    prediction = [normalize_text(t) for t in eval_df[pred_col]]\n",
        "    accuracy = np.mean([1 if gt == pred else 0 for gt, pred in zip(ground_truth, prediction)]) * 100\n",
        "    error_metrics = jiwer.compute_measures(ground_truth, prediction)\n",
        "    wer = error_metrics.get('wer', 0) * 100\n",
        "    cer = error_metrics.get('cer', 0) * 100\n",
        "    H = error_metrics.get('hits', 0); I = error_metrics.get('insertions', 0); D = error_metrics.get('deletions', 0); S = error_metrics.get('substitutions', 0)\n",
        "    precision = H / (H + I + S) if (H + I + S) > 0 else 0\n",
        "    recall = H / (H + D + S) if (H + D + S) > 0 else 0\n",
        "    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    evaluation_summary_list.append({\n",
        "        'Field': field, 'Accuracy (%)': round(accuracy, 2), 'WER (%)': round(wer, 2),\n",
        "        'CER (%)': round(cer, 2), 'F1-score (%)': round(f1_score * 100, 2)\n",
        "    })\n",
        "eval_summary_df = pd.DataFrame(evaluation_summary_list)\n",
        "eval_summary_df.insert(0, 'Approach', APPROACH_NAME)\n",
        "eval_summary_df.insert(0, 'Model', MODEL_NAME)\n",
        "print(eval_summary_df.to_string(index=False))\n",
        "print(\"--- âœ… à¸›à¸£à¸°à¹€à¸¡à¸´à¸™à¸œà¸¥à¹€à¸ªà¸£à¹‡à¸ˆà¸ªà¸´à¹‰à¸™ ---\\n\")\n",
        "\n",
        "\n",
        "# --- 6. à¸šà¸±à¸™à¸—à¸¶à¸à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œ (à¸—à¸³ 2 à¸­à¸¢à¹ˆà¸²à¸‡) ---\n",
        "# --- 6.A: à¸šà¸±à¸™à¸—à¸¶à¸à¸œà¸¥à¸à¸²à¸£à¸—à¸”à¸¥à¸­à¸‡à¸™à¸µà¹‰à¹à¸¢à¸à¹„à¸Ÿà¸¥à¹Œ (2 à¸Šà¸µà¸•) ---\n",
        "print(f\"--- ğŸ’¾ à¸à¸³à¸¥à¸±à¸‡à¸šà¸±à¸™à¸—à¸¶à¸à¸£à¸²à¸¢à¸‡à¸²à¸™à¹€à¸‰à¸à¸²à¸°à¸‚à¸­à¸‡ {MODEL_NAME} à¸¥à¸‡à¹ƒà¸™à¹„à¸Ÿà¸¥à¹Œ: {INDIVIDUAL_REPORT_PATH} ---\")\n",
        "with pd.ExcelWriter(INDIVIDUAL_REPORT_PATH, engine='openpyxl') as writer:\n",
        "    ocr_results_df.to_excel(writer, sheet_name='Detailed_Results', index=False)\n",
        "    eval_summary_df.drop(columns=['Model', 'Approach']).to_excel(writer, sheet_name='Evaluation_Summary', index=False)\n",
        "print(f\"ğŸ‰ à¸šà¸±à¸™à¸—à¸¶à¸à¹„à¸Ÿà¸¥à¹Œ {INDIVIDUAL_REPORT_PATH} à¸ªà¸³à¹€à¸£à¹‡à¸ˆ!\")\n",
        "\n",
        "# --- 6.B: à¸­à¸±à¸›à¹€à¸”à¸•à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸¥à¸‡ Master File à¸ªà¸³à¸«à¸£à¸±à¸šà¸ªà¸£à¹‰à¸²à¸‡à¸à¸£à¸²à¸Ÿ ---\n",
        "print(f\"--- ğŸ’¾ à¸à¸³à¸¥à¸±à¸‡à¸­à¸±à¸›à¹€à¸”à¸•à¹„à¸Ÿà¸¥à¹Œ Master Report: {MASTER_OUTPUT_PATH} ---\")\n",
        "SHEET_NAME = 'Master_Evaluation'\n",
        "try:\n",
        "    with pd.ExcelFile(MASTER_OUTPUT_PATH) as xls:\n",
        "        master_df = pd.read_excel(xls, sheet_name=SHEET_NAME)\n",
        "        master_df = master_df[(master_df['Model'] != MODEL_NAME) | (master_df['Approach'] != APPROACH_NAME)]\n",
        "    combined_df = pd.concat([master_df, eval_summary_df], ignore_index=True)\n",
        "except FileNotFoundError:\n",
        "    print(f\"à¹„à¸¡à¹ˆà¸à¸šà¹„à¸Ÿà¸¥à¹Œ Master à¹€à¸”à¸´à¸¡, à¸à¸³à¸¥à¸±à¸‡à¸ªà¸£à¹‰à¸²à¸‡à¹„à¸Ÿà¸¥à¹Œà¹ƒà¸«à¸¡à¹ˆ...\")\n",
        "    combined_df = eval_summary_df\n",
        "with pd.ExcelWriter(MASTER_OUTPUT_PATH, engine='openpyxl') as writer:\n",
        "    combined_df.to_excel(writer, sheet_name=SHEET_NAME, index=False)\n",
        "\n",
        "print(f\"ğŸ‰ à¸­à¸±à¸›à¹€à¸”à¸•à¹„à¸Ÿà¸¥à¹Œ Master Report à¸ªà¸³à¹€à¸£à¹‡à¸ˆ!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5YP-TqLmeY3"
      },
      "source": [
        "# Visualization only Pure OCR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9CDGQ_fmeCq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# --- 1. à¸à¸³à¸«à¸™à¸”à¸„à¹ˆà¸²à¹à¸¥à¸°à¹‚à¸«à¸¥à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ ---\n",
        "MASTER_REPORT_PATH = \"/content/drive/MyDrive/kmitl_dataset/final-excel/final_comparison_report.xlsx\"\n",
        "SHEET_NAME = 'Master_Evaluation'\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/kmitl_dataset/final-excel/graphs_pure_ocr/\" # â¬…ï¸ à¸ªà¸£à¹‰à¸²à¸‡à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œà¹ƒà¸«à¸¡à¹ˆà¸ªà¸³à¸«à¸£à¸±à¸šà¸à¸£à¸²à¸Ÿà¸Šà¸¸à¸”à¸™à¸µà¹‰\n",
        "\n",
        "# à¸ªà¸£à¹‰à¸²à¸‡à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œà¸–à¹‰à¸²à¸¢à¸±à¸‡à¹„à¸¡à¹ˆà¸¡à¸µ\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"à¸à¸³à¸¥à¸±à¸‡à¹‚à¸«à¸¥à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ˆà¸²à¸: {MASTER_REPORT_PATH}\")\n",
        "try:\n",
        "    df_all = pd.read_excel(MASTER_REPORT_PATH, sheet_name=SHEET_NAME)\n",
        "except FileNotFoundError:\n",
        "    print(f\"âŒ à¹„à¸¡à¹ˆà¸à¸šà¹„à¸Ÿà¸¥à¹Œ Master Report! à¸à¸£à¸¸à¸“à¸²à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š Path: {MASTER_REPORT_PATH}\")\n",
        "    exit()\n",
        "\n",
        "# â¬‡ï¸â¬‡ï¸â¬‡ï¸ à¸ªà¹ˆà¸§à¸™à¸—à¸µà¹ˆà¹€à¸à¸´à¹ˆà¸¡à¹€à¸‚à¹‰à¸²à¸¡à¸²: à¸à¸£à¸­à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹ƒà¸«à¹‰à¹€à¸«à¸¥à¸·à¸­à¹€à¸‰à¸à¸²à¸° Pure OCR â¬‡ï¸â¬‡ï¸â¬‡ï¸\n",
        "df = df_all[df_all['Approach'].str.contains('Pure OCR', case=False)].copy()\n",
        "print(\"à¸à¸£à¸­à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ªà¸³à¹€à¸£à¹‡à¸ˆ! à¹à¸ªà¸”à¸‡à¸œà¸¥à¹€à¸‰à¸à¸²à¸°à¹à¸™à¸§à¸—à¸²à¸‡ Pure OCR à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™\")\n",
        "\n",
        "\n",
        "print(\"à¹‚à¸«à¸¥à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ªà¸³à¹€à¸£à¹‡à¸ˆ! à¹€à¸£à¸´à¹ˆà¸¡à¸ªà¸£à¹‰à¸²à¸‡à¸à¸£à¸²à¸Ÿ...\")\n",
        "\n",
        "# à¸ªà¸£à¹‰à¸²à¸‡à¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œà¹ƒà¸«à¸¡à¹ˆà¹€à¸à¸·à¹ˆà¸­à¹ƒà¸Šà¹‰à¹€à¸›à¹‡à¸™à¹à¸à¸™ X à¹ƒà¸™à¸à¸£à¸²à¸Ÿà¹ƒà¸«à¹‰à¸ªà¸§à¸¢à¸‡à¸²à¸¡\n",
        "df['Model_Approach'] = df['Model'] + ' (' + df['Approach'] + ')'\n",
        "\n",
        "\n",
        "# --- 2. à¸ªà¸£à¹‰à¸²à¸‡à¸à¸£à¸²à¸Ÿà¸—à¸µà¹ˆ 1: à¹€à¸›à¸£à¸µà¸¢à¸šà¹€à¸—à¸µà¸¢à¸šà¸›à¸£à¸°à¸ªà¸´à¸—à¸˜à¸´à¸ à¸²à¸à¹‚à¸”à¸¢à¸£à¸§à¸¡ (à¸„à¹ˆà¸²à¹€à¸‰à¸¥à¸µà¹ˆà¸¢à¸‚à¸­à¸‡à¸—à¸¸à¸à¸Ÿà¸´à¸¥à¸”à¹Œ) ---\n",
        "df_agg = df.groupby('Model_Approach')[['Accuracy (%)', 'WER (%)', 'CER (%)', 'F1-score (%)']].mean().reset_index()\n",
        "df_melted = df_agg.melt(id_vars='Model_Approach', var_name='Metric', value_name='Average Score')\n",
        "\n",
        "plt.figure(figsize=(16, 9))\n",
        "sns.barplot(data=df_melted, x='Metric', y='Average Score', hue='Model_Approach', palette='viridis')\n",
        "plt.title('Overall Average Performance Comparison (Pure OCR Only)', fontsize=20, pad=20)\n",
        "plt.ylabel('Average Score (%)', fontsize=14)\n",
        "plt.xlabel('Evaluation Metric', fontsize=14)\n",
        "plt.xticks(rotation=0, ha='center', fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "plt.legend(title='Model (Approach)', fontsize=11)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "\n",
        "# à¸šà¸±à¸™à¸—à¸¶à¸à¸à¸£à¸²à¸Ÿ\n",
        "graph1_path = os.path.join(OUTPUT_DIR, '1_pure_ocr_overall_performance.png')\n",
        "plt.savefig(graph1_path)\n",
        "print(f\"âœ… à¸šà¸±à¸™à¸—à¸¶à¸à¸à¸£à¸²à¸Ÿà¸—à¸µà¹ˆ 1 à¸ªà¸³à¹€à¸£à¹‡à¸ˆ: {graph1_path}\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# --- 3. à¸ªà¸£à¹‰à¸²à¸‡à¸à¸£à¸²à¸Ÿà¸—à¸µà¹ˆ 2: à¹€à¸ˆà¸²à¸°à¸¥à¸¶à¸à¸›à¸£à¸°à¸ªà¸´à¸—à¸˜à¸´à¸ à¸²à¸à¹ƒà¸™à¹à¸•à¹ˆà¸¥à¸°à¸Ÿà¸´à¸¥à¸”à¹Œ (à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡: Name à¹à¸¥à¸° Application No.) ---\n",
        "fields_to_plot = ['Name', 'Application No.']\n",
        "for field in fields_to_plot:\n",
        "    df_field = df[df['Field'] == field]\n",
        "\n",
        "    plt.figure(figsize=(16, 9))\n",
        "    sns.barplot(data=df_field, x='Model_Approach', y='Accuracy (%)', palette='plasma')\n",
        "    plt.title(f'Accuracy Comparison for Field: \"{field}\" (Pure OCR Only)', fontsize=20, pad=20)\n",
        "    plt.ylabel('Accuracy (%)', fontsize=14)\n",
        "    plt.xlabel('Model (Approach)', fontsize=14)\n",
        "    plt.xticks(rotation=15, ha='right', fontsize=12)\n",
        "    plt.yticks(fontsize=12)\n",
        "    plt.ylim(0, 105)\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # à¸šà¸±à¸™à¸—à¸¶à¸à¸à¸£à¸²à¸Ÿ\n",
        "    graph2_path = os.path.join(OUTPUT_DIR, f'2_pure_ocr_accuracy_for_{field.replace(\" \", \"_\")}.png')\n",
        "    plt.savefig(graph2_path)\n",
        "    print(f\"âœ… à¸šà¸±à¸™à¸—à¸¶à¸à¸à¸£à¸²à¸Ÿà¸—à¸µà¹ˆ 2 ({field}) à¸ªà¸³à¹€à¸£à¹‡à¸ˆ: {graph2_path}\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# --- 4. à¸ªà¸£à¹‰à¸²à¸‡à¸à¸£à¸²à¸Ÿà¸—à¸µà¹ˆ 3: Heatmap à¹à¸ªà¸”à¸‡à¸ à¸²à¸à¸£à¸§à¸¡à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸” ---\n",
        "df_pivot = df.pivot_table(index='Field', columns='Model_Approach', values='Accuracy (%)')\n",
        "\n",
        "plt.figure(figsize=(12, 12)) # à¸›à¸£à¸±à¸šà¸‚à¸™à¸²à¸”à¹ƒà¸«à¹‰à¹€à¸«à¸¡à¸²à¸°à¸à¸±à¸šà¸ˆà¸³à¸™à¸§à¸™à¹‚à¸¡à¹€à¸”à¸¥à¸—à¸µà¹ˆà¸™à¹‰à¸­à¸¢à¸¥à¸‡\n",
        "sns.heatmap(df_pivot, annot=True, fmt=\".1f\", linewidths=.5, cmap='RdYlGn', vmin=0, vmax=100)\n",
        "plt.title('Overall Accuracy (%) Heatmap (Pure OCR Only)', fontsize=20, pad=20)\n",
        "plt.ylabel('Data Field', fontsize=14)\n",
        "plt.xlabel('Model (Approach)', fontsize=14)\n",
        "plt.xticks(rotation=20, ha='right')\n",
        "plt.tight_layout()\n",
        "\n",
        "# à¸šà¸±à¸™à¸—à¸¶à¸à¸à¸£à¸²à¸Ÿ\n",
        "graph3_path = os.path.join(OUTPUT_DIR, '3_pure_ocr_accuracy_heatmap.png')\n",
        "plt.savefig(graph3_path)\n",
        "print(f\"âœ… à¸šà¸±à¸™à¸—à¸¶à¸à¸à¸£à¸²à¸Ÿà¸—à¸µà¹ˆ 3 à¸ªà¸³à¹€à¸£à¹‡à¸ˆ: {graph3_path}\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmAFX2oZm1hJ"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "### **à¸šà¸—à¸ªà¸£à¸¸à¸›à¸ªà¸³à¸«à¸£à¸±à¸šà¸œà¸¹à¹‰à¸šà¸£à¸´à¸«à¸²à¸£ (Executive Summary)**\n",
        "\n",
        "à¹‚à¸¡à¹€à¸”à¸¥à¹à¸¥à¸°à¹à¸™à¸§à¸—à¸²à¸‡à¸—à¸µà¹ˆà¹ƒà¸«à¹‰à¸›à¸£à¸°à¸ªà¸´à¸—à¸˜à¸´à¸ à¸²à¸ **à¸”à¸µà¸—à¸µà¹ˆà¸ªà¸¸à¸” à¸–à¸¹à¸à¸•à¹‰à¸­à¸‡à¹à¸¥à¸°à¹à¸¡à¹ˆà¸™à¸¢à¸³à¸—à¸µà¹ˆà¸ªà¸¸à¸”** à¸ªà¸³à¸«à¸£à¸±à¸šà¸à¸²à¸£à¸ªà¸à¸±à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ˆà¸²à¸à¹ƒà¸šà¸£à¸±à¸šà¸£à¸­à¸‡à¸œà¸¥à¸„à¸°à¹à¸™à¸™à¸ªà¸­à¸š KMITL-TEP à¸„à¸·à¸­ **TrOCR à¹€à¸¡à¸·à¹ˆà¸­à¹ƒà¸Šà¹‰à¸£à¹ˆà¸§à¸¡à¸à¸±à¸šà¹à¸™à¸§à¸—à¸²à¸‡ Boxing OCR** à¸­à¸¢à¹ˆà¸²à¸‡à¹„à¸¡à¹ˆà¸¡à¸µà¸‚à¹‰à¸­à¹‚à¸•à¹‰à¹à¸¢à¹‰à¸‡ à¹‚à¸”à¸¢à¹ƒà¸«à¹‰à¸„à¹ˆà¸² Accuracy à¹à¸¥à¸° F1-score à¸ªà¸¹à¸‡à¸—à¸µà¹ˆà¸ªà¸¸à¸” à¹à¸¥à¸°à¸¡à¸µà¸„à¹ˆà¸² Error Rate (WER/CER) à¸•à¹ˆà¸³à¸—à¸µà¹ˆà¸ªà¸¸à¸”à¹ƒà¸™à¸—à¸¸à¸à¸à¸²à¸£à¸—à¸”à¸ªà¸­à¸š\n",
        "\n",
        "---\n",
        "\n",
        "### **à¸à¸²à¸£à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸œà¸¥à¹ƒà¸™à¸£à¸²à¸¢à¸¥à¸°à¹€à¸­à¸µà¸¢à¸”**\n",
        "\n",
        "à¹€à¸£à¸²à¸ªà¸²à¸¡à¸²à¸£à¸–à¹à¸šà¹ˆà¸‡à¸à¸²à¸£à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸­à¸­à¸à¹€à¸›à¹‡à¸™ 2 à¸›à¸£à¸°à¹€à¸”à¹‡à¸™à¸«à¸¥à¸±à¸ à¸„à¸·à¸­à¸à¸²à¸£à¹€à¸›à¸£à¸µà¸¢à¸šà¹€à¸—à¸µà¸¢à¸šà¸£à¸°à¸«à¸§à¹ˆà¸²à¸‡ \"à¹à¸™à¸§à¸—à¸²à¸‡\" à¹à¸¥à¸°à¸à¸²à¸£à¹€à¸›à¸£à¸µà¸¢à¸šà¹€à¸—à¸µà¸¢à¸šà¸£à¸°à¸«à¸§à¹ˆà¸²à¸‡ \"à¹‚à¸¡à¹€à¸”à¸¥\"\n",
        "\n",
        "#### **1. à¸à¸²à¸£à¹€à¸›à¸£à¸µà¸¢à¸šà¹€à¸—à¸µà¸¢à¸šà¹à¸™à¸§à¸—à¸²à¸‡: Boxing OCR vs. Pure OCR**\n",
        "\n",
        "à¸ˆà¸²à¸à¸à¸£à¸²à¸Ÿà¸—à¸±à¹‰à¸‡à¸«à¸¡à¸” à¹‚à¸”à¸¢à¹€à¸‰à¸à¸²à¸° **à¸à¸£à¸²à¸Ÿà¸—à¸µà¹ˆ 1 (Overall Performance)** à¹à¸¥à¸° **à¸à¸£à¸²à¸Ÿà¸—à¸µà¹ˆ 4 (Heatmap)** à¹à¸ªà¸”à¸‡à¹ƒà¸«à¹‰à¹€à¸«à¹‡à¸™à¸­à¸¢à¹ˆà¸²à¸‡à¸Šà¸±à¸”à¹€à¸ˆà¸™à¸§à¹ˆà¸²:\n",
        "\n",
        "* **Boxing OCR (à¸à¸²à¸£à¸£à¸°à¸šà¸¸à¸•à¸³à¹à¸«à¸™à¹ˆà¸‡) à¸Šà¸™à¸°à¸‚à¸²à¸”à¸¥à¸­à¸¢:** à¹à¸™à¸§à¸—à¸²à¸‡à¸à¸²à¸£à¹ƒà¸Šà¹‰ Bounding Box à¹ƒà¸«à¹‰à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œà¸—à¸µà¹ˆà¹à¸¡à¹ˆà¸™à¸¢à¸³à¸à¸§à¹ˆà¸²à¹à¸™à¸§à¸—à¸²à¸‡ Pure OCR à¹ƒà¸™à¸—à¸¸à¸à¹† à¹‚à¸¡à¹€à¸”à¸¥à¹à¸¥à¸°à¸—à¸¸à¸à¹† Metric à¸—à¸µà¹ˆà¸§à¸±à¸”à¸œà¸¥\n",
        "* **à¹€à¸«à¸•à¸¸à¸œà¸¥:** à¹€à¸™à¸·à¹ˆà¸­à¸‡à¸ˆà¸²à¸à¹ƒà¸šà¸£à¸±à¸šà¸£à¸­à¸‡à¸œà¸¥à¸„à¸°à¹à¸™à¸™à¸ªà¸­à¸šà¸™à¸µà¹‰à¸¡à¸µ **à¹‚à¸„à¸£à¸‡à¸ªà¸£à¹‰à¸²à¸‡à¸—à¸µà¹ˆà¸•à¸²à¸¢à¸•à¸±à¸§ (Fixed Layout)** à¸à¸²à¸£à¸à¸³à¸«à¸™à¸”à¸‚à¸­à¸šà¹€à¸‚à¸•à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¸Šà¸±à¸”à¹€à¸ˆà¸™à¸à¹ˆà¸­à¸™à¸ªà¹ˆà¸‡à¹ƒà¸«à¹‰ OCR à¸­à¹ˆà¸²à¸™ à¸ˆà¸°à¸Šà¹ˆà¸§à¸¢à¸¥à¸”à¸„à¸§à¸²à¸¡à¸œà¸´à¸”à¸à¸¥à¸²à¸”à¹à¸¥à¸°à¸—à¸³à¹ƒà¸«à¹‰à¹‚à¸¡à¹€à¸”à¸¥à¸—à¸³à¸‡à¸²à¸™à¹„à¸”à¹‰à¸‡à¹ˆà¸²à¸¢à¸‚à¸¶à¹‰à¸™ à¹ƒà¸™à¸‚à¸“à¸°à¸—à¸µà¹ˆà¹à¸™à¸§à¸—à¸²à¸‡ Pure OCR à¸•à¹‰à¸­à¸‡à¹€à¸ˆà¸­à¸à¸±à¸šà¸„à¸§à¸²à¸¡à¸—à¹‰à¸²à¸—à¸²à¸¢à¹ƒà¸™à¸à¸²à¸£à¸—à¸³à¸„à¸§à¸²à¸¡à¹€à¸‚à¹‰à¸²à¹ƒà¸ˆà¹‚à¸„à¸£à¸‡à¸ªà¸£à¹‰à¸²à¸‡à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”à¸‚à¸­à¸‡à¸«à¸™à¹‰à¸² à¸‹à¸¶à¹ˆà¸‡à¸‹à¸±à¸šà¸‹à¹‰à¸­à¸™à¹à¸¥à¸°à¸¡à¸µà¹‚à¸­à¸à¸²à¸ªà¸œà¸´à¸”à¸à¸¥à¸²à¸”à¸ªà¸¹à¸‡à¸à¸§à¹ˆà¸²\n",
        "\n",
        "#### **2. à¸à¸²à¸£à¹€à¸›à¸£à¸µà¸¢à¸šà¹€à¸—à¸µà¸¢à¸šà¹‚à¸¡à¹€à¸”à¸¥ (à¹€à¸¡à¸·à¹ˆà¸­à¹ƒà¸Šà¹‰à¹à¸™à¸§à¸—à¸²à¸‡ Boxing à¸—à¸µà¹ˆà¸”à¸µà¸—à¸µà¹ˆà¸ªà¸¸à¸”)**\n",
        "\n",
        "à¹€à¸¡à¸·à¹ˆà¸­à¹€à¸£à¸²à¸à¸´à¸ˆà¸²à¸£à¸“à¸²à¹€à¸‰à¸à¸²à¸°à¹à¸™à¸§à¸—à¸²à¸‡ Boxing à¸—à¸µà¹ˆà¹ƒà¸«à¹‰à¸œà¸¥à¸”à¸µà¸—à¸µà¹ˆà¸ªà¸¸à¸” à¹€à¸£à¸²à¸ªà¸²à¸¡à¸²à¸£à¸–à¸ˆà¸±à¸”à¸­à¸±à¸™à¸”à¸±à¸šà¸„à¸§à¸²à¸¡à¹à¸¡à¹ˆà¸™à¸¢à¸³à¸‚à¸­à¸‡à¹‚à¸¡à¹€à¸”à¸¥à¹„à¸”à¹‰à¸”à¸±à¸‡à¸™à¸µà¹‰:\n",
        "\n",
        "ğŸ¥‡ **à¸­à¸±à¸™à¸”à¸±à¸šà¸—à¸µà¹ˆ 1: TrOCR (Boxing)**\n",
        "* **à¹€à¸›à¹‡à¸™à¸œà¸¹à¹‰à¸Šà¸™à¸°à¹€à¸¥à¸´à¸¨à¸‚à¸­à¸‡à¹‚à¸›à¸£à¹€à¸ˆà¸à¸•à¹Œà¸™à¸µà¹‰** à¸ˆà¸²à¸ Heatmap (à¸à¸£à¸²à¸Ÿà¸—à¸µà¹ˆ 4) à¸ˆà¸°à¹€à¸«à¹‡à¸™à¸§à¹ˆà¸²à¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œà¸‚à¸­à¸‡ `TrOCR (Boxing)` à¹€à¸›à¹‡à¸™à¸ªà¸µà¹€à¸‚à¸µà¸¢à¸§à¹€à¸‚à¹‰à¸¡à¹€à¸à¸·à¸­à¸šà¸—à¸±à¹‰à¸‡à¸«à¸¡à¸” à¹à¸ªà¸”à¸‡à¹ƒà¸«à¹‰à¹€à¸«à¹‡à¸™à¸–à¸¶à¸‡à¸›à¸£à¸°à¸ªà¸´à¸—à¸˜à¸´à¸ à¸²à¸à¸—à¸µà¹ˆà¸¢à¸­à¸”à¹€à¸¢à¸µà¹ˆà¸¢à¸¡à¹à¸¥à¸°à¸¡à¸µà¸„à¸§à¸²à¸¡à¹€à¸ªà¸–à¸µà¸¢à¸£à¸ªà¸¹à¸‡à¹ƒà¸™à¸—à¸¸à¸à¹† à¸Ÿà¸´à¸¥à¸”à¹Œà¸‚à¹‰à¸­à¸¡à¸¹à¸¥\n",
        "* **à¸ˆà¸¸à¸”à¹à¸‚à¹‡à¸‡:** à¸¡à¸µà¸„à¹ˆà¸² Accuracy à¹à¸¥à¸° F1-score à¹ƒà¸à¸¥à¹‰à¹€à¸„à¸µà¸¢à¸‡ 100% à¹à¸¥à¸°à¸¡à¸µà¸„à¹ˆà¸² WER/CER à¸•à¹ˆà¸³à¸¡à¸²à¸ à¸‹à¸¶à¹ˆà¸‡à¹€à¸«à¸¡à¸²à¸°à¸ªà¸¡à¸­à¸¢à¹ˆà¸²à¸‡à¸¢à¸´à¹ˆà¸‡à¸ªà¸³à¸«à¸£à¸±à¸šà¸‡à¸²à¸™à¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¸„à¸§à¸²à¸¡à¸–à¸¹à¸à¸•à¹‰à¸­à¸‡à¸ªà¸¹à¸‡à¸ªà¸¸à¸”\n",
        "\n",
        "ğŸ¥ˆ **à¸­à¸±à¸™à¸”à¸±à¸šà¸—à¸µà¹ˆ 2: EasyOCR (Boxing)**\n",
        "* **à¹€à¸›à¹‡à¸™à¸•à¸±à¸§à¹€à¸¥à¸·à¸­à¸à¸—à¸µà¹ˆà¸”à¸µà¸¡à¸²à¸à¹à¸¥à¸°à¹€à¸›à¹‡à¸™à¸­à¸±à¸™à¸”à¸±à¸šà¸ªà¸­à¸‡à¸—à¸µà¹ˆà¹à¸‚à¹‡à¸‡à¹à¸à¸£à¹ˆà¸‡** à¹ƒà¸«à¹‰à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œà¸—à¸µà¹ˆà¹à¸¡à¹ˆà¸™à¸¢à¸³à¸ªà¸¹à¸‡à¹ƒà¸™à¹€à¸à¸·à¸­à¸šà¸—à¸¸à¸à¸Ÿà¸´à¸¥à¸”à¹Œ (à¸ªà¹ˆà¸§à¸™à¹ƒà¸«à¸à¹ˆà¹€à¸›à¹‡à¸™à¸ªà¸µà¹€à¸‚à¸µà¸¢à¸§à¸­à¹ˆà¸­à¸™à¹ƒà¸™ Heatmap) à¹à¸¥à¸°à¸”à¸µà¸à¸§à¹ˆà¸² Pytesseract à¸­à¸¢à¹ˆà¸²à¸‡à¹€à¸«à¹‡à¸™à¹„à¸”à¹‰à¸Šà¸±à¸”\n",
        "* **à¸ˆà¸¸à¸”à¹à¸‚à¹‡à¸‡:** à¹€à¸›à¹‡à¸™à¹‚à¸¡à¹€à¸”à¸¥à¸—à¸µà¹ˆà¸ªà¸¡à¸”à¸¸à¸¥à¸£à¸°à¸«à¸§à¹ˆà¸²à¸‡à¸„à¸§à¸²à¸¡à¹€à¸£à¹‡à¸§à¹à¸¥à¸°à¸„à¸§à¸²à¸¡à¹à¸¡à¹ˆà¸™à¸¢à¸³ à¹ƒà¸Šà¹‰à¸‡à¸²à¸™à¸‡à¹ˆà¸²à¸¢ à¹à¸¥à¸°à¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¸—à¸£à¸±à¸à¸¢à¸²à¸à¸£ (GPU) à¸¡à¸²à¸à¹€à¸—à¹ˆà¸² TrOCR\n",
        "\n",
        "ğŸ¥‰ **à¸­à¸±à¸™à¸”à¸±à¸šà¸—à¸µà¹ˆ 3: Pytesseract (Boxing)**\n",
        "* **à¹€à¸›à¹‡à¸™à¹‚à¸¡à¹€à¸”à¸¥à¸à¸·à¹‰à¸™à¸à¸²à¸™à¸—à¸µà¹ˆà¸”à¸µ** à¹à¸•à¹ˆà¸›à¸£à¸°à¸ªà¸´à¸—à¸˜à¸´à¸ à¸²à¸à¹‚à¸”à¸¢à¸£à¸§à¸¡à¸¢à¸±à¸‡à¸ªà¸¹à¹‰à¹‚à¸¡à¹€à¸”à¸¥à¸—à¸µà¹ˆà¹ƒà¸«à¸¡à¹ˆà¸à¸§à¹ˆà¸²à¸­à¸¢à¹ˆà¸²à¸‡ EasyOCR à¹à¸¥à¸° TrOCR à¹„à¸¡à¹ˆà¹„à¸”à¹‰ à¹‚à¸”à¸¢à¹€à¸‰à¸à¸²à¸°à¹ƒà¸™à¸Ÿà¸´à¸¥à¸”à¹Œà¸—à¸µà¹ˆà¹€à¸›à¹‡à¸™à¸•à¸±à¸§à¹€à¸¥à¸‚à¸«à¸£à¸·à¸­à¸•à¸±à¸§à¸­à¸±à¸à¸©à¸£à¸—à¸µà¹ˆà¸•à¸´à¸”à¸à¸±à¸™à¸¡à¸²à¸à¹†\n",
        "* **à¸ˆà¸¸à¸”à¹à¸‚à¹‡à¸‡:** à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡à¸‡à¹ˆà¸²à¸¢à¹à¸¥à¸°à¹€à¸›à¹‡à¸™à¸—à¸µà¹ˆà¸£à¸¹à¹‰à¸ˆà¸±à¸à¹à¸à¸£à¹ˆà¸«à¸¥à¸²à¸¢ à¹€à¸«à¸¡à¸²à¸°à¸ªà¸³à¸«à¸£à¸±à¸šà¸‡à¸²à¸™à¸—à¸µà¹ˆà¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¸„à¸§à¸²à¸¡à¹à¸¡à¹ˆà¸™à¸¢à¸³à¸ªà¸¹à¸‡à¸¡à¸²à¸à¸™à¸±à¸\n",
        "\n",
        "---\n",
        "\n",
        "### **à¸‚à¹‰à¸­à¹€à¸ªà¸™à¸­à¹à¸™à¸°à¸ªà¸¸à¸”à¸—à¹‰à¸²à¸¢ (Final Recommendation)**\n",
        "\n",
        "* **à¹€à¸à¸·à¹ˆà¸­à¸„à¸§à¸²à¸¡à¹à¸¡à¹ˆà¸™à¸¢à¸³à¸ªà¸¹à¸‡à¸ªà¸¸à¸”:** à¸«à¸²à¸à¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¸£à¸°à¸šà¸šà¸—à¸µà¹ˆ **à¸–à¸¹à¸à¸•à¹‰à¸­à¸‡à¹à¸¥à¸°à¸™à¹ˆà¸²à¹€à¸Šà¸·à¹ˆà¸­à¸–à¸·à¸­à¸—à¸µà¹ˆà¸ªà¸¸à¸”** à¸ªà¸³à¸«à¸£à¸±à¸šà¸™à¸³à¹„à¸›à¹ƒà¸Šà¹‰à¸‡à¸²à¸™à¸ˆà¸£à¸´à¸‡ (Production) à¸„à¸§à¸£à¹€à¸¥à¸·à¸­à¸à¹ƒà¸Šà¹‰ **TrOCR à¸£à¹ˆà¸§à¸¡à¸à¸±à¸šà¹à¸™à¸§à¸—à¸²à¸‡ Boxing OCR**\n",
        "* **à¸•à¸±à¸§à¹€à¸¥à¸·à¸­à¸à¸—à¸µà¹ˆà¸ªà¸¡à¸”à¸¸à¸¥:** à¸«à¸²à¸à¸¡à¸µà¸‚à¹‰à¸­à¸ˆà¸³à¸à¸±à¸”à¸”à¹‰à¸²à¸™à¸—à¸£à¸±à¸à¸¢à¸²à¸à¸£ à¹€à¸Šà¹ˆà¸™ à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¹ƒà¸Šà¹‰ GPU à¹„à¸”à¹‰ à¸«à¸£à¸·à¸­à¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¸„à¸§à¸²à¸¡à¹€à¸£à¹‡à¸§à¹ƒà¸™à¸à¸²à¸£à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¸ªà¸¹à¸‡ **EasyOCR à¸£à¹ˆà¸§à¸¡à¸à¸±à¸šà¹à¸™à¸§à¸—à¸²à¸‡ Boxing OCR** à¹€à¸›à¹‡à¸™à¸•à¸±à¸§à¹€à¸¥à¸·à¸­à¸à¸—à¸µà¹ˆà¸¢à¸­à¸”à¹€à¸¢à¸µà¹ˆà¸¢à¸¡ à¹ƒà¸«à¹‰à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œà¸—à¸µà¹ˆà¹à¸¡à¹ˆà¸™à¸¢à¸³à¸¡à¸²à¸à¹€à¸à¸µà¸¢à¸‡à¸à¸­à¸ªà¸³à¸«à¸£à¸±à¸šà¸à¸²à¸£à¹ƒà¸Šà¹‰à¸‡à¸²à¸™à¸ªà¹ˆà¸§à¸™à¹ƒà¸«à¸à¹ˆ\n",
        "\n",
        "à¸à¸²à¸£à¸—à¸”à¸¥à¸­à¸‡à¸™à¸µà¹‰à¹à¸ªà¸”à¸‡à¹ƒà¸«à¹‰à¹€à¸«à¹‡à¸™à¸§à¹ˆà¸² à¸™à¸­à¸à¸ˆà¸²à¸à¸à¸²à¸£à¹€à¸¥à¸·à¸­à¸ \"à¹‚à¸¡à¹€à¸”à¸¥\" à¸—à¸µà¹ˆà¸”à¸µà¹à¸¥à¹‰à¸§ à¸à¸²à¸£à¹€à¸¥à¸·à¸­à¸ \"à¹à¸™à¸§à¸—à¸²à¸‡\" (Approach) à¹ƒà¸«à¹‰à¹€à¸«à¸¡à¸²à¸°à¸ªà¸¡à¸à¸±à¸šà¸¥à¸±à¸à¸©à¸“à¸°à¸‚à¸­à¸‡à¹€à¸­à¸à¸ªà¸²à¸£à¸à¹‡à¸¡à¸µà¸„à¸§à¸²à¸¡à¸ªà¸³à¸„à¸±à¸à¹„à¸¡à¹ˆà¹à¸à¹‰à¸à¸±à¸™à¸„à¸£à¸±à¸š"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdoIsYAJg5R_"
      },
      "source": [
        "# Visualization all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "kQu6QBtXg971"
      },
      "outputs": [],
      "source": [
        "pip install matplotlib seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtqVxijphKZO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# --- 1. à¸à¸³à¸«à¸™à¸”à¸„à¹ˆà¸²à¹à¸¥à¸°à¹‚à¸«à¸¥à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ ---\n",
        "MASTER_REPORT_PATH = \"/content/drive/MyDrive/kmitl_dataset/final-excel/final_comparison_report.xlsx\"\n",
        "SHEET_NAME = 'Master_Evaluation'\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/kmitl_dataset/final-excel/graphs/\" # à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œà¸ªà¸³à¸«à¸£à¸±à¸šà¹€à¸à¹‡à¸šà¸à¸£à¸²à¸Ÿ\n",
        "\n",
        "# à¸ªà¸£à¹‰à¸²à¸‡à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œà¸–à¹‰à¸²à¸¢à¸±à¸‡à¹„à¸¡à¹ˆà¸¡à¸µ\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"à¸à¸³à¸¥à¸±à¸‡à¹‚à¸«à¸¥à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ˆà¸²à¸: {MASTER_REPORT_PATH}\")\n",
        "try:\n",
        "    df = pd.read_excel(MASTER_REPORT_PATH, sheet_name=SHEET_NAME)\n",
        "except FileNotFoundError:\n",
        "    print(f\"âŒ à¹„à¸¡à¹ˆà¸à¸šà¹„à¸Ÿà¸¥à¹Œ Master Report! à¸à¸£à¸¸à¸“à¸²à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š Path: {MASTER_REPORT_PATH}\")\n",
        "    exit()\n",
        "\n",
        "print(\"à¹‚à¸«à¸¥à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ªà¸³à¹€à¸£à¹‡à¸ˆ! à¹€à¸£à¸´à¹ˆà¸¡à¸ªà¸£à¹‰à¸²à¸‡à¸à¸£à¸²à¸Ÿ...\")\n",
        "\n",
        "# à¸ªà¸£à¹‰à¸²à¸‡à¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œà¹ƒà¸«à¸¡à¹ˆà¹€à¸à¸·à¹ˆà¸­à¹ƒà¸Šà¹‰à¹€à¸›à¹‡à¸™à¹à¸à¸™ X à¹ƒà¸™à¸à¸£à¸²à¸Ÿà¹ƒà¸«à¹‰à¸ªà¸§à¸¢à¸‡à¸²à¸¡\n",
        "df['Model_Approach'] = df['Model'] + ' (' + df['Approach'] + ')'\n",
        "\n",
        "\n",
        "# --- 2. à¸ªà¸£à¹‰à¸²à¸‡à¸à¸£à¸²à¸Ÿà¸—à¸µà¹ˆ 1: à¹€à¸›à¸£à¸µà¸¢à¸šà¹€à¸—à¸µà¸¢à¸šà¸›à¸£à¸°à¸ªà¸´à¸—à¸˜à¸´à¸ à¸²à¸à¹‚à¸”à¸¢à¸£à¸§à¸¡ (à¸„à¹ˆà¸²à¹€à¸‰à¸¥à¸µà¹ˆà¸¢à¸‚à¸­à¸‡à¸—à¸¸à¸à¸Ÿà¸´à¸¥à¸”à¹Œ) ---\n",
        "# à¸„à¸³à¸™à¸§à¸“à¸„à¹ˆà¸²à¹€à¸‰à¸¥à¸µà¹ˆà¸¢à¸‚à¸­à¸‡à¹à¸•à¹ˆà¸¥à¸° Metric à¹‚à¸”à¸¢à¹à¸¢à¸à¸•à¸²à¸¡ Model à¹à¸¥à¸° Approach\n",
        "df_agg = df.groupby('Model_Approach')[['Accuracy (%)', 'WER (%)', 'CER (%)', 'F1-score (%)']].mean().reset_index()\n",
        "\n",
        "# Melt DataFrame à¹€à¸à¸·à¹ˆà¸­à¹ƒà¸«à¹‰à¸‡à¹ˆà¸²à¸¢à¸•à¹ˆà¸­à¸à¸²à¸£à¸à¸¥à¹‡à¸­à¸•à¸”à¹‰à¸§à¸¢ Seaborn\n",
        "df_melted = df_agg.melt(id_vars='Model_Approach', var_name='Metric', value_name='Average Score')\n",
        "\n",
        "plt.figure(figsize=(16, 9))\n",
        "sns.barplot(data=df_melted, x='Metric', y='Average Score', hue='Model_Approach', palette='viridis')\n",
        "plt.title('Overall Average Performance Comparison', fontsize=20, pad=20)\n",
        "plt.ylabel('Average Score (%)', fontsize=14)\n",
        "plt.xlabel('Evaluation Metric', fontsize=14)\n",
        "plt.xticks(rotation=0, ha='center', fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "plt.legend(title='Model (Approach)', fontsize=11)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "\n",
        "# à¸šà¸±à¸™à¸—à¸¶à¸à¸à¸£à¸²à¸Ÿ\n",
        "graph1_path = os.path.join(OUTPUT_DIR, '1_overall_performance.png')\n",
        "plt.savefig(graph1_path)\n",
        "print(f\"âœ… à¸šà¸±à¸™à¸—à¸¶à¸à¸à¸£à¸²à¸Ÿà¸—à¸µà¹ˆ 1 à¸ªà¸³à¹€à¸£à¹‡à¸ˆ: {graph1_path}\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# --- 3. à¸ªà¸£à¹‰à¸²à¸‡à¸à¸£à¸²à¸Ÿà¸—à¸µà¹ˆ 2: à¹€à¸ˆà¸²à¸°à¸¥à¸¶à¸à¸›à¸£à¸°à¸ªà¸´à¸—à¸˜à¸´à¸ à¸²à¸à¹ƒà¸™à¹à¸•à¹ˆà¸¥à¸°à¸Ÿà¸´à¸¥à¸”à¹Œ (à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡: Name à¹à¸¥à¸° Application No.) ---\n",
        "fields_to_plot = ['Name', 'Application No.']\n",
        "for field in fields_to_plot:\n",
        "    df_field = df[df['Field'] == field]\n",
        "\n",
        "    plt.figure(figsize=(16, 9))\n",
        "    sns.barplot(data=df_field, x='Model_Approach', y='Accuracy (%)', palette='plasma')\n",
        "    plt.title(f'Accuracy Comparison for Field: \"{field}\"', fontsize=20, pad=20)\n",
        "    plt.ylabel('Accuracy (%)', fontsize=14)\n",
        "    plt.xlabel('Model (Approach)', fontsize=14)\n",
        "    plt.xticks(rotation=15, ha='right', fontsize=12)\n",
        "    plt.yticks(fontsize=12)\n",
        "    plt.ylim(0, 105) # à¸à¸³à¸«à¸™à¸”à¹ƒà¸«à¹‰à¹à¸à¸™ Y à¹€à¸•à¹‡à¸¡ 100%\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # à¸šà¸±à¸™à¸—à¸¶à¸à¸à¸£à¸²à¸Ÿ\n",
        "    graph2_path = os.path.join(OUTPUT_DIR, f'2_accuracy_for_{field.replace(\" \", \"_\")}.png')\n",
        "    plt.savefig(graph2_path)\n",
        "    print(f\"âœ… à¸šà¸±à¸™à¸—à¸¶à¸à¸à¸£à¸²à¸Ÿà¸—à¸µà¹ˆ 2 ({field}) à¸ªà¸³à¹€à¸£à¹‡à¸ˆ: {graph2_path}\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# --- 4. à¸ªà¸£à¹‰à¸²à¸‡à¸à¸£à¸²à¸Ÿà¸—à¸µà¹ˆ 3: Heatmap à¹à¸ªà¸”à¸‡à¸ à¸²à¸à¸£à¸§à¸¡à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸” ---\n",
        "# à¹€à¸£à¸²à¸ˆà¸°à¹ƒà¸Šà¹‰ Accuracy à¹€à¸›à¹‡à¸™à¸•à¸±à¸§à¸Šà¸µà¹‰à¸§à¸±à¸”à¸«à¸¥à¸±à¸à¹ƒà¸™ Heatmap\n",
        "df_pivot = df.pivot_table(index='Field', columns='Model_Approach', values='Accuracy (%)')\n",
        "\n",
        "plt.figure(figsize=(16, 12))\n",
        "sns.heatmap(df_pivot, annot=True, fmt=\".1f\", linewidths=.5, cmap='RdYlGn', vmin=0, vmax=100)\n",
        "plt.title('Overall Accuracy (%) Heatmap', fontsize=20, pad=20)\n",
        "plt.ylabel('Data Field', fontsize=14)\n",
        "plt.xlabel('Model (Approach)', fontsize=14)\n",
        "plt.xticks(rotation=20, ha='right')\n",
        "plt.tight_layout()\n",
        "\n",
        "# à¸šà¸±à¸™à¸—à¸¶à¸à¸à¸£à¸²à¸Ÿ\n",
        "graph3_path = os.path.join(OUTPUT_DIR, '3_accuracy_heatmap.png')\n",
        "plt.savefig(graph3_path)\n",
        "print(f\"âœ… à¸šà¸±à¸™à¸—à¸¶à¸à¸à¸£à¸²à¸Ÿà¸—à¸µà¹ˆ 3 à¸ªà¸³à¹€à¸£à¹‡à¸ˆ: {graph3_path}\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4SKR-qrjPom"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "df = pd.read_excel('/content/drive/MyDrive/kmitl_dataset/final-excel/final_comparison_report.xlsx')\n",
        "\n",
        "# Display the first 5 rows\n",
        "print(df.head().to_string(index=False))\n",
        "\n",
        "# Print the column names and their data types\n",
        "print(df.info())\n",
        "\n",
        "# Get unique values in `Model` column\n",
        "print(df['Model'].unique())\n",
        "\n",
        "# Get unique values in `Approach` column\n",
        "print(df['Approach'].unique())\n",
        "\n",
        "# Group by `Model` and `Approach` and calculate the mean of the performance metrics\n",
        "df_agg = df.groupby(['Model', 'Approach']).agg(\n",
        "    Accuracy=('Accuracy (%)', 'mean'),\n",
        "    WER=('WER (%)', 'mean'),\n",
        "    CER=('CER (%)', 'mean'),\n",
        "    F1_score=('F1-score (%)', 'mean')\n",
        ").reset_index()\n",
        "\n",
        "# Print the aggregated DataFrame\n",
        "print(df_agg.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfO3vlLjj29Z"
      },
      "outputs": [],
      "source": [
        "import altair as alt\n",
        "# Create a bar chart for average F1-score by Model and Approach\n",
        "chart = alt.Chart(df_agg).mark_bar().encode(\n",
        " x=alt.X('Model', axis=alt.Axis(title='Model')),\n",
        " y=alt.Y('F1_score', axis=alt.Axis(title='F1-score (%)')),\n",
        " color=alt.Color('Approach', title='Approach'),\n",
        " tooltip=['Model', 'Approach', 'F1_score']\n",
        ").properties(\n",
        " title='Average F1-score by Model and Approach'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdWkgZdlg9jD"
      },
      "source": [
        "1.  **à¸›à¸£à¸°à¸ªà¸´à¸—à¸˜à¸´à¸ à¸²à¸à¸‚à¸­à¸‡à¹‚à¸¡à¹€à¸”à¸¥à¹à¸¥à¸°à¹à¸™à¸§à¸—à¸²à¸‡à¸—à¸µà¹ˆà¹à¸•à¸à¸•à¹ˆà¸²à¸‡à¸à¸±à¸™**:\n",
        "    *   **Pytesseract** à¹à¸¥à¸° **TrOCR** à¸¡à¸µà¸›à¸£à¸°à¸ªà¸´à¸—à¸˜à¸´à¸ à¸²à¸à¸”à¸µà¸—à¸µà¹ˆà¸ªà¸¸à¸”à¹€à¸¡à¸·à¹ˆà¸­à¹ƒà¸Šà¹‰à¹à¸™à¸§à¸—à¸²à¸‡ **Boxing** à¹‚à¸”à¸¢à¸¡à¸µà¸„à¹ˆà¸²à¹€à¸‰à¸¥à¸µà¹ˆà¸¢ `Accuracy (%)` à¹à¸¥à¸° `F1-score (%)` à¸ªà¸¹à¸‡à¸ªà¸¸à¸” à¹à¸¥à¸° `WER (%)` à¸•à¹ˆà¸³à¸ªà¸¸à¸”\n",
        "    *   **EasyOCR** à¸¡à¸µà¸›à¸£à¸°à¸ªà¸´à¸—à¸˜à¸´à¸ à¸²à¸à¸”à¸µà¸—à¸µà¹ˆà¸ªà¸¸à¸”à¹€à¸¡à¸·à¹ˆà¸­à¹ƒà¸Šà¹‰à¹à¸™à¸§à¸—à¸²à¸‡ **Boxing** à¹€à¸Šà¹ˆà¸™à¸à¸±à¸™ à¹à¸•à¹ˆà¸¡à¸µà¸„à¹ˆà¸²à¹€à¸‰à¸¥à¸µà¹ˆà¸¢ `Accuracy (%)` à¹à¸¥à¸° `F1-score (%)` à¸•à¹ˆà¸³à¸à¸§à¹ˆà¸²à¹€à¸¥à¹‡à¸à¸™à¹‰à¸­à¸¢à¹€à¸¡à¸·à¹ˆà¸­à¹€à¸—à¸µà¸¢à¸šà¸à¸±à¸š Pytesseract à¹à¸¥à¸° TrOCR\n",
        "    *   à¹à¸™à¸§à¸—à¸²à¸‡ **Pure OCR (Hybrid)** à¸‚à¸­à¸‡ **TrOCR** à¹à¸ªà¸”à¸‡à¸›à¸£à¸°à¸ªà¸´à¸—à¸˜à¸´à¸ à¸²à¸à¸—à¸µà¹ˆà¹à¸¢à¹ˆà¸—à¸µà¹ˆà¸ªà¸¸à¸” à¹‚à¸”à¸¢à¸¡à¸µ `Accuracy (%)` à¹à¸¥à¸° `F1-score (%)` à¹€à¸›à¹‡à¸™ 0 à¹à¸¥à¸° `WER (%)` à¹€à¸›à¹‡à¸™ 100 à¸‹à¸¶à¹ˆà¸‡à¸­à¸²à¸ˆà¸šà¹ˆà¸‡à¸Šà¸µà¹‰à¸–à¸¶à¸‡à¸›à¸±à¸à¸«à¸²à¹ƒà¸™à¸à¸²à¸£à¹ƒà¸Šà¹‰à¸‡à¸²à¸™à¸«à¸£à¸·à¸­à¸„à¸§à¸²à¸¡à¹„à¸¡à¹ˆà¹€à¸«à¸¡à¸²à¸°à¸ªà¸¡à¸‚à¸­à¸‡à¹à¸™à¸§à¸—à¸²à¸‡à¸™à¸µà¹‰à¸ªà¸³à¸«à¸£à¸±à¸šà¹‚à¸¡à¹€à¸”à¸¥ TrOCR à¹ƒà¸™à¸Šà¸¸à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸™à¸µà¹‰\n",
        "    *   à¹à¸™à¸§à¸—à¸²à¸‡ **Pure OCR (Spatial)** à¸‚à¸­à¸‡ **Pytesseract** à¸à¹‡à¸¡à¸µà¸›à¸£à¸°à¸ªà¸´à¸—à¸˜à¸´à¸ à¸²à¸à¸•à¹ˆà¸³à¹€à¸Šà¹ˆà¸™à¸à¸±à¸™ à¹‚à¸”à¸¢à¸¡à¸µ `Accuracy (%)` à¹à¸¥à¸° `F1-score (%)` à¸•à¹ˆà¸³ à¹à¸¥à¸° `WER (%)` à¸ªà¸¹à¸‡\n",
        "\n",
        "2.  **à¸„à¸§à¸²à¸¡à¸ªà¸³à¸„à¸±à¸à¸‚à¸­à¸‡à¹à¸™à¸§à¸—à¸²à¸‡ Boxing**:\n",
        "    *   à¹à¸™à¸§à¸—à¸²à¸‡ **Boxing** à¹ƒà¸«à¹‰à¸›à¸£à¸°à¸ªà¸´à¸—à¸˜à¸´à¸ à¸²à¸à¸—à¸µà¹ˆà¸”à¸µà¸—à¸µà¹ˆà¸ªà¸¸à¸”à¸ªà¸³à¸«à¸£à¸±à¸šà¸—à¸¸à¸à¹‚à¸¡à¹€à¸”à¸¥ (Pytesseract, EasyOCR, TrOCR) à¹ƒà¸™à¹à¸‡à¹ˆà¸‚à¸­à¸‡ `Accuracy (%)`, `WER (%)`, à¹à¸¥à¸° `F1-score (%)`\n",
        "    *   à¸ªà¸´à¹ˆà¸‡à¸™à¸µà¹‰à¸Šà¸µà¹‰à¹ƒà¸«à¹‰à¹€à¸«à¹‡à¸™à¸§à¹ˆà¸²à¸à¸²à¸£à¸£à¸°à¸šà¸¸à¸‚à¸­à¸šà¹€à¸‚à¸•à¸‚à¸­à¸‡à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸­à¸¢à¹ˆà¸²à¸‡à¹à¸¡à¹ˆà¸™à¸¢à¸³à¸à¹ˆà¸­à¸™à¸à¸²à¸£à¸£à¸¹à¹‰à¸ˆà¸³à¸”à¹‰à¸§à¸¢ OCR (Optical Character Recognition) à¸¡à¸µà¸œà¸¥à¸­à¸¢à¹ˆà¸²à¸‡à¸¡à¸²à¸à¸•à¹ˆà¸­à¸„à¸§à¸²à¸¡à¸–à¸¹à¸à¸•à¹‰à¸­à¸‡à¸‚à¸­à¸‡à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œ\n",
        "\n",
        "3.  **à¸‚à¹‰à¸­à¸ªà¸±à¸‡à¹€à¸à¸•à¹€à¸à¸µà¹ˆà¸¢à¸§à¸à¸±à¸š CER (%)**:\n",
        "    *   à¸„à¹ˆà¸² `CER (%)` à¹€à¸›à¹‡à¸™ 0 à¸ªà¸³à¸«à¸£à¸±à¸šà¸—à¸¸à¸à¹‚à¸¡à¹€à¸”à¸¥à¹à¸¥à¸°à¹à¸™à¸§à¸—à¸²à¸‡ à¸‹à¸¶à¹ˆà¸‡à¸œà¸´à¸”à¸›à¸à¸•à¸´à¹à¸¥à¸°à¸­à¸²à¸ˆà¸šà¹ˆà¸‡à¸Šà¸µà¹‰à¸§à¹ˆà¸²à¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œà¸™à¸µà¹‰à¹„à¸¡à¹ˆà¹„à¸”à¹‰à¸ªà¸°à¸—à¹‰à¸­à¸™à¸–à¸¶à¸‡à¸„à¸§à¸²à¸¡à¹à¸•à¸à¸•à¹ˆà¸²à¸‡à¸‚à¸­à¸‡à¸›à¸£à¸°à¸ªà¸´à¸—à¸˜à¸´à¸ à¸²à¸à¸—à¸µà¹ˆà¹à¸—à¹‰à¸ˆà¸£à¸´à¸‡ à¸«à¸£à¸·à¸­à¸­à¸²à¸ˆà¸¡à¸µà¸‚à¹‰à¸­à¸œà¸´à¸”à¸à¸¥à¸²à¸”à¹ƒà¸™à¸à¸²à¸£à¸£à¸§à¸šà¸£à¸§à¸¡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸«à¸£à¸·à¸­à¸à¸²à¸£à¸„à¸³à¸™à¸§à¸“\n",
        "\n",
        "à¸à¸²à¸£à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹à¸ªà¸”à¸‡à¹ƒà¸«à¹‰à¹€à¸«à¹‡à¸™à¸–à¸¶à¸‡à¸›à¸£à¸°à¸ªà¸´à¸—à¸˜à¸´à¸ à¸²à¸à¸‚à¸­à¸‡à¹‚à¸¡à¹€à¸”à¸¥à¹à¸¥à¸°à¹à¸™à¸§à¸—à¸²à¸‡à¸•à¹ˆà¸²à¸‡à¹† à¹ƒà¸™à¸à¸²à¸£à¸£à¸¹à¹‰à¸ˆà¸³à¸­à¸±à¸à¸‚à¸£à¸°à¸”à¹‰à¸§à¸¢à¹à¸ªà¸‡ (OCR) à¹‚à¸”à¸¢à¸¡à¸µà¸‚à¹‰à¸­à¸ªà¸±à¸‡à¹€à¸à¸•à¸—à¸µà¹ˆà¸ªà¸³à¸„à¸±à¸à¸”à¸±à¸‡à¸™à¸µà¹‰:\n",
        "\n",
        "**à¸›à¸£à¸°à¹€à¸”à¹‡à¸™à¸ªà¸³à¸„à¸±à¸:**\n",
        "\n",
        "1.  **à¸›à¸£à¸°à¸ªà¸´à¸—à¸˜à¸´à¸ à¸²à¸à¸—à¸µà¹ˆà¹à¸•à¸à¸•à¹ˆà¸²à¸‡à¸à¸±à¸™à¸‚à¸­à¸‡à¹‚à¸¡à¹€à¸”à¸¥à¹à¸¥à¸°à¹à¸™à¸§à¸—à¸²à¸‡:**\n",
        "    *   **Pytesseract** à¹à¸¥à¸° **TrOCR** à¹à¸ªà¸”à¸‡à¸›à¸£à¸°à¸ªà¸´à¸—à¸˜à¸´à¸ à¸²à¸à¸—à¸µà¹ˆà¹‚à¸”à¸”à¹€à¸”à¹ˆà¸™à¸—à¸µà¹ˆà¸ªà¸¸à¸”à¹€à¸¡à¸·à¹ˆà¸­à¹ƒà¸Šà¹‰à¹à¸™à¸§à¸—à¸²à¸‡ **Boxing** à¹‚à¸”à¸¢à¸¡à¸µà¸„à¹ˆà¸²à¹€à¸‰à¸¥à¸µà¹ˆà¸¢à¸„à¸§à¸²à¸¡à¹à¸¡à¹ˆà¸™à¸¢à¸³ (Accuracy) à¹à¸¥à¸° F1-score à¸ªà¸¹à¸‡à¸ªà¸¸à¸” à¹à¸¥à¸°à¸­à¸±à¸•à¸£à¸²à¸‚à¹‰à¸­à¸œà¸´à¸”à¸à¸¥à¸²à¸”à¸‚à¸­à¸‡à¸„à¸³ (WER) à¸•à¹ˆà¸³à¸ªà¸¸à¸”\n",
        "    *   **EasyOCR** à¸¡à¸µà¸›à¸£à¸°à¸ªà¸´à¸—à¸˜à¸´à¸ à¸²à¸à¸—à¸µà¹ˆà¸”à¸µà¸—à¸µà¹ˆà¸ªà¸¸à¸”à¹€à¸¡à¸·à¹ˆà¸­à¹ƒà¸Šà¹‰à¹à¸™à¸§à¸—à¸²à¸‡ **Boxing** à¹€à¸Šà¹ˆà¸™à¸à¸±à¸™ à¹à¸•à¹ˆà¹‚à¸”à¸¢à¸£à¸§à¸¡à¹à¸¥à¹‰à¸§ Pytesseract à¹à¸¥à¸° TrOCR à¸¡à¸µà¸›à¸£à¸°à¸ªà¸´à¸—à¸˜à¸´à¸ à¸²à¸à¸—à¸µà¹ˆà¹€à¸«à¸™à¸·à¸­à¸à¸§à¹ˆà¸²\n",
        "    *   à¹à¸™à¸§à¸—à¸²à¸‡ **Pure OCR (Hybrid)** à¸‚à¸­à¸‡ **TrOCR** à¹à¸ªà¸”à¸‡à¸›à¸£à¸°à¸ªà¸´à¸—à¸˜à¸´à¸ à¸²à¸à¸—à¸µà¹ˆà¹à¸¢à¹ˆà¸—à¸µà¹ˆà¸ªà¸¸à¸” à¹‚à¸”à¸¢à¸¡à¸µà¸„à¹ˆà¸²à¸„à¸§à¸²à¸¡à¹à¸¡à¹ˆà¸™à¸¢à¸³à¹à¸¥à¸° F1-score à¹€à¸›à¹‡à¸™ 0% à¹à¸¥à¸° WER à¹€à¸›à¹‡à¸™ 100% à¸‹à¸¶à¹ˆà¸‡à¸šà¹ˆà¸‡à¸Šà¸µà¹‰à¸–à¸¶à¸‡à¸›à¸±à¸à¸«à¸²à¸ªà¸³à¸„à¸±à¸à¹ƒà¸™à¸à¸²à¸£à¹ƒà¸Šà¹‰à¸‡à¸²à¸™à¸«à¸£à¸·à¸­à¸„à¸§à¸²à¸¡à¹„à¸¡à¹ˆà¹€à¸«à¸¡à¸²à¸°à¸ªà¸¡à¸‚à¸­à¸‡à¹à¸™à¸§à¸—à¸²à¸‡à¸™à¸µà¹‰à¸ªà¸³à¸«à¸£à¸±à¸š TrOCR\n",
        "    *   à¹à¸™à¸§à¸—à¸²à¸‡ **Pure OCR (Spatial)** à¸‚à¸­à¸‡ **Pytesseract** à¸à¹‡à¹à¸ªà¸”à¸‡à¸›à¸£à¸°à¸ªà¸´à¸—à¸˜à¸´à¸ à¸²à¸à¸—à¸µà¹ˆà¸„à¹ˆà¸­à¸™à¸‚à¹‰à¸²à¸‡à¸•à¹ˆà¸³à¹€à¸Šà¹ˆà¸™à¸à¸±à¸™\n",
        "\n",
        "2.  **à¸„à¸§à¸²à¸¡à¸ªà¸³à¸„à¸±à¸à¸‚à¸­à¸‡à¹à¸™à¸§à¸—à¸²à¸‡ Boxing:**\n",
        "    *   à¹à¸™à¸§à¸—à¸²à¸‡ **Boxing** à¹ƒà¸«à¹‰à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œà¸—à¸µà¹ˆà¸”à¸µà¸—à¸µà¹ˆà¸ªà¸¸à¸”à¸­à¸¢à¹ˆà¸²à¸‡à¸ªà¸¡à¹ˆà¸³à¹€à¸ªà¸¡à¸­à¸ªà¸³à¸«à¸£à¸±à¸šà¸—à¸¸à¸à¹‚à¸¡à¹€à¸”à¸¥à¸—à¸µà¹ˆà¸—à¸”à¸ªà¸­à¸š (Pytesseract, EasyOCR, TrOCR) à¹ƒà¸™à¸—à¸¸à¸à¸•à¸±à¸§à¸Šà¸µà¹‰à¸§à¸±à¸”à¸›à¸£à¸°à¸ªà¸´à¸—à¸˜à¸´à¸ à¸²à¸ (Accuracy, WER, F1-score)\n",
        "    *   à¸ªà¸´à¹ˆà¸‡à¸™à¸µà¹‰à¹€à¸™à¹‰à¸™à¸¢à¹‰à¸³à¸–à¸¶à¸‡à¸„à¸§à¸²à¸¡à¸ªà¸³à¸„à¸±à¸à¸‚à¸­à¸‡à¸à¸²à¸£à¸£à¸°à¸šà¸¸à¸‚à¸­à¸šà¹€à¸‚à¸•à¸‚à¸­à¸‡à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸­à¸¢à¹ˆà¸²à¸‡à¹à¸¡à¹ˆà¸™à¸¢à¸³à¸à¹ˆà¸­à¸™à¸à¸²à¸£à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥ OCR à¸‹à¸¶à¹ˆà¸‡à¸ªà¹ˆà¸‡à¸œà¸¥à¹‚à¸”à¸¢à¸•à¸£à¸‡à¸•à¹ˆà¸­à¸„à¸§à¸²à¸¡à¸–à¸¹à¸à¸•à¹‰à¸­à¸‡à¸‚à¸­à¸‡à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œ\n",
        "\n",
        "3.  **à¸‚à¹‰à¸­à¸ªà¸±à¸‡à¹€à¸à¸•à¹€à¸à¸µà¹ˆà¸¢à¸§à¸à¸±à¸š CER (%):**\n",
        "    *   à¸„à¹ˆà¸²à¸­à¸±à¸•à¸£à¸²à¸‚à¹‰à¸­à¸œà¸´à¸”à¸à¸¥à¸²à¸”à¸‚à¸­à¸‡à¸­à¸±à¸à¸‚à¸£à¸° (CER) à¹€à¸›à¹‡à¸™ 0% à¸ªà¸³à¸«à¸£à¸±à¸šà¸—à¸¸à¸à¹‚à¸¡à¹€à¸”à¸¥à¹à¸¥à¸°à¹à¸™à¸§à¸—à¸²à¸‡ à¸‹à¸¶à¹ˆà¸‡à¹€à¸›à¹‡à¸™à¹€à¸£à¸·à¹ˆà¸­à¸‡à¸œà¸´à¸”à¸›à¸à¸•à¸´à¹à¸¥à¸°à¸­à¸²à¸ˆà¸šà¹ˆà¸‡à¸Šà¸µà¹‰à¸§à¹ˆà¸²à¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œà¸™à¸µà¹‰à¹„à¸¡à¹ˆà¹„à¸”à¹‰à¸ªà¸°à¸—à¹‰à¸­à¸™à¸–à¸¶à¸‡à¸„à¸§à¸²à¸¡à¹à¸•à¸à¸•à¹ˆà¸²à¸‡à¸‚à¸­à¸‡à¸›à¸£à¸°à¸ªà¸´à¸—à¸˜à¸´à¸ à¸²à¸à¸—à¸µà¹ˆà¹à¸—à¹‰à¸ˆà¸£à¸´à¸‡ à¸«à¸£à¸·à¸­à¸­à¸²à¸ˆà¸¡à¸µà¸‚à¹‰à¸­à¸œà¸´à¸”à¸à¸¥à¸²à¸”à¹ƒà¸™à¸à¸²à¸£à¸£à¸§à¸šà¸£à¸§à¸¡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸«à¸£à¸·à¸­à¸à¸²à¸£à¸„à¸³à¸™à¸§à¸“\n",
        "\n",
        "**à¸ªà¸£à¸¸à¸›:**\n",
        "\n",
        "à¹‚à¸”à¸¢à¸£à¸§à¸¡à¹à¸¥à¹‰à¸§ à¹à¸™à¸§à¸—à¸²à¸‡ **Boxing** à¹€à¸›à¹‡à¸™à¹à¸™à¸§à¸—à¸²à¸‡à¸—à¸µà¹ˆà¸¡à¸µà¸›à¸£à¸°à¸ªà¸´à¸—à¸˜à¸´à¸ à¸²à¸à¸ªà¸¹à¸‡à¸ªà¸¸à¸”à¸ªà¸³à¸«à¸£à¸±à¸šà¸‡à¸²à¸™ OCR à¹ƒà¸™à¸Šà¸¸à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸™à¸µà¹‰ à¹‚à¸”à¸¢à¹€à¸‰à¸à¸²à¸°à¸­à¸¢à¹ˆà¸²à¸‡à¸¢à¸´à¹ˆà¸‡à¹€à¸¡à¸·à¹ˆà¸­à¹ƒà¸Šà¹‰à¸£à¹ˆà¸§à¸¡à¸à¸±à¸šà¹‚à¸¡à¹€à¸”à¸¥ **Pytesseract** à¹à¸¥à¸° **TrOCR**\n",
        "\n",
        "à¸™à¸µà¹ˆà¸„à¸·à¸­à¹à¸œà¸™à¸ à¸¹à¸¡à¸´à¸—à¸µà¹ˆà¹à¸ªà¸”à¸‡à¸„à¹ˆà¸²à¹€à¸‰à¸¥à¸µà¹ˆà¸¢à¸„à¸§à¸²à¸¡à¹à¸¡à¹ˆà¸™à¸¢à¸³, à¸­à¸±à¸•à¸£à¸²à¸‚à¹‰à¸­à¸œà¸´à¸”à¸à¸¥à¸²à¸”à¸‚à¸­à¸‡à¸„à¸³ (WER), à¹à¸¥à¸° F1-score à¸•à¸²à¸¡à¹‚à¸¡à¹€à¸”à¸¥à¹à¸¥à¸°à¹à¸™à¸§à¸—à¸²à¸‡:\n",
        "\n",
        "*   **Average Accuracy by Model and Approach**\n",
        "*   **Average Word Error Rate (WER) by Model and Approach**\n",
        "*   **Average F1-score by Model and Approach**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "ZchumLaT2_-Z",
        "b7zujbWD5iSE",
        "VEo-U66vu107",
        "G0pRHBRmjpUV"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}