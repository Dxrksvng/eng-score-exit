{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dxrksvng/eng-score-exit/blob/main/ktep_oldversion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RUN all ใหม่แล้ว GPU หมดขีดจำกัดจร้า ระเบิ้ดเบิ้ดแหล่ว**"
      ],
      "metadata": {
        "id": "wdBzxjSQBRyp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fojeGgi4JNa",
        "outputId": "984ad326-da93-4fe7-a7c4-6db0449cc652"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZchumLaT2_-Z"
      },
      "source": [
        "# SET UP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "tVLhvN9S2_xM",
        "outputId": "b23d62e0-5509-4fdf-bf61-e59cbecf5f2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdf2image\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from pdf2image) (11.2.1)\n",
            "Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pdf2image\n",
            "Successfully installed pdf2image-1.17.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 697 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.8 [186 kB]\n",
            "Fetched 186 kB in 0s (406 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 126111 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.8_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.8) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.8) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting jiwer\n",
            "  Downloading jiwer-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from jiwer) (8.2.1)\n",
            "Collecting rapidfuzz>=3.9.7 (from jiwer)\n",
            "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Downloading jiwer-3.1.0-py3-none-any.whl (22 kB)\n",
            "Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\n",
            "Successfully installed jiwer-3.1.0 rapidfuzz-3.13.0\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (24.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (11.2.1)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.13\n",
            "Collecting easyocr\n",
            "  Downloading easyocr-1.7.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from easyocr) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.11/dist-packages (from easyocr) (0.21.0+cu124)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.11/dist-packages (from easyocr) (4.11.0.86)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from easyocr) (1.15.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from easyocr) (2.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from easyocr) (11.2.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from easyocr) (0.25.2)\n",
            "Collecting python-bidi (from easyocr)\n",
            "  Downloading python_bidi-0.6.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from easyocr) (6.0.2)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.11/dist-packages (from easyocr) (2.1.1)\n",
            "Collecting pyclipper (from easyocr)\n",
            "  Downloading pyclipper-1.3.0.post6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Collecting ninja (from easyocr)\n",
            "  Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->easyocr)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->easyocr)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->easyocr)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->easyocr)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->easyocr)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->easyocr)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->easyocr)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->easyocr)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->easyocr)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->easyocr)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->easyocr) (1.3.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr) (2025.6.1)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr) (0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->easyocr) (3.0.2)\n",
            "Downloading easyocr-1.7.2-py3-none-any.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m121.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyclipper-1.3.0.post6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (969 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m969.6/969.6 kB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_bidi-0.6.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (292 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.9/292.9 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-bidi, pyclipper, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ninja, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, easyocr\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed easyocr-1.7.2 ninja-1.11.1.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pyclipper-1.3.0.post6 python-bidi-0.6.6\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.32.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"
          ]
        }
      ],
      "source": [
        "!pip install pdf2image\n",
        "!apt-get install poppler-utils\n",
        "!pip install jiwer\n",
        "!pip install pytesseract\n",
        "!pip install easyocr\n",
        "!pip install transformers torch sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7zujbWD5iSE"
      },
      "source": [
        "# Tranfrom to image (.png) AUTO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "vAtBHGjn5m0r"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from pdf2image import convert_from_path\n",
        "\n",
        "# --- 1. การตั้งค่า ---\n",
        "\n",
        "# โฟลเดอร์ที่เก็บไฟล์ต้นทาง (PDF, JPG, etc.)\n",
        "INPUT_DIR = '/content/drive/MyDrive/kmitl_dataset/dataset'\n",
        "\n",
        "# โฟลเดอร์สำหรับเก็บไฟล์ภาพ .png ที่แปลงเสร็จแล้ว\n",
        "OUTPUT_DIR = '/content/drive/MyDrive/kmitl_dataset/final-tran-to-img'\n",
        "\n",
        "# --- 2. โค้ดหลักในการแปลงไฟล์ ---\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    ฟังก์ชันหลักที่จะวนลูปเพื่อแปลงไฟล์ทั้งหมด\n",
        "    \"\"\"\n",
        "    # สร้างโฟลเดอร์ปลายทางหากยังไม่มี\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "    # ค้นหาไฟล์ทั้งหมดที่ต้องการแปลงในโฟลเดอร์ต้นทาง\n",
        "    files_to_process = [f for f in os.listdir(INPUT_DIR) if f.lower().endswith(('.pdf', '.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "    if not files_to_process:\n",
        "        print(f\"ไม่พบไฟล์ที่สามารถแปลงได้ในโฟลเดอร์: {INPUT_DIR}\")\n",
        "        return\n",
        "\n",
        "    print(f\"พบไฟล์ทั้งหมด {len(files_to_process)} ไฟล์ เริ่มดำเนินการแปลง...\")\n",
        "\n",
        "    # วนลูปทำงานทีละไฟล์\n",
        "    for filename in files_to_process:\n",
        "        try:\n",
        "            filepath = os.path.join(INPUT_DIR, filename)\n",
        "\n",
        "            # --- ส่วนของการแปลงไฟล์ ---\n",
        "            if filename.lower().endswith('.pdf'):\n",
        "                # ถ้าเป็น PDF ให้ใช้ pdf2image แปลงหน้าแรก\n",
        "                images = convert_from_path(filepath)\n",
        "                bgr_image = np.array(images[0])\n",
        "                bgr_image = cv2.cvtColor(bgr_image, cv2.COLOR_RGB2BGR)\n",
        "            else:\n",
        "                # ถ้าเป็นไฟล์ภาพอยู่แล้ว ให้ใช้ OpenCV อ่านเข้ามาโดยตรง\n",
        "                bgr_image = cv2.imread(filepath)\n",
        "\n",
        "            # ตรวจสอบว่าโหลดภาพสำเร็จหรือไม่\n",
        "            if bgr_image is None:\n",
        "                print(f\"  -> ไม่สามารถอ่านไฟล์: {filename}, ขอข้ามไฟล์นี้\")\n",
        "                continue\n",
        "\n",
        "            # --- ส่วนของการบันทึกไฟล์ ---\n",
        "            # สร้างชื่อไฟล์ใหม่ให้เป็น .png เสมอ\n",
        "            new_filename = f\"{os.path.splitext(filename)[0]}.png\"\n",
        "            save_path = os.path.join(OUTPUT_DIR, new_filename)\n",
        "\n",
        "            # บันทึกไฟล์ภาพ\n",
        "            cv2.imwrite(save_path, bgr_image)\n",
        "\n",
        "            print(f\"  -> แปลงไฟล์ '{filename}' เป็น '{new_filename}' เรียบร้อยแล้ว\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  !!!! เกิดข้อผิดพลาดกับไฟล์ {filename}: {e}\")\n",
        "\n",
        "    print(\"-\" * 50)\n",
        "    print(\"การแปลงไฟล์ทั้งหมดเสร็จสิ้น!\")\n",
        "    print(f\"ไฟล์ภาพ .png ทั้งหมดถูกบันทึกไว้ที่: {OUTPUT_DIR}\")\n",
        "\n",
        "\n",
        "# สั่งให้โปรแกรมเริ่มทำงานเมื่อรันไฟล์นี้\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEo-U66vu107"
      },
      "source": [
        "# Preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qewHtLn_CzPp"
      },
      "source": [
        "**Test single image**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "CzZMOIxQu51F"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# อ่านภาพ\n",
        "image = cv2.imread('/content/drive/MyDrive/kmitl_dataset/final-tran-to-img/KMITL-TEP PILOT-1.png')  # ใส่ชื่อไฟล์ภาพที่ต้องการ\n",
        "\n",
        "# แปลงภาพเป็น HSV เพื่อแยกเฉพาะสีขาว\n",
        "hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "## กำหนดช่วงสีดำใน HSV\n",
        "lower_black = np.array([0, 0, 0])  # ค่าสีดำต่ำสุด ดำสนิท\n",
        "upper_black = np.array([255, 255, 110])  # ค่าสีดำสูงสุด ดำอ่อน\n",
        "# H สี\n",
        "#S ความเข้มสี 0-255 (ไม่สด-สดมาก)\n",
        "#V ความสว่าง ดำ-สว่างสุด\n",
        "\n",
        "\n",
        "# กำหนดช่วงสีน้ำเงินใน HSV\n",
        "lower_blue = np.array([90, 30, 50])  # ค่าสีน้ำเงินต่ำสุด\n",
        "upper_blue = np.array([130, 255, 255])  # ค่าสีน้ำเงินสูงสุด\n",
        "\n",
        "# สร้าง Mask สำหรับสีดำและน้ำเงิน\n",
        "mask_black = cv2.inRange(hsv, lower_black, upper_black)\n",
        "mask_blue = cv2.inRange(hsv, lower_blue, upper_blue)\n",
        "\n",
        "# รวม Mask สีดำและน้ำเงินเข้าด้วยกัน\n",
        "mask = cv2.bitwise_or(mask_black, mask_blue)\n",
        "\n",
        "# สร้างพื้นหลังที่มีสีเฉลี่ยของภาพ\n",
        "background_color = cv2.mean(image, mask=cv2.bitwise_not(mask))[:3]  # หาค่าสีพื้นหลัง\n",
        "background = np.full_like(image, background_color, dtype=np.uint8)\n",
        "\n",
        "# แทนที่ส่วนที่ไม่ใช่ตัวอักษรด้วยพื้นหลัง\n",
        "inverted_mask = cv2.bitwise_not(mask)\n",
        "result = cv2.bitwise_and(background, background, mask=inverted_mask)\n",
        "final_result = cv2.bitwise_or(image & mask[:, :, None], result)\n",
        "\n",
        "# แสดงผลลัพธ์\n",
        "cv2_imshow(image)\n",
        "cv2_imshow(final_result)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "# บันทึกภาพผลลัพธ์\n",
        "cv2.imwrite('output.png', final_result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxYJUmdpu1zR"
      },
      "source": [
        "**AUTO Preprocess**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "NCtFn55mu1Vh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# --- 1. ส่วนของการตั้งค่า ---\n",
        "\n",
        "# โฟลเดอร์ที่เก็บไฟล์ภาพต้นทาง\n",
        "INPUT_DIR = '/content/drive/MyDrive/kmitl_dataset/final-tran-to-img'\n",
        "\n",
        "# โฟลเดอร์สำหรับเก็บภาพที่ประมวลผลเสร็จแล้ว\n",
        "OUTPUT_DIR = '/content/drive/MyDrive/kmitl_dataset/final-preprecessed'\n",
        "\n",
        "\n",
        "# ---. ฟังก์ชันประมวลผลภาพ (นำมาจากโค้ดของคุณ) ---\n",
        "\n",
        "def process_image_with_color_mask(image):\n",
        "    \"\"\"\n",
        "    ประมวลผลภาพโดยใช้เทคนิคการแยกสี (Color Masking)\n",
        "    เพื่อคงไว้เฉพาะตัวอักษรสีดำและน้ำเงิน\n",
        "    \"\"\"\n",
        "    # แปลงภาพเป็น HSV\n",
        "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "    # กำหนดช่วงสีดำใน HSV\n",
        "    lower_black = np.array([0, 0, 0])\n",
        "    upper_black = np.array([255, 255, 110])\n",
        "\n",
        "    # กำหนดช่วงสีน้ำเงินใน HSV\n",
        "    lower_blue = np.array([90, 30, 50])\n",
        "    upper_blue = np.array([130, 255, 255])\n",
        "\n",
        "    # สร้าง Mask สำหรับสีดำและน้ำเงิน\n",
        "    mask_black = cv2.inRange(hsv, lower_black, upper_black)\n",
        "    mask_blue = cv2.inRange(hsv, lower_blue, upper_blue)\n",
        "\n",
        "    # รวม Mask ทั้งสองสีเข้าด้วยกัน\n",
        "    mask = cv2.bitwise_or(mask_black, mask_blue)\n",
        "\n",
        "    # สร้างพื้นหลังสีขาว (เพื่อให้ OCR อ่านง่าย)\n",
        "    background = np.full_like(image, (255, 255, 255), dtype=np.uint8)\n",
        "\n",
        "    # ใช้ mask เพื่อคงไว้เฉพาะส่วนที่เป็นตัวอักษร (สีดำและน้ำเงิน)\n",
        "    # และแทนที่ส่วนอื่นด้วยพื้นหลังสีขาว\n",
        "    inverted_mask = cv2.bitwise_not(mask)\n",
        "    background_part = cv2.bitwise_and(background, background, mask=inverted_mask)\n",
        "    text_part = cv2.bitwise_and(image, image, mask=mask)\n",
        "\n",
        "    final_result = cv2.add(text_part, background_part)\n",
        "\n",
        "    return final_result\n",
        "\n",
        "# --- 3. โค้ดหลักสำหรับทำงานแบบอัตโนมัติ ---\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    วนลูปทำงานกับทุกไฟล์ในโฟลเดอร์\n",
        "    \"\"\"\n",
        "    # สร้างโฟลเดอร์ปลายทางหากยังไม่มี\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "    # ค้นหาไฟล์ทั้งหมดในโฟลเดอร์ต้นทาง\n",
        "    files_to_process = [f for f in os.listdir(INPUT_DIR) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "    if not files_to_process:\n",
        "        print(f\"ไม่พบไฟล์ภาพในโฟลเดอร์: {INPUT_DIR}\")\n",
        "        return\n",
        "\n",
        "    print(f\"พบไฟล์ทั้งหมด {len(files_to_process)} ไฟล์ เริ่มดำเนินการ...\")\n",
        "\n",
        "    # วนลูปทำงานทีละไฟล์\n",
        "    for filename in files_to_process:\n",
        "        try:\n",
        "            filepath = os.path.join(INPUT_DIR, filename)\n",
        "\n",
        "            # อ่านไฟล์ภาพ\n",
        "            original_image = cv2.imread(filepath)\n",
        "\n",
        "            if original_image is None:\n",
        "                print(f\"  -> ไม่สามารถอ่านไฟล์: {filename}, ขอข้ามไฟล์นี้\")\n",
        "                continue\n",
        "\n",
        "            # ส่งภาพไปประมวลผลด้วยฟังก์ชันของคุณ\n",
        "            processed_image = process_image_with_color_mask(original_image)\n",
        "\n",
        "            # บันทึกผลลัพธ์\n",
        "            save_path = os.path.join(OUTPUT_DIR, filename)\n",
        "            cv2.imwrite(save_path, processed_image)\n",
        "\n",
        "            print(f\"  -> ประมวลผลไฟล์ '{filename}' เรียบร้อยแล้ว\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  !!!! เกิดข้อผิดพลาดกับไฟล์ {filename}: {e}\")\n",
        "\n",
        "    print(\"-\" * 50)\n",
        "    print(\"การประมวลผลทั้งหมดเสร็จสิ้น!\")\n",
        "    print(f\"ไฟล์ที่ประมวลผลแล้วถูกบันทึกไว้ที่: {OUTPUT_DIR}\")\n",
        "\n",
        "\n",
        "# สั่งให้โปรแกรมเริ่มทำงาน\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0pRHBRmjpUV"
      },
      "source": [
        "# OCR AUTOMATE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jm13ks44kY2Y"
      },
      "source": [
        "**pytesseract**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Xzi8fHaGXvAX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2016e3c7-6bf9-4843-f09e-0021a29e9630"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 🔍 ขั้นตอนตรวจสอบข้อมูล ---\n",
            "พบ ID ใน Ground Truth ทั้งหมด: 120 ID | พบไฟล์รูปภาพ: 120 ไฟล์\n",
            "--- ✅ ตรวจสอบข้อมูลเสร็จสิ้น ---\n",
            "\n",
            "--- 🚀 เริ่มการสกัดข้อมูลจากรูปภาพ 120 ไฟล์ ด้วย Pytesseract (Pure OCR) ---\n",
            "  - กำลังประมวลผล ID: 1 (KMITL-TEP PILOT-1.png)\n",
            "  - กำลังประมวลผล ID: 2 (KMITL-TEP PILOT-2.png)\n",
            "  - กำลังประมวลผล ID: 3 (KMITL-TEP PILOT-3.png)\n",
            "  - กำลังประมวลผล ID: 4 (KMITL-TEP PILOT-4.png)\n",
            "  - กำลังประมวลผล ID: 5 (KMITL-TEP PILOT-5.png)\n",
            "  - กำลังประมวลผล ID: 6 (KMITL-TEP PILOT-6.png)\n",
            "  - กำลังประมวลผล ID: 7 (KMITL-TEP PILOT-7.png)\n",
            "  - กำลังประมวลผล ID: 8 (KMITL-TEP PILOT-8.png)\n",
            "  - กำลังประมวลผล ID: 9 (KMITL-TEP PILOT-9.png)\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import jiwer\n",
        "import numpy as np\n",
        "import pytesseract\n",
        "\n",
        "# --- 1. กำหนดค่าและ Path ที่สำคัญ ---\n",
        "MODEL_NAME = 'Pytesseract'\n",
        "APPROACH_NAME = 'Pure OCR'\n",
        "MASTER_OUTPUT_PATH = \"/content/drive/MyDrive/kmitl_dataset/final-excel/final_comparison_report.xlsx\"\n",
        "INDIVIDUAL_REPORT_PATH = f\"/content/drive/MyDrive/kmitl_dataset/final-excel/{MODEL_NAME.lower()}_{APPROACH_NAME.replace(' ', '_').lower()}_report.xlsx\"\n",
        "\n",
        "IMAGE_DIR = \"/content/drive/MyDrive/kmitl_dataset/final-preprecessed\"\n",
        "GROUND_TRUTH_PATH = \"/content/drive/MyDrive/kmitl_dataset/dataset/คะแนน TEP (Pilot Study)_Total_IT.xlsx\"\n",
        "\n",
        "\n",
        "# --- 2. ฟังก์ชันช่วยเหลือ ---\n",
        "\n",
        "def parse_score(text):\n",
        "    if not text: return None, None\n",
        "    match = re.search(r'([A-Z][1-2])\\s*\\((\\d+)\\)', text, re.IGNORECASE)\n",
        "    if match: return match.group(1).upper(), match.group(2)\n",
        "    return None, None\n",
        "\n",
        "def get_id_from_filename(filename):\n",
        "    match = re.search(r'(\\d+)\\.(png|jpg|jpeg)$', filename.lower())\n",
        "    if match: return int(match.group(1))\n",
        "    return None\n",
        "\n",
        "def normalize_text(text):\n",
        "    if text is None: return \"\"\n",
        "    text = str(text).strip().lower()\n",
        "    if text.endswith('.0'): text = text[:-2]\n",
        "    return text\n",
        "\n",
        "# ⬇️⬇️ นำฟังก์ชันของคุณมาปรับปรุงและเพิ่มการสกัด Total Score ⬇️⬇️\n",
        "def parse_pytesseract_pure_ocr(text):\n",
        "    result = {}\n",
        "    valid_levels = {\"A1\", \"A2\", \"B1\", \"B2\", \"C1\", \"C2\"}\n",
        "    lines = [l.strip().replace('’', \"'\").replace('–', '-').replace('!', '1').replace('I', '1').replace('l', '1') for l in text.split('\\n') if l.strip()]\n",
        "    prev = \"\"\n",
        "\n",
        "    # ใช้ Regex กับข้อความทั้งหมดก่อนเพื่อความแม่นยำ\n",
        "    full_text = \"\\n\".join(lines)\n",
        "\n",
        "    # Name\n",
        "    name_match = re.search(r'Name\\s*:?\\s*(.*?)\\s*Application', full_text, re.DOTALL | re.IGNORECASE)\n",
        "    if name_match: result[\"name\"] = name_match.group(1).strip()\n",
        "\n",
        "    # Application No.\n",
        "    app_no_match = re.search(r'Application\\s*No\\.?\\s*:*\\s*(\\S+)', full_text, re.IGNORECASE)\n",
        "    if app_no_match: result[\"application_no\"] = app_no_match.group(1).strip()\n",
        "\n",
        "    # Date\n",
        "    date_match = re.search(r'Date of test Administration\\s*:?\\s*(.+)', full_text, re.IGNORECASE)\n",
        "    if date_match:\n",
        "        # เอาแค่บรรทัดแรกของผลลัพธ์เผื่อมีการขึ้นบรรทัดใหม่ผิดพลาด\n",
        "        result[\"test_date\"] = date_match.group(1).strip().split('\\n')[0]\n",
        "\n",
        "    # Skill Scores (ใช้ Logic เดิมของคุณเป็นฐาน แต่ค้นหาจาก full_text)\n",
        "    score_pattern_full = r'([A-Z][1-2]\\s*\\(\\d+\\))'\n",
        "    for skill in [\"Grammar\", \"Reading\", \"Speaking\", \"Writing\"]:\n",
        "        m = re.search(rf\"{skill}.*?({score_pattern_full})\", full_text, re.IGNORECASE)\n",
        "        if m:\n",
        "            # แยก Level และ Score จากผลลัพธ์\n",
        "            score_text = m.group(1)\n",
        "            level, score = parse_score(score_text)\n",
        "            if level in valid_levels:\n",
        "                result[f\"{skill.lower()}_level\"] = level\n",
        "                result[f\"{skill.lower()}_score\"] = score\n",
        "\n",
        "    # Total Score (เพิ่มตรรกะใหม่)\n",
        "    all_scores_text = re.findall(score_pattern_full, full_text, re.IGNORECASE)\n",
        "    used_scores_text = []\n",
        "    for skill in [\"grammar\", \"reading\", \"speaking\", \"writing\"]:\n",
        "        if f\"{skill}_level\" in result:\n",
        "             used_scores_text.append(f\"{result[f'{skill}_level']} ({result[f'{skill}_score']})\")\n",
        "\n",
        "    remaining_scores = [s for s in all_scores_text if s not in used_scores_text]\n",
        "    if remaining_scores:\n",
        "        total_level, total_score = parse_score(remaining_scores[0])\n",
        "        result[\"total_level\"] = total_level\n",
        "        result[\"total_score\"] = total_score\n",
        "\n",
        "    return result\n",
        "\n",
        "# --- 3. โหลดและตรวจสอบข้อมูลก่อนเริ่ม ---\n",
        "print(\"--- 🔍 ขั้นตอนตรวจสอบข้อมูล ---\")\n",
        "gt_df = pd.read_excel(GROUND_TRUTH_PATH)\n",
        "ground_truth_ids = set(gt_df['No.'].unique())\n",
        "image_files = sorted([f for f in os.listdir(IMAGE_DIR) if f.lower().endswith(('.png', '.jpg', '.jpeg'))], key=get_id_from_filename)\n",
        "print(f\"พบ ID ใน Ground Truth ทั้งหมด: {len(ground_truth_ids)} ID | พบไฟล์รูปภาพ: {len(image_files)} ไฟล์\")\n",
        "print(\"--- ✅ ตรวจสอบข้อมูลเสร็จสิ้น ---\\n\")\n",
        "\n",
        "# --- 4. เริ่มกระบวนการสกัดข้อมูล ---\n",
        "results_list = []\n",
        "print(f\"--- 🚀 เริ่มการสกัดข้อมูลจากรูปภาพ {len(image_files)} ไฟล์ ด้วย {MODEL_NAME} ({APPROACH_NAME}) ---\")\n",
        "for filename in image_files:\n",
        "    image_id = get_id_from_filename(filename)\n",
        "    if image_id is None: continue\n",
        "    gt_row = gt_df[gt_df['No.'] == image_id]\n",
        "    if gt_row.empty: continue\n",
        "\n",
        "    print(f\"  - กำลังประมวลผล ID: {image_id} ({filename})\")\n",
        "    image_path = os.path.join(IMAGE_DIR, filename)\n",
        "    img = cv2.imread(image_path)\n",
        "\n",
        "    custom_config = r'--oem 3 --psm 6'\n",
        "    text = pytesseract.image_to_string(img, config=custom_config)\n",
        "\n",
        "    extracted_raw = parse_pytesseract_pure_ocr(text)\n",
        "\n",
        "    # นำข้อมูลที่สกัดได้มาจัดลง Dictionary\n",
        "    results_list.append({\n",
        "        \"No.\": image_id,\n",
        "        \"Application No. (GT)\": gt_row.iloc[0][\"Application No.\"], \"Application No. (Pred)\": extracted_raw.get('application_no'),\n",
        "        \"Name (GT)\": gt_row.iloc[0][\"Name\"], \"Name (Pred)\": extracted_raw.get('name'),\n",
        "        \"Test Date (GT)\": gt_row.iloc[0][\"Test Date\"], \"Test Date (Pred)\": extracted_raw.get('test_date'),\n",
        "        \"Grammar_Level (GT)\": gt_row.iloc[0][\"Grammar_Level\"], \"Grammar_Level (Pred)\": extracted_raw.get('grammar_level'),\n",
        "        \"Grammar_Score (GT)\": gt_row.iloc[0][\"Grammar_Score\"], \"Grammar_Score (Pred)\": extracted_raw.get('grammar_score'),\n",
        "        \"Reading_Level (GT)\": gt_row.iloc[0][\"Reading_Level\"], \"Reading_Level (Pred)\": extracted_raw.get('reading_level'),\n",
        "        \"Reading_Score (GT)\": gt_row.iloc[0][\"Reading_Score\"], \"Reading_Score (Pred)\": extracted_raw.get('reading_score'),\n",
        "        \"Speaking_Level (GT)\": gt_row.iloc[0][\"Speaking_Level\"], \"Speaking_Level (Pred)\": extracted_raw.get('speaking_level'),\n",
        "        \"Speaking_Score (GT)\": gt_row.iloc[0][\"Speaking_Score\"], \"Speaking_Score (Pred)\": extracted_raw.get('speaking_score'),\n",
        "        \"Writing_Level (GT)\": gt_row.iloc[0][\"Writing_Level\"], \"Writing_Level (Pred)\": extracted_raw.get('writing_level'),\n",
        "        \"Writing_Score (GT)\": gt_row.iloc[0][\"Writing_Score\"], \"Writing_Score (Pred)\": extracted_raw.get('writing_score'),\n",
        "        \"Total_Level (GT)\": gt_row.iloc[0][\"Total_Level\"], \"Total_Level (Pred)\": extracted_raw.get('total_level'),\n",
        "        \"Total_Score (GT)\": gt_row.iloc[0][\"Total_Score\"], \"Total_Score (Pred)\": extracted_raw.get('total_score'),\n",
        "    })\n",
        "\n",
        "ocr_results_df = pd.DataFrame(results_list)\n",
        "print(\"--- ✅ สกัดข้อมูลเสร็จสิ้น ---\\n\")\n",
        "\n",
        "\n",
        "# --- 5. เริ่มกระบวนการประเมินผล (Evaluation) ---\n",
        "# ... (โค้ดส่วนนี้เหมือนกับเวอร์ชันก่อนหน้าทุกประการ) ...\n",
        "print(\"--- 📊 เริ่มการประเมินผล ---\")\n",
        "eval_df = ocr_results_df.fillna('')\n",
        "fields_to_evaluate = {\n",
        "    'Name': ('Name (GT)', 'Name (Pred)'), 'Application No.': ('Application No. (GT)', 'Application No. (Pred)'),\n",
        "    'Test Date': ('Test Date (GT)', 'Test Date (Pred)'), 'Grammar_Level': ('Grammar_Level (GT)', 'Grammar_Level (Pred)'),\n",
        "    'Grammar_Score': ('Grammar_Score (GT)', 'Grammar_Score (Pred)'), 'Reading_Level': ('Reading_Level (GT)', 'Reading_Level (Pred)'),\n",
        "    'Reading_Score': ('Reading_Score (GT)', 'Reading_Score (Pred)'), 'Speaking_Level': ('Speaking_Level (GT)', 'Speaking_Level (Pred)'),\n",
        "    'Speaking_Score': ('Speaking_Score (GT)', 'Speaking_Score (Pred)'), 'Writing_Level': ('Writing_Level (GT)', 'Writing_Level (Pred)'),\n",
        "    'Writing_Score': ('Writing_Score (GT)', 'Writing_Score (Pred)'), 'Total_Level': ('Total_Level (GT)', 'Total_Level (Pred)'),\n",
        "    'Total_Score': ('Total_Score (GT)', 'Total_Score (Pred)'),\n",
        "}\n",
        "evaluation_summary_list = []\n",
        "for field, (gt_col, pred_col) in fields_to_evaluate.items():\n",
        "    ground_truth = [normalize_text(t) for t in eval_df[gt_col]]\n",
        "    prediction = [normalize_text(t) for t in eval_df[pred_col]]\n",
        "    accuracy = np.mean([1 if gt == pred else 0 for gt, pred in zip(ground_truth, prediction)]) * 100\n",
        "    error_metrics = jiwer.compute_measures(ground_truth, prediction)\n",
        "    wer = error_metrics.get('wer', 0) * 100\n",
        "    cer = error_metrics.get('cer', 0) * 100\n",
        "    H = error_metrics.get('hits', 0); I = error_metrics.get('insertions', 0); D = error_metrics.get('deletions', 0); S = error_metrics.get('substitutions', 0)\n",
        "    precision = H / (H + I + S) if (H + I + S) > 0 else 0\n",
        "    recall = H / (H + D + S) if (H + D + S) > 0 else 0\n",
        "    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    evaluation_summary_list.append({\n",
        "        'Field': field, 'Accuracy (%)': round(accuracy, 2), 'WER (%)': round(wer, 2),\n",
        "        'CER (%)': round(cer, 2), 'F1-score (%)': round(f1_score * 100, 2)\n",
        "    })\n",
        "eval_summary_df = pd.DataFrame(evaluation_summary_list)\n",
        "eval_summary_df.insert(0, 'Approach', APPROACH_NAME)\n",
        "eval_summary_df.insert(0, 'Model', MODEL_NAME)\n",
        "print(eval_summary_df.to_string(index=False))\n",
        "print(\"--- ✅ ประเมินผลเสร็จสิ้น ---\\n\")\n",
        "\n",
        "\n",
        "# --- 6. บันทึกผลลัพธ์ (ทำ 2 อย่าง) ---\n",
        "# --- 6.A: บันทึกผลการทดลองนี้แยกไฟล์ ---\n",
        "print(f\"--- 💾 กำลังบันทึกรายงานเฉพาะของ {MODEL_NAME} ({APPROACH_NAME}) ลงในไฟล์: {INDIVIDUAL_REPORT_PATH} ---\")\n",
        "with pd.ExcelWriter(INDIVIDUAL_REPORT_PATH, engine='openpyxl') as writer:\n",
        "    ocr_results_df.to_excel(writer, sheet_name='Detailed_Results', index=False)\n",
        "    eval_summary_df.drop(columns=['Model', 'Approach']).to_excel(writer, sheet_name='Evaluation_Summary', index=False)\n",
        "print(f\"🎉 บันทึกไฟล์ {INDIVIDUAL_REPORT_PATH} สำเร็จ!\")\n",
        "\n",
        "# --- 6.B: อัปเดตข้อมูลลง Master File ---\n",
        "print(f\"--- 💾 กำลังอัปเดตไฟล์ Master Report: {MASTER_OUTPUT_PATH} ---\")\n",
        "SHEET_NAME = 'Master_Evaluation'\n",
        "try:\n",
        "    with pd.ExcelFile(MASTER_OUTPUT_PATH) as xls:\n",
        "        master_df = pd.read_excel(xls, sheet_name=SHEET_NAME)\n",
        "        master_df = master_df[(master_df['Model'] != MODEL_NAME) | (master_df['Approach'] != APPROACH_NAME)]\n",
        "    combined_df = pd.concat([master_df, eval_summary_df], ignore_index=True)\n",
        "except FileNotFoundError:\n",
        "    print(f\"ไม่พบไฟล์ Master เดิม, กำลังสร้างไฟล์ใหม่...\")\n",
        "    combined_df = eval_summary_df\n",
        "with pd.ExcelWriter(MASTER_OUTPUT_PATH, engine='openpyxl') as writer:\n",
        "    combined_df.to_excel(writer, sheet_name=SHEET_NAME, index=False)\n",
        "print(f\"🎉 อัปเดตไฟล์ Master Report สำเร็จ!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdtOu7aSW58l"
      },
      "source": [
        "**ปรับให้มันอ่านค่าได้แม่นๆก่อน**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4oFkaAIiTuqf"
      },
      "outputs": [],
      "source": [
        "# 1. IMPORT LIBRARIES\n",
        "import cv2\n",
        "import pytesseract\n",
        "import re\n",
        "import os\n",
        "import pandas as pd\n",
        "import json # เพิ่มเข้ามาเพื่อพิมพ์ Dictionary ให้อ่านง่าย\n",
        "\n",
        "# 2. ฟังก์ชันสกัดข้อมูลของคุณ (ไม่เปลี่ยนแปลง)\n",
        "def extract_ktep_key_value(text):\n",
        "    result = {}\n",
        "    valid_levels = {\"A1\", \"A2\", \"B1\", \"B2\", \"C1\", \"C2\"}\n",
        "    lines = [l.strip().replace('’', \"'\").replace('–', '-').replace('!', '1').replace('I', '1') for l in text.split('\\n') if l.strip()]\n",
        "    prev = \"\"\n",
        "    # เพิ่ม print(lines) เพื่อช่วยดีบัก ว่า Tesseract อ่านอะไรออกมาบ้าง\n",
        "    print(\"--- ข้อความที่ Tesseract อ่านได้ (แบ่งบรรทัดแล้ว) ---\")\n",
        "    print(lines)\n",
        "    print(\"-------------------------------------------------\")\n",
        "\n",
        "\n",
        "    for idx, line in enumerate(lines):\n",
        "        # Name & Application No.\n",
        "        if \"Name:\" in line and \"Application No.\" in line:\n",
        "            name = re.search(r'Name:\\s*(.*?)\\s*Application No', line, re.IGNORECASE)\n",
        "            app_no = re.search(r'Application No\\.?:\\s*(\\S+)', line, re.IGNORECASE)\n",
        "            if name:\n",
        "                result[\"name\"] = name.group(1).strip()\n",
        "            if app_no:\n",
        "                result[\"application_no\"] = app_no.group(1).strip()\n",
        "\n",
        "        # fallback แยกบรรทัด\n",
        "        if \"Name:\" in line and idx + 1 < len(lines) and \"name\" not in result:\n",
        "            result[\"name\"] = lines[idx + 1].strip()\n",
        "        if \"Application No.\" in line and idx + 1 < len(lines) and \"application_no\" not in result:\n",
        "            app_no_fallback = re.search(r'\\d{3}', lines[idx + 1])\n",
        "            if app_no_fallback:\n",
        "                result[\"application_no\"] = app_no_fallback.group()\n",
        "        # Date\n",
        "        if \"Date of test Administration\" in line:\n",
        "            date_match = re.search(r'Date of test Administration:\\s*(.+)', line, re.IGNORECASE)\n",
        "            if date_match:\n",
        "                result[\"test_date\"] = date_match.group(1).strip()\n",
        "        elif \"June\" in line or \"May\" in line or \"July\" in line:\n",
        "            date_match = re.search(r\"(June|July|May)\\s+\\d{1,2},\\s+\\d{4}\", line)\n",
        "            if date_match and \"test_date\" not in result:\n",
        "                result[\"test_date\"] = date_match.group()\n",
        "\n",
        "        # Skills: บรรทัดเดียว\n",
        "        for skill in [\"Grammar\", \"Reading\", \"Speaking\", \"Writing\"]:\n",
        "            if skill.lower() + \"_score\" in result:\n",
        "                continue\n",
        "            m = re.search(rf\"{skill}.*?([A-Z][12])\\s*\\(?(\\d+)\\)?\", line)\n",
        "            if m and m.group(1) in valid_levels:\n",
        "                result[f\"{skill.lower()}_level\"] = m.group(1)\n",
        "                result[f\"{skill.lower()}_score\"] = m.group(2)\n",
        "\n",
        "        # Skills: fallback บรรทัดก่อน\n",
        "        for skill in [\"Grammar\", \"Reading\", \"Speaking\", \"Writing\"]:\n",
        "            if skill.lower() + \"_score\" not in result and skill.lower() in prev.lower():\n",
        "                m = re.search(r'([A-Z][12])\\s*\\(?(\\d+)\\)?', line)\n",
        "                if m and m.group(1) in valid_levels:\n",
        "                    result[f\"{skill.lower()}_level\"] = m.group(1)\n",
        "                    result[f\"{skill.lower()}_score\"] = m.group(2)\n",
        "\n",
        "        prev = line\n",
        "\n",
        "    return result\n",
        "\n",
        "# 3. MAIN SCRIPT FOR SINGLE IMAGE TEST\n",
        "if __name__ == \"__main__\":\n",
        "    # --- ⚙️ ตั้งค่า ---\n",
        "    # ❗️❗️ แก้ไข Path รูปภาพของคุณตรงนี้ ❗️❗️\n",
        "    single_image_path = \"/content/drive/MyDrive/kmitl_dataset/removebg_images/KMITL-TEP PILOT-1.png\"\n",
        "\n",
        "    # --- โหลดรูปภาพ ---\n",
        "    print(f\"กำลังโหลดรูปภาพจาก: {single_image_path}\")\n",
        "    img = cv2.imread(single_image_path)\n",
        "\n",
        "    if img is None:\n",
        "        print(f\"❌ เกิดข้อผิดพลาด: ไม่พบหรือไม่สามารถโหลดรูปภาพได้จาก Path ที่ระบุ\")\n",
        "    else:\n",
        "        # --- รัน Tesseract OCR ---\n",
        "        print(\"กำลังรัน Tesseract OCR...\")\n",
        "        custom_config = r'--oem 3 --psm 6'\n",
        "        text = pytesseract.image_to_string(img, config=custom_config)\n",
        "\n",
        "        # --- สกัดข้อมูลด้วยฟังก์ชันของคุณ ---\n",
        "        print(\"กำลังสกัดข้อมูล Key-Value...\")\n",
        "        extracted_data = extract_ktep_key_value(text)\n",
        "\n",
        "        # --- แสดงผลลัพธ์สุดท้าย ---\n",
        "        print(\"\\n--- ผลลัพธ์ที่สกัดได้ ---\")\n",
        "        # ใช้ json.dumps เพื่อพิมพ์ Dictionary ออกมาสวยงาม อ่านง่าย\n",
        "        print(json.dumps(extracted_data, indent=4, ensure_ascii=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoAefk8okiUK"
      },
      "source": [
        "**Easy OCR**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KHSF-OtCkliu"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import jiwer\n",
        "import numpy as np\n",
        "import easyocr\n",
        "\n",
        "# --- 1. กำหนดค่าและ Path ที่สำคัญ ---\n",
        "MODEL_NAME = 'EasyOCR'\n",
        "APPROACH_NAME = 'Pure OCR'\n",
        "MASTER_OUTPUT_PATH = \"/content/drive/MyDrive/kmitl_dataset/final-excel/final_comparison_report.xlsx\"\n",
        "INDIVIDUAL_REPORT_PATH = f\"/content/drive/MyDrive/kmitl_dataset/final-excel/{MODEL_NAME.lower()}_{APPROACH_NAME.replace(' ', '_').lower()}_report.xlsx\"\n",
        "\n",
        "IMAGE_DIR = \"/content/drive/MyDrive/kmitl_dataset/final-preprecessed\"\n",
        "GROUND_TRUTH_PATH = \"/content/drive/MyDrive/kmitl_dataset/dataset/คะแนน TEP (Pilot Study)_Total_IT.xlsx\"\n",
        "\n",
        "# --- 2. ฟังก์ชันช่วยเหลือ ---\n",
        "\n",
        "def parse_score(text):\n",
        "    if not text: return None, None\n",
        "    match = re.search(r'([A-Z][1-2])\\s*\\((\\d+)\\)', text, re.IGNORECASE)\n",
        "    if match: return match.group(1).upper(), match.group(2)\n",
        "    return None, None\n",
        "\n",
        "def get_id_from_filename(filename):\n",
        "    match = re.search(r'(\\d+)\\.(png|jpg|jpeg)$', filename.lower())\n",
        "    if match: return int(match.group(1))\n",
        "    return None\n",
        "\n",
        "def normalize_text(text):\n",
        "    if text is None: return \"\"\n",
        "    text = str(text).strip().lower()\n",
        "    if text.endswith('.0'): text = text[:-2]\n",
        "    return text\n",
        "\n",
        "def parse_pure_ocr_text(text):\n",
        "    \"\"\"\n",
        "    Parser ที่ทำงานกับข้อความยาวๆ ที่ได้จาก OCR (ใช้ได้ทั้ง Pytesseract และ EasyOCR)\n",
        "    \"\"\"\n",
        "    result = {}\n",
        "    clean_text = text.replace('’', \"'\").replace('–', '-').replace('!', '1').replace('l', '1').replace('I', '1')\n",
        "\n",
        "    # Name\n",
        "    name_match = re.search(r'Name\\s*:?\\s*(.*?)\\s*Application', clean_text, re.DOTALL | re.IGNORECASE)\n",
        "    if name_match: result[\"name\"] = name_match.group(1).strip()\n",
        "\n",
        "    # Application No.\n",
        "    app_no_match = re.search(r'Application\\s*No\\.?\\s*:*\\s*(\\S+)', clean_text, re.IGNORECASE)\n",
        "    if app_no_match: result[\"application_no\"] = app_no_match.group(1).strip()\n",
        "\n",
        "    # Date\n",
        "    date_match = re.search(r'Date of test Administration\\s*:?\\s*(.+)', clean_text, re.IGNORECASE)\n",
        "    if date_match:\n",
        "        result[\"test_date\"] = date_match.group(1).strip().split('\\n')[0]\n",
        "\n",
        "    # Skill Scores\n",
        "    score_pattern_full = r'([A-B][1-2]\\s*\\(\\d+\\))'\n",
        "    for skill in [\"Grammar\", \"Reading\", \"Speaking\", \"Writing\"]:\n",
        "        m = re.search(rf\"{skill}.*?({score_pattern_full})\", clean_text, re.IGNORECASE)\n",
        "        if m:\n",
        "            score_text = m.group(1)\n",
        "            level, score = parse_score(score_text)\n",
        "            if level:\n",
        "                result[f\"{skill.lower()}_level\"] = level\n",
        "                result[f\"{skill.lower()}_score\"] = score\n",
        "\n",
        "    # Total Score\n",
        "    all_scores_text = re.findall(score_pattern_full, clean_text, re.IGNORECASE)\n",
        "    used_scores_text = []\n",
        "    for skill in [\"grammar\", \"reading\", \"speaking\", \"writing\"]:\n",
        "        if f\"{skill}_level\" in result:\n",
        "             used_scores_text.append(f\"{result[f'{skill}_level']} ({result[f'{skill}_score']})\")\n",
        "    remaining_scores = [s for s in all_scores_text if s not in used_scores_text]\n",
        "    if remaining_scores:\n",
        "        total_level, total_score = parse_score(remaining_scores[0])\n",
        "        result[\"total_level\"] = total_level\n",
        "        result[\"total_score\"] = total_score\n",
        "\n",
        "    return result\n",
        "\n",
        "# --- 3. โหลดและตรวจสอบข้อมูลก่อนเริ่ม ---\n",
        "print(\"--- 🔍 ขั้นตอนตรวจสอบข้อมูล ---\")\n",
        "gt_df = pd.read_excel(GROUND_TRUTH_PATH)\n",
        "ground_truth_ids = set(gt_df['No.'].unique())\n",
        "image_files = sorted([f for f in os.listdir(IMAGE_DIR) if f.lower().endswith(('.png', '.jpg', '.jpeg'))], key=get_id_from_filename)\n",
        "print(f\"พบ ID ใน Ground Truth ทั้งหมด: {len(ground_truth_ids)} ID | พบไฟล์รูปภาพ: {len(image_files)} ไฟล์\")\n",
        "print(\"--- ✅ ตรวจสอบข้อมูลเสร็จสิ้น ---\\n\")\n",
        "\n",
        "# --- 4. เริ่มกระบวนการสกัดข้อมูล ---\n",
        "# ⬇️⬇️ โหลดโมเดล EasyOCR (ทำครั้งเดียว) ⬇️⬇️\n",
        "print(\"กำลังโหลดโมเดล EasyOCR...\")\n",
        "reader = easyocr.Reader(['en'])\n",
        "print(\"โหลดโมเดลสำเร็จ!\")\n",
        "\n",
        "results_list = []\n",
        "print(f\"--- 🚀 เริ่มการสกัดข้อมูลจากรูปภาพ {len(image_files)} ไฟล์ ด้วย {MODEL_NAME} ({APPROACH_NAME}) ---\")\n",
        "for filename in image_files:\n",
        "    image_id = get_id_from_filename(filename)\n",
        "    if image_id is None: continue\n",
        "    gt_row = gt_df[gt_df['No.'] == image_id]\n",
        "    if gt_row.empty: continue\n",
        "\n",
        "    print(f\"  - กำลังประมวลผล ID: {image_id} ({filename})\")\n",
        "    image_path = os.path.join(IMAGE_DIR, filename)\n",
        "\n",
        "    # ⬇️⬇️ ใช้ EasyOCR อ่านภาพทั้งใบ ⬇️⬇️\n",
        "    # paragraph=True ช่วยรวมข้อความที่อยู่ใกล้กัน ทำให้ผลลัพธ์สะอาดขึ้น\n",
        "    raw_text_list = reader.readtext(image_path, detail=0, paragraph=True)\n",
        "    text = \"\\n\".join(raw_text_list)\n",
        "\n",
        "    # เรียกใช้ Parser ตัวเดียวกันกับของ Pytesseract\n",
        "    extracted_raw = parse_pure_ocr_text(text)\n",
        "\n",
        "    # (ส่วนที่เหลือเหมือนเดิมทุกประการ)\n",
        "    # ...\n",
        "    # นำข้อมูลที่สกัดได้มาจัดลง Dictionary\n",
        "    results_list.append({\n",
        "        \"No.\": image_id,\n",
        "        \"Application No. (GT)\": gt_row.iloc[0][\"Application No.\"], \"Application No. (Pred)\": extracted_raw.get('application_no'),\n",
        "        \"Name (GT)\": gt_row.iloc[0][\"Name\"], \"Name (Pred)\": extracted_raw.get('name'),\n",
        "        \"Test Date (GT)\": gt_row.iloc[0][\"Test Date\"], \"Test Date (Pred)\": extracted_raw.get('test_date'),\n",
        "        \"Grammar_Level (GT)\": gt_row.iloc[0][\"Grammar_Level\"], \"Grammar_Level (Pred)\": extracted_raw.get('grammar_level'),\n",
        "        \"Grammar_Score (GT)\": gt_row.iloc[0][\"Grammar_Score\"], \"Grammar_Score (Pred)\": extracted_raw.get('grammar_score'),\n",
        "        \"Reading_Level (GT)\": gt_row.iloc[0][\"Reading_Level\"], \"Reading_Level (Pred)\": extracted_raw.get('reading_level'),\n",
        "        \"Reading_Score (GT)\": gt_row.iloc[0][\"Reading_Score\"], \"Reading_Score (Pred)\": extracted_raw.get('reading_score'),\n",
        "        \"Speaking_Level (GT)\": gt_row.iloc[0][\"Speaking_Level\"], \"Speaking_Level (Pred)\": extracted_raw.get('speaking_level'),\n",
        "        \"Speaking_Score (GT)\": gt_row.iloc[0][\"Speaking_Score\"], \"Speaking_Score (Pred)\": extracted_raw.get('speaking_score'),\n",
        "        \"Writing_Level (GT)\": gt_row.iloc[0][\"Writing_Level\"], \"Writing_Level (Pred)\": extracted_raw.get('writing_level'),\n",
        "        \"Writing_Score (GT)\": gt_row.iloc[0][\"Writing_Score\"], \"Writing_Score (Pred)\": extracted_raw.get('writing_score'),\n",
        "        \"Total_Level (GT)\": gt_row.iloc[0][\"Total_Level\"], \"Total_Level (Pred)\": extracted_raw.get('total_level'),\n",
        "        \"Total_Score (GT)\": gt_row.iloc[0][\"Total_Score\"], \"Total_Score (Pred)\": extracted_raw.get('total_score'),\n",
        "    })\n",
        "\n",
        "ocr_results_df = pd.DataFrame(results_list)\n",
        "print(\"--- ✅ สกัดข้อมูลเสร็จสิ้น ---\\n\")\n",
        "\n",
        "\n",
        "# --- 5. เริ่มกระบวนการประเมินผล (Evaluation) ---\n",
        "# ... (โค้ดส่วนนี้เหมือนกับเวอร์ชันก่อนหน้าทุกประการ) ...\n",
        "print(\"--- 📊 เริ่มการประเมินผล ---\")\n",
        "eval_df = ocr_results_df.fillna('')\n",
        "fields_to_evaluate = {\n",
        "    'Name': ('Name (GT)', 'Name (Pred)'), 'Application No.': ('Application No. (GT)', 'Application No. (Pred)'),\n",
        "    'Test Date': ('Test Date (GT)', 'Test Date (Pred)'), 'Grammar_Level': ('Grammar_Level (GT)', 'Grammar_Level (Pred)'),\n",
        "    'Grammar_Score': ('Grammar_Score (GT)', 'Grammar_Score (Pred)'), 'Reading_Level': ('Reading_Level (GT)', 'Reading_Level (Pred)'),\n",
        "    'Reading_Score': ('Reading_Score (GT)', 'Reading_Score (Pred)'), 'Speaking_Level': ('Speaking_Level (GT)', 'Speaking_Level (Pred)'),\n",
        "    'Speaking_Score': ('Speaking_Score (GT)', 'Speaking_Score (Pred)'), 'Writing_Level': ('Writing_Level (GT)', 'Writing_Level (Pred)'),\n",
        "    'Writing_Score': ('Writing_Score (GT)', 'Writing_Score (Pred)'), 'Total_Level': ('Total_Level (GT)', 'Total_Level (Pred)'),\n",
        "    'Total_Score': ('Total_Score (GT)', 'Total_Score (Pred)'),\n",
        "}\n",
        "evaluation_summary_list = []\n",
        "for field, (gt_col, pred_col) in fields_to_evaluate.items():\n",
        "    ground_truth = [normalize_text(t) for t in eval_df[gt_col]]\n",
        "    prediction = [normalize_text(t) for t in eval_df[pred_col]]\n",
        "    accuracy = np.mean([1 if gt == pred else 0 for gt, pred in zip(ground_truth, prediction)]) * 100\n",
        "    error_metrics = jiwer.compute_measures(ground_truth, prediction)\n",
        "    wer = error_metrics.get('wer', 0) * 100\n",
        "    cer = error_metrics.get('cer', 0) * 100\n",
        "    H = error_metrics.get('hits', 0); I = error_metrics.get('insertions', 0); D = error_metrics.get('deletions', 0); S = error_metrics.get('substitutions', 0)\n",
        "    precision = H / (H + I + S) if (H + I + S) > 0 else 0\n",
        "    recall = H / (H + D + S) if (H + D + S) > 0 else 0\n",
        "    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    evaluation_summary_list.append({\n",
        "        'Field': field, 'Accuracy (%)': round(accuracy, 2), 'WER (%)': round(wer, 2),\n",
        "        'CER (%)': round(cer, 2), 'F1-score (%)': round(f1_score * 100, 2)\n",
        "    })\n",
        "eval_summary_df = pd.DataFrame(evaluation_summary_list)\n",
        "eval_summary_df.insert(0, 'Approach', APPROACH_NAME)\n",
        "eval_summary_df.insert(0, 'Model', MODEL_NAME)\n",
        "print(eval_summary_df.to_string(index=False))\n",
        "print(\"--- ✅ ประเมินผลเสร็จสิ้น ---\\n\")\n",
        "\n",
        "\n",
        "# --- 6. บันทึกผลลัพธ์ (ทำ 2 อย่าง) ---\n",
        "# --- 6.A: บันทึกผลการทดลองนี้แยกไฟล์ ---\n",
        "print(f\"--- 💾 กำลังบันทึกรายงานเฉพาะของ {MODEL_NAME} ({APPROACH_NAME}) ลงในไฟล์: {INDIVIDUAL_REPORT_PATH} ---\")\n",
        "with pd.ExcelWriter(INDIVIDUAL_REPORT_PATH, engine='openpyxl') as writer:\n",
        "    ocr_results_df.to_excel(writer, sheet_name='Detailed_Results', index=False)\n",
        "    eval_summary_df.drop(columns=['Model', 'Approach']).to_excel(writer, sheet_name='Evaluation_Summary', index=False)\n",
        "print(f\"🎉 บันทึกไฟล์ {INDIVIDUAL_REPORT_PATH} สำเร็จ!\")\n",
        "\n",
        "# --- 6.B: อัปเดตข้อมูลลง Master File ---\n",
        "print(f\"--- 💾 กำลังอัปเดตไฟล์ Master Report: {MASTER_OUTPUT_PATH} ---\")\n",
        "SHEET_NAME = 'Master_Evaluation'\n",
        "try:\n",
        "    with pd.ExcelFile(MASTER_OUTPUT_PATH) as xls:\n",
        "        master_df = pd.read_excel(xls, sheet_name=SHEET_NAME)\n",
        "        master_df = master_df[(master_df['Model'] != MODEL_NAME) | (master_df['Approach'] != APPROACH_NAME)]\n",
        "    combined_df = pd.concat([master_df, eval_summary_df], ignore_index=True)\n",
        "except FileNotFoundError:\n",
        "    print(f\"ไม่พบไฟล์ Master เดิม, กำลังสร้างไฟล์ใหม่...\")\n",
        "    combined_df = eval_summary_df\n",
        "with pd.ExcelWriter(MASTER_OUTPUT_PATH, engine='openpyxl') as writer:\n",
        "    combined_df.to_excel(writer, sheet_name=SHEET_NAME, index=False)\n",
        "print(f\"🎉 อัปเดตไฟล์ Master Report สำเร็จ!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wc4xvsKakl_p"
      },
      "source": [
        "**TRocr**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc7pYdeFf9pj"
      },
      "source": [
        " TrOCR ถูกออกแบบมาให้อ่านข้อความทีละ \"บรรทัด\" ได้ดีเยี่ยม แต่ไม่เก่งในการวิเคราะห์โครงสร้างของเอกสารทั้งหน้าในคราวเดียว หากเราส่งภาพทั้งใบให้ TrOCR อ่านโดยตรง ผลลัพธ์ที่ได้มักจะเป็นข้อความยาวๆ ที่ไม่มีการจัดเรียง ทำให้แยกแยะข้อมูลได้ยากมาก\n",
        "\n",
        "ดังนั้น เพื่อดึงประสิทธิภาพสูงสุดของ TrOCR ออกมา เราจะใช้ แนวทางไฮบริด (Hybrid Approach) ที่ชาญฉลาดกว่า ดังนี้:\n",
        "\n",
        "1. ใช้ Pytesseract หาตำแหน่งบรรทัด (Layout Detection): เราจะใช้ pytesseract.image_to_data เพื่อสแกนหา \"ตำแหน่ง\" ของข้อความแต่ละบรรทัดบนหน้าโดยอัตโนมัติ (ขั้นตอนนี้เหมือนการทำ Bounding Box อัตโนมัติ)\n",
        "2. ใช้ TrOCR อ่านทีละบรรทัด (Line-by-Line OCR): เมื่อเรารู้ตำแหน่งของแต่ละบรรทัดแล้ว เราจะตัดภาพเฉพาะบรรทัดนั้นๆ แล้วส่งให้ TrOCR อ่าน ซึ่งเป็นงานที่ TrOCR ถนัดที่สุด จะได้ผลลัพธ์ที่แม่นยำมาก\n",
        "3. ใช้ Parser สกัดข้อมูล: นำข้อความทุกบรรทัดที่ TrOCR อ่านได้มารวมกัน แล้วใช้ Parser ที่เราพัฒนาไว้ (แบบเดียวกับ Pytesseract Pure OCR) เพื่อสกัดข้อมูล Key-Value ออกมา"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "8aeJD-FbkppT"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import jiwer\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
        "import pytesseract # ⬅️ เพิ่มเข้ามาเพื่อใช้หา Layout\n",
        "\n",
        "# --- 1. กำหนดค่าและ Path ที่สำคัญ ---\n",
        "MODEL_NAME = 'TrOCR'\n",
        "APPROACH_NAME = 'Pure OCR (Hybrid)' # ⬅️ ระบุว่าเป็นแนวทางไฮบริด\n",
        "INDIVIDUAL_REPORT_PATH = f\"/content/drive/MyDrive/kmitl_dataset/final-excel/{MODEL_NAME.lower()}_{APPROACH_NAME.replace(' ', '_').lower()}_report.xlsx\"\n",
        "MASTER_OUTPUT_PATH = \"/content/drive/MyDrive/kmitl_dataset/final-excel/final_comparison_report.xlsx\"\n",
        "\n",
        "IMAGE_DIR = \"/content/drive/MyDrive/kmitl_dataset/final-preprecessed\"\n",
        "GROUND_TRUTH_PATH = \"/content/drive/MyDrive/kmitl_dataset/dataset/คะแนน TEP (Pilot Study)_Total_IT.xlsx\"\n",
        "\n",
        "# --- 2. ฟังก์ชันช่วยเหลือ (เหมือนเดิม) ---\n",
        "def parse_score(text):\n",
        "    if not text: return None, None\n",
        "    processed_text = text.replace('l', '1').replace('I', '1')\n",
        "    match = re.search(r'([A-Z][1-2])\\s*\\((\\d+)\\)', processed_text, re.IGNORECASE)\n",
        "    if match: return match.group(1).upper(), match.group(2)\n",
        "    return None, None\n",
        "\n",
        "def get_id_from_filename(filename):\n",
        "    match = re.search(r'(\\d+)\\.(png|jpg|jpeg)$', filename.lower())\n",
        "    if match: return int(match.group(1))\n",
        "    return None\n",
        "\n",
        "def normalize_text(text):\n",
        "    if text is None: return \"\"\n",
        "    text = str(text).strip().lower()\n",
        "    if text.endswith('.0'): text = text[:-2]\n",
        "    return text\n",
        "\n",
        "def parse_pytesseract_pure_ocr(text): # ⬅️ เราสามารถใช้ Parser เดิมของ Pytesseract ได้เลย\n",
        "    result = {}\n",
        "    clean_text = text.replace('’', \"'\").replace('–', '-').replace('!', '1').replace('l', '1').replace('I', '1')\n",
        "    name_match = re.search(r'Name\\s*:?\\s*(.*?)\\s*Application', clean_text, re.DOTALL | re.IGNORECASE)\n",
        "    if name_match: result[\"name\"] = name_match.group(1).strip()\n",
        "    app_no_match = re.search(r'Application\\s*No\\.?\\s*:*\\s*(\\S+)', clean_text, re.IGNORECASE)\n",
        "    if app_no_match: result[\"application_no\"] = app_no_match.group(1).strip()\n",
        "    date_match = re.search(r'Date of test Administration\\s*:?\\s*(.+)', clean_text, re.IGNORECASE)\n",
        "    if date_match: result[\"test_date\"] = date_match.group(1).strip().split('\\n')[0]\n",
        "    score_pattern_full = r'([A-B][1-2]\\s*\\(\\d+\\))'\n",
        "    for skill in [\"Grammar\", \"Reading\", \"Speaking\", \"Writing\"]:\n",
        "        m = re.search(rf\"{skill}.*?({score_pattern_full})\", clean_text, re.IGNORECASE)\n",
        "        if m:\n",
        "            level, score = parse_score(m.group(1))\n",
        "            if level:\n",
        "                result[f\"{skill.lower()}_level\"] = level\n",
        "                result[f\"{skill.lower()}_score\"] = score\n",
        "    all_scores_text = re.findall(score_pattern_full, clean_text, re.IGNORECASE)\n",
        "    used_scores_text = []\n",
        "    for skill in [\"grammar\", \"reading\", \"speaking\", \"writing\"]:\n",
        "        if f\"{skill}_level\" in result:\n",
        "             used_scores_text.append(f\"{result[f'{skill}_level']} ({result[f'{skill}_score']})\")\n",
        "    remaining_scores = [s for s in all_scores_text if s not in used_scores_text]\n",
        "    if remaining_scores:\n",
        "        total_level, total_score = parse_score(remaining_scores[0])\n",
        "        result[\"total_level\"] = total_level\n",
        "        result[\"total_score\"] = total_score\n",
        "    return result\n",
        "\n",
        "# --- 3. โหลดและตรวจสอบข้อมูลก่อนเริ่ม ---\n",
        "print(\"--- 🔍 ขั้นตอนตรวจสอบข้อมูล ---\")\n",
        "gt_df = pd.read_excel(GROUND_TRUTH_PATH)\n",
        "ground_truth_ids = set(gt_df['No.'].unique())\n",
        "image_files = sorted([f for f in os.listdir(IMAGE_DIR) if f.lower().endswith(('.png', '.jpg', '.jpeg'))], key=get_id_from_filename)\n",
        "print(f\"พบ ID ใน Ground Truth ทั้งหมด: {len(ground_truth_ids)} ID | พบไฟล์รูปภาพ: {len(image_files)} ไฟล์\")\n",
        "print(\"--- ✅ ตรวจสอบข้อมูลเสร็จสิ้น ---\\n\")\n",
        "\n",
        "# --- 4. เริ่มกระบวนการสกัดข้อมูล (TrOCR Hybrid) ---\n",
        "# ⬇️ โหลดโมเดล TrOCR (ทำครั้งเดียว)\n",
        "print(\"--- 🤖 กำลังโหลดโมเดล TrOCR ---\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"จะใช้ Device: {device}\")\n",
        "processor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-printed')\n",
        "model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed').to(device)\n",
        "print(\"--- ✅ โหลดโมเดลสำเร็จ! ---\\n\")\n",
        "\n",
        "results_list = []\n",
        "print(f\"--- 🚀 เริ่มการสกัดข้อมูลจากรูปภาพ {len(image_files)} ไฟล์ ด้วย {MODEL_NAME} ({APPROACH_NAME}) ---\")\n",
        "for filename in image_files:\n",
        "    image_id = get_id_from_filename(filename)\n",
        "    if image_id is None: continue\n",
        "    gt_row = gt_df[gt_df['No.'] == image_id]\n",
        "    if gt_row.empty: continue\n",
        "\n",
        "    print(f\"  - กำลังประมวลผล ID: {image_id} ({filename})\")\n",
        "    image_path = os.path.join(IMAGE_DIR, filename)\n",
        "    img = cv2.imread(image_path)\n",
        "\n",
        "    # ⬇️⬇️⬇️ ส่วนของ Hybrid Approach ⬇️⬇️⬇️\n",
        "    # 1. ใช้ Pytesseract หาตำแหน่งของทุกบรรทัด (Layout Detection)\n",
        "    line_data = pytesseract.image_to_data(img, output_type=pytesseract.Output.DATAFRAME)\n",
        "    line_data = line_data[line_data.conf > 30].dropna(subset=['text'])\n",
        "\n",
        "    recognized_lines = []\n",
        "    # 2. อ่านข้อความทีละบรรทัดด้วย TrOCR\n",
        "    for line_num in line_data['line_num'].unique():\n",
        "        line_df = line_data[line_data['line_num'] == line_num]\n",
        "        if not line_df.empty:\n",
        "            # หา Bounding Box ของทั้งบรรทัด\n",
        "            x, y, w, h = (line_df.left.min(), line_df.top.min(),\n",
        "                          line_df.left.max() + line_df.width.max() - line_df.left.min(),\n",
        "                          line_df.top.max() + line_df.height.max() - line_df.top.min())\n",
        "\n",
        "            # ตัดภาพเฉพาะบรรทัด\n",
        "            line_image = img[y:y+h, x:x+w]\n",
        "\n",
        "            # ส่งภาพบรรทัดให้ TrOCR อ่าน\n",
        "            if line_image.size > 0:\n",
        "                line_rgb = cv2.cvtColor(line_image, cv2.COLOR_BGR2RGB)\n",
        "                pixel_values = processor(images=line_rgb, return_tensors=\"pt\").pixel_values.to(device)\n",
        "                generated_ids = model.generate(pixel_values)\n",
        "                generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "                recognized_lines.append(generated_text)\n",
        "\n",
        "    # 3. รวมข้อความที่ได้จาก TrOCR แล้วส่งให้ Parser\n",
        "    full_text = \"\\n\".join(recognized_lines)\n",
        "    extracted_raw = parse_pytesseract_pure_ocr(full_text)\n",
        "\n",
        "    # (ส่วนที่เหลือเหมือนเดิมทุกประการ)\n",
        "    # ...\n",
        "    # (ส่วน Parse และ Append to results_list เหมือนเดิม)\n",
        "    g_level, g_score = parse_score(extracted_raw.get('grammar'))\n",
        "    r_level, r_score = parse_score(extracted_raw.get('reading'))\n",
        "    s_level, s_score = parse_score(extracted_raw.get('speaking'))\n",
        "    w_level, w_score = parse_score(extracted_raw.get('writing'))\n",
        "    t_level, t_score = parse_score(extracted_raw.get('total'))\n",
        "    results_list.append({\n",
        "        \"No.\": image_id,\n",
        "        \"Application No. (GT)\": gt_row.iloc[0][\"Application No.\"], \"Application No. (Pred)\": extracted_raw.get('application_no'),\n",
        "        \"Name (GT)\": gt_row.iloc[0][\"Name\"], \"Name (Pred)\": extracted_raw.get('name'),\n",
        "        \"Test Date (GT)\": gt_row.iloc[0][\"Test Date\"], \"Test Date (Pred)\": extracted_raw.get('test_date'),\n",
        "        \"Grammar_Level (GT)\": gt_row.iloc[0][\"Grammar_Level\"], \"Grammar_Level (Pred)\": g_level,\n",
        "        \"Grammar_Score (GT)\": gt_row.iloc[0][\"Grammar_Score\"], \"Grammar_Score (Pred)\": g_score,\n",
        "        \"Reading_Level (GT)\": gt_row.iloc[0][\"Reading_Level\"], \"Reading_Level (Pred)\": r_level,\n",
        "        \"Reading_Score (GT)\": gt_row.iloc[0][\"Reading_Score\"], \"Reading_Score (Pred)\": r_score,\n",
        "        \"Speaking_Level (GT)\": gt_row.iloc[0][\"Speaking_Level\"], \"Speaking_Level (Pred)\": s_level,\n",
        "        \"Speaking_Score (GT)\": gt_row.iloc[0][\"Speaking_Score\"], \"Speaking_Score (Pred)\": s_score,\n",
        "        \"Writing_Level (GT)\": gt_row.iloc[0][\"Writing_Level\"], \"Writing_Level (Pred)\": w_level,\n",
        "        \"Writing_Score (GT)\": gt_row.iloc[0][\"Writing_Score\"], \"Writing_Score (Pred)\": w_score,\n",
        "        \"Total_Level (GT)\": gt_row.iloc[0][\"Total_Level\"], \"Total_Level (Pred)\": t_level,\n",
        "        \"Total_Score (GT)\": gt_row.iloc[0][\"Total_Score\"], \"Total_Score (Pred)\": t_score,\n",
        "    })\n",
        "\n",
        "\n",
        "ocr_results_df = pd.DataFrame(results_list)\n",
        "print(\"--- ✅ สกัดข้อมูลเสร็จสิ้น ---\\n\")\n",
        "\n",
        "\n",
        "# --- 5. เริ่มกระบวนการประเมินผล (Evaluation) ---\n",
        "# ... (โค้ดส่วนนี้เหมือนกับเวอร์ชันก่อนหน้าทุกประการ) ...\n",
        "print(\"--- 📊 เริ่มการประเมินผล ---\")\n",
        "eval_df = ocr_results_df.fillna('')\n",
        "fields_to_evaluate = {\n",
        "    'Name': ('Name (GT)', 'Name (Pred)'), 'Application No.': ('Application No. (GT)', 'Application No. (Pred)'),\n",
        "    'Test Date': ('Test Date (GT)', 'Test Date (Pred)'), 'Grammar_Level': ('Grammar_Level (GT)', 'Grammar_Level (Pred)'),\n",
        "    'Grammar_Score': ('Grammar_Score (GT)', 'Grammar_Score (Pred)'), 'Reading_Level': ('Reading_Level (GT)', 'Reading_Level (Pred)'),\n",
        "    'Reading_Score': ('Reading_Score (GT)', 'Reading_Score (Pred)'), 'Speaking_Level': ('Speaking_Level (GT)', 'Speaking_Level (Pred)'),\n",
        "    'Speaking_Score': ('Speaking_Score (GT)', 'Speaking_Score (Pred)'), 'Writing_Level': ('Writing_Level (GT)', 'Writing_Level (Pred)'),\n",
        "    'Writing_Score': ('Writing_Score (GT)', 'Writing_Score (Pred)'), 'Total_Level': ('Total_Level (GT)', 'Total_Level (Pred)'),\n",
        "    'Total_Score': ('Total_Score (GT)', 'Total_Score (Pred)'),\n",
        "}\n",
        "evaluation_summary_list = []\n",
        "for field, (gt_col, pred_col) in fields_to_evaluate.items():\n",
        "    ground_truth = [normalize_text(t) for t in eval_df[gt_col]]\n",
        "    prediction = [normalize_text(t) for t in eval_df[pred_col]]\n",
        "    accuracy = np.mean([1 if gt == pred else 0 for gt, pred in zip(ground_truth, prediction)]) * 100\n",
        "    error_metrics = jiwer.compute_measures(ground_truth, prediction)\n",
        "    wer = error_metrics.get('wer', 0) * 100\n",
        "    cer = error_metrics.get('cer', 0) * 100\n",
        "    H = error_metrics.get('hits', 0); I = error_metrics.get('insertions', 0); D = error_metrics.get('deletions', 0); S = error_metrics.get('substitutions', 0)\n",
        "    precision = H / (H + I + S) if (H + I + S) > 0 else 0\n",
        "    recall = H / (H + D + S) if (H + D + S) > 0 else 0\n",
        "    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    evaluation_summary_list.append({\n",
        "        'Field': field, 'Accuracy (%)': round(accuracy, 2), 'WER (%)': round(wer, 2),\n",
        "        'CER (%)': round(cer, 2), 'F1-score (%)': round(f1_score * 100, 2)\n",
        "    })\n",
        "eval_summary_df = pd.DataFrame(evaluation_summary_list)\n",
        "eval_summary_df.insert(0, 'Approach', APPROACH_NAME)\n",
        "eval_summary_df.insert(0, 'Model', MODEL_NAME)\n",
        "print(eval_summary_df.to_string(index=False))\n",
        "print(\"--- ✅ ประเมินผลเสร็จสิ้น ---\\n\")\n",
        "\n",
        "\n",
        "# --- 6. บันทึกผลลัพธ์ (ทำ 2 อย่าง) ---\n",
        "# --- 6.A: บันทึกผลการทดลองนี้แยกไฟล์ ---\n",
        "print(f\"--- 💾 กำลังบันทึกรายงานเฉพาะของ {MODEL_NAME} ({APPROACH_NAME}) ลงในไฟล์: {INDIVIDUAL_REPORT_PATH} ---\")\n",
        "with pd.ExcelWriter(INDIVIDUAL_REPORT_PATH, engine='openpyxl') as writer:\n",
        "    ocr_results_df.to_excel(writer, sheet_name='Detailed_Results', index=False)\n",
        "    eval_summary_df.drop(columns=['Model', 'Approach']).to_excel(writer, sheet_name='Evaluation_Summary', index=False)\n",
        "print(f\"🎉 บันทึกไฟล์ {INDIVIDUAL_REPORT_PATH} สำเร็จ!\")\n",
        "\n",
        "# --- 6.B: อัปเดตข้อมูลลง Master File ---\n",
        "print(f\"--- 💾 กำลังอัปเดตไฟล์ Master Report: {MASTER_OUTPUT_PATH} ---\")\n",
        "SHEET_NAME = 'Master_Evaluation'\n",
        "try:\n",
        "    with pd.ExcelFile(MASTER_OUTPUT_PATH) as xls:\n",
        "        master_df = pd.read_excel(xls, sheet_name=SHEET_NAME)\n",
        "        master_df = master_df[(master_df['Model'] != MODEL_NAME) | (master_df['Approach'] != APPROACH_NAME)]\n",
        "    combined_df = pd.concat([master_df, eval_summary_df], ignore_index=True)\n",
        "except FileNotFoundError:\n",
        "    print(f\"ไม่พบไฟล์ Master เดิม, กำลังสร้างไฟล์ใหม่...\")\n",
        "    combined_df = eval_summary_df\n",
        "with pd.ExcelWriter(MASTER_OUTPUT_PATH, engine='openpyxl') as writer:\n",
        "    combined_df.to_excel(writer, sheet_name=SHEET_NAME, index=False)\n",
        "print(f\"🎉 อัปเดตไฟล์ Master Report สำเร็จ!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niL2R_vzDxgX"
      },
      "source": [
        "# **Pytessecrac Boxing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "RHNGGu6ARflT"
      },
      "outputs": [],
      "source": [
        "!pip install pytesseract\n",
        "import cv2\n",
        "import pytesseract\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "S7CnfpilLP30"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import pytesseract\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- 1. กำหนดค่าและโหลดรูปภาพ ---\n",
        "# สำหรับ Windows, หากจำเป็นต้องระบุ Path ของ Tesseract\n",
        "# pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
        "\n",
        "# ระบุ Path ของรูปภาพที่คุณต้องการทดสอบ\n",
        "IMAGE_PATH = \"/content/drive/MyDrive/kmitl_dataset/final-preprecessed/KMITL-TEP PILOT-10.png\"\n",
        "\n",
        "# กำหนดพิกัดของแต่ละส่วนที่สนใจ (Region of Interest)\n",
        "# รูปแบบคือ: (x_start, y_start, width, height)\n",
        "ROI_CONFIG = {\n",
        "    \"name\":          (200, 485, 600, 65),\n",
        "    \"application_no\": (1100, 485, 500, 60),\n",
        "    \"test_date\":     (1310, 545, 400, 60),\n",
        "    \"overall_score\": (820, 625, 800, 100),\n",
        "    \"grammar\":       (320, 785, 500, 80),\n",
        "    \"reading\":       (1050, 785, 500, 80),\n",
        "    \"speaking\":      (320, 900, 500, 80),\n",
        "    \"writing\":       (1050, 900, 500, 80),\n",
        "}\n",
        "\n",
        "# --- 2. โหลดและประมวลผลรูปภาพ ---\n",
        "image = cv2.imread(IMAGE_PATH)\n",
        "if image is None:\n",
        "    print(f\"ไม่สามารถโหลดรูปภาพได้จาก: {IMAGE_PATH}\")\n",
        "else:\n",
        "    # สร้าง Dictionary ว่างสำหรับเก็บข้อมูลที่สกัดได้\n",
        "    extracted_data = {}\n",
        "\n",
        "    # สร้างสำเนาของรูปภาพเพื่อวาดกรอบสี่เหลี่ยม (สำหรับแสดงผล)\n",
        "    image_with_boxes = image.copy()\n",
        "\n",
        "    # --- 3. วนลูปเพื่อสกัดข้อมูลตาม ROI_CONFIG ---\n",
        "    print(\"กำลังสกัดข้อมูลตามพิกัด ROI...\")\n",
        "    for field_name, (x, y, w, h) in ROI_CONFIG.items():\n",
        "\n",
        "        # 3.1 ตัดภาพ (Crop) ตามพิกัดที่กำหนด\n",
        "        roi_image = image[y:y+h, x:x+w]\n",
        "\n",
        "        # 3.2 ประมวลผลภาพชิ้นเล็กๆ (ROI) เพื่อเพิ่มความแม่นยำ\n",
        "        # แปลงเป็น Grayscale และทำ Binarization\n",
        "        gray_roi = cv2.cvtColor(roi_image, cv2.COLOR_BGR2GRAY)\n",
        "        # เพิ่มความละเอียดของภาพ ROI เพื่อให้ Tesseract อ่านได้ดีขึ้น\n",
        "        upscaled_roi = cv2.resize(gray_roi, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)\n",
        "        # ใช้ Thresholding เพื่อเปลี่ยนเป็นภาพขาว-ดำที่คมชัด\n",
        "        _, binary_roi = cv2.threshold(upscaled_roi, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "\n",
        "        # 3.3 ทำ OCR กับภาพ ROI ที่ประมวลผลแล้ว\n",
        "        # ใช้ Page Segmentation Mode (PSM) 7 ซึ่งเหมาะสำหรับข้อความบรรทัดเดียว\n",
        "        custom_config = r'--oem 3 --psm 7 -l eng'\n",
        "        text = pytesseract.image_to_string(binary_roi, config=custom_config)\n",
        "\n",
        "        # 3.4 ทำความสะอาดและเก็บผลลัพธ์\n",
        "        cleaned_text = text.strip()\n",
        "        extracted_data[field_name] = cleaned_text\n",
        "        print(f\"  - {field_name}: '{cleaned_text}'\")\n",
        "\n",
        "        # 3.5 วาดกรอบสี่เหลี่ยมบนภาพสำเนาเพื่อแสดงผล\n",
        "        cv2.rectangle(image_with_boxes, (x, y), (x + w, y + h), (0, 255, 0), 3)\n",
        "        cv2.putText(image_with_boxes, field_name, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (36, 255, 12), 3)\n",
        "\n",
        "\n",
        "    # --- 4. สรุปผลลัพธ์ ---\n",
        "    print(\"\\n--- สรุปข้อมูลที่สกัดได้ ---\")\n",
        "    print(extracted_data)\n",
        "\n",
        "    # --- 5. แสดงภาพพร้อมกรอบ ROI ---\n",
        "    # แปลง BGR เป็น RGB เพื่อให้ Matplotlib แสดงสีถูกต้อง\n",
        "    plt.figure(figsize=(20, 15))\n",
        "    plt.imshow(cv2.cvtColor(image_with_boxes, cv2.COLOR_BGR2RGB))\n",
        "    plt.title('Image with ROIs')\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nq2wtP3RFn4z"
      },
      "source": [
        "**Final auto pytes update to master file**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5S2ntXP-R4Gc"
      },
      "outputs": [],
      "source": [
        "!pip install jiwer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KdP_fdY3Axhj"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import jiwer\n",
        "import numpy as np\n",
        "import pytesseract\n",
        "\n",
        "# --- 1. กำหนดค่าและ Path ที่สำคัญ ---\n",
        "# ⬇️ เปลี่ยนแค่ 2 บรรทัดนี้\n",
        "MODEL_NAME = 'Pytesseract'\n",
        "INDIVIDUAL_REPORT_PATH = \"/content/drive/MyDrive/kmitl_dataset/final-excel/pytesseract_boxing_report.xlsx\"\n",
        "\n",
        "# Path สำหรับไฟล์ Master จะใช้ชื่อเดิมเพื่อรวบรวมผล\n",
        "APPROACH_NAME = 'Boxing'\n",
        "MASTER_OUTPUT_PATH = \"/content/drive/MyDrive/kmitl_dataset/final-excel/final_comparison_report.xlsx\"\n",
        "\n",
        "IMAGE_DIR = \"/content/drive/MyDrive/kmitl_dataset/final-preprecessed\"\n",
        "GROUND_TRUTH_PATH = \"/content/drive/MyDrive/kmitl_dataset/dataset/คะแนน TEP (Pilot Study)_Total_IT.xlsx\"\n",
        "\n",
        "ROI_CONFIG = {\n",
        "    \"name\":           (200, 485, 600, 65),\n",
        "    \"application_no\": (1100, 485, 500, 60),\n",
        "    \"test_date\":      (1310, 545, 400, 60),\n",
        "    \"total\":          (820, 625, 800, 100),\n",
        "    \"grammar\":        (320, 785, 500, 80),\n",
        "    \"reading\":        (1050, 785, 500, 80),\n",
        "    \"speaking\":       (320, 900, 500, 80),\n",
        "    \"writing\":        (1050, 900, 500, 80),\n",
        "}\n",
        "\n",
        "# --- 2. ฟังก์ชันช่วยเหลือ (ฉบับสมบูรณ์) ---\n",
        "def parse_score(text):\n",
        "    if not text: return None, None\n",
        "    processed_text = text.replace('l', '1').replace('I', '1')\n",
        "    match = re.search(r'([A-Z][1-2])\\s*\\((\\d+)\\)', processed_text, re.IGNORECASE)\n",
        "    if match:\n",
        "        return match.group(1).upper(), match.group(2)\n",
        "    return None, None\n",
        "\n",
        "def get_id_from_filename(filename):\n",
        "    match = re.search(r'(\\d+)\\.(png|jpg|jpeg)$', filename.lower())\n",
        "    if match: return int(match.group(1))\n",
        "    return None\n",
        "\n",
        "def normalize_text(text):\n",
        "    if text is None: return \"\"\n",
        "    text = str(text).strip().lower()\n",
        "    if text.endswith('.0'): text = text[:-2]\n",
        "    return text\n",
        "\n",
        "# --- 3. โหลดและตรวจสอบข้อมูลก่อนเริ่ม (เหมือนเดิม) ---\n",
        "print(\"--- 🔍 ขั้นตอนตรวจสอบข้อมูล ---\")\n",
        "gt_df = pd.read_excel(GROUND_TRUTH_PATH)\n",
        "ground_truth_ids = set(gt_df['No.'].unique())\n",
        "print(f\"พบ ID ใน Ground Truth ทั้งหมด: {len(ground_truth_ids)} ID\")\n",
        "allowed_extensions = ['.png', '.jpg', '.jpeg']\n",
        "all_files_in_dir = os.listdir(IMAGE_DIR)\n",
        "image_files = [f for f in all_files_in_dir if f.lower().endswith(tuple(allowed_extensions))]\n",
        "image_files.sort(key=get_id_from_filename)\n",
        "print(f\"พบไฟล์รูปภาพในโฟลเดอร์: {len(image_files)} ไฟล์\")\n",
        "print(\"--- ✅ ตรวจสอบข้อมูลเสร็จสิ้น ---\\n\")\n",
        "\n",
        "# --- 4. เริ่มกระบวนการสกัดข้อมูลด้วย Pytesseract ---\n",
        "results_list = []\n",
        "print(f\"--- 🚀 เริ่มการสกัดข้อมูลจากรูปภาพ {len(image_files)} ไฟล์ ด้วย {MODEL_NAME} ({APPROACH_NAME}) ---\")\n",
        "for filename in image_files:\n",
        "    image_id = get_id_from_filename(filename)\n",
        "    if image_id is None: continue\n",
        "    gt_row = gt_df[gt_df['No.'] == image_id]\n",
        "    if gt_row.empty: continue\n",
        "\n",
        "    print(f\"  - กำลังประมวลผล ID: {image_id} ({filename})\")\n",
        "    image_path = os.path.join(IMAGE_DIR, filename)\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    extracted_raw = {}\n",
        "    for field_name, (x, y, w, h) in ROI_CONFIG.items():\n",
        "        roi_image = image[y:y+h, x:x+w]\n",
        "\n",
        "        # ⬇️ Preprocessing และ OCR สำหรับ Pytesseract\n",
        "        gray_roi = cv2.cvtColor(roi_image, cv2.COLOR_BGR2GRAY)\n",
        "        upscaled_roi = cv2.resize(gray_roi, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)\n",
        "        _, binary_roi = cv2.threshold(upscaled_roi, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "\n",
        "        custom_config = r'--oem 3 --psm 7 -l eng'\n",
        "        text = pytesseract.image_to_string(binary_roi, config=custom_config).strip()\n",
        "\n",
        "        extracted_raw[field_name] = text\n",
        "\n",
        "    # (ส่วน Parse และ Append to results_list เหมือนเดิม)\n",
        "    g_level, g_score = parse_score(extracted_raw.get('grammar'))\n",
        "    r_level, r_score = parse_score(extracted_raw.get('reading'))\n",
        "    s_level, s_score = parse_score(extracted_raw.get('speaking'))\n",
        "    w_level, w_score = parse_score(extracted_raw.get('writing'))\n",
        "    t_level, t_score = parse_score(extracted_raw.get('total'))\n",
        "    results_list.append({\n",
        "        \"No.\": image_id,\n",
        "        \"Application No. (GT)\": gt_row.iloc[0][\"Application No.\"], \"Application No. (Pred)\": extracted_raw.get('application_no'),\n",
        "        \"Name (GT)\": gt_row.iloc[0][\"Name\"], \"Name (Pred)\": extracted_raw.get('name'),\n",
        "        \"Test Date (GT)\": gt_row.iloc[0][\"Test Date\"], \"Test Date (Pred)\": extracted_raw.get('test_date'),\n",
        "        \"Grammar_Level (GT)\": gt_row.iloc[0][\"Grammar_Level\"], \"Grammar_Level (Pred)\": g_level,\n",
        "        \"Grammar_Score (GT)\": gt_row.iloc[0][\"Grammar_Score\"], \"Grammar_Score (Pred)\": g_score,\n",
        "        \"Reading_Level (GT)\": gt_row.iloc[0][\"Reading_Level\"], \"Reading_Level (Pred)\": r_level,\n",
        "        \"Reading_Score (GT)\": gt_row.iloc[0][\"Reading_Score\"], \"Reading_Score (Pred)\": r_score,\n",
        "        \"Speaking_Level (GT)\": gt_row.iloc[0][\"Speaking_Level\"], \"Speaking_Level (Pred)\": s_level,\n",
        "        \"Speaking_Score (GT)\": gt_row.iloc[0][\"Speaking_Score\"], \"Speaking_Score (Pred)\": s_score,\n",
        "        \"Writing_Level (GT)\": gt_row.iloc[0][\"Writing_Level\"], \"Writing_Level (Pred)\": w_level,\n",
        "        \"Writing_Score (GT)\": gt_row.iloc[0][\"Writing_Score\"], \"Writing_Score (Pred)\": w_score,\n",
        "        \"Total_Level (GT)\": gt_row.iloc[0][\"Total_Level\"], \"Total_Level (Pred)\": t_level,\n",
        "        \"Total_Score (GT)\": gt_row.iloc[0][\"Total_Score\"], \"Total_Score (Pred)\": t_score,\n",
        "    })\n",
        "\n",
        "ocr_results_df = pd.DataFrame(results_list)\n",
        "print(\"--- ✅ สกัดข้อมูลเสร็จสิ้น ---\\n\")\n",
        "\n",
        "# --- 5. เริ่มกระบวนการประเมินผล (Evaluation) ---\n",
        "# (ส่วนนี้เหมือนเดิมทุกประการ)\n",
        "print(\"--- 📊 เริ่มการประเมินผล ---\")\n",
        "eval_df = ocr_results_df.fillna('')\n",
        "fields_to_evaluate = {\n",
        "    'Name': ('Name (GT)', 'Name (Pred)'), 'Application No.': ('Application No. (GT)', 'Application No. (Pred)'),\n",
        "    'Test Date': ('Test Date (GT)', 'Test Date (Pred)'), 'Grammar_Level': ('Grammar_Level (GT)', 'Grammar_Level (Pred)'),\n",
        "    'Grammar_Score': ('Grammar_Score (GT)', 'Grammar_Score (Pred)'), 'Reading_Level': ('Reading_Level (GT)', 'Reading_Level (Pred)'),\n",
        "    'Reading_Score': ('Reading_Score (GT)', 'Reading_Score (Pred)'), 'Speaking_Level': ('Speaking_Level (GT)', 'Speaking_Level (Pred)'),\n",
        "    'Speaking_Score': ('Speaking_Score (GT)', 'Speaking_Score (Pred)'), 'Writing_Level': ('Writing_Level (GT)', 'Writing_Level (Pred)'),\n",
        "    'Writing_Score': ('Writing_Score (GT)', 'Writing_Score (Pred)'), 'Total_Level': ('Total_Level (GT)', 'Total_Level (Pred)'),\n",
        "    'Total_Score': ('Total_Score (GT)', 'Total_Score (Pred)'),\n",
        "}\n",
        "evaluation_summary_list = []\n",
        "for field, (gt_col, pred_col) in fields_to_evaluate.items():\n",
        "    ground_truth = [normalize_text(t) for t in eval_df[gt_col]]\n",
        "    prediction = [normalize_text(t) for t in eval_df[pred_col]]\n",
        "    accuracy = np.mean([1 if gt == pred else 0 for gt, pred in zip(ground_truth, prediction)]) * 100\n",
        "    error_metrics = jiwer.compute_measures(ground_truth, prediction)\n",
        "    wer = error_metrics.get('wer', 0) * 100\n",
        "    cer = error_metrics.get('cer', 0) * 100\n",
        "    H = error_metrics.get('hits', 0); I = error_metrics.get('insertions', 0); D = error_metrics.get('deletions', 0); S = error_metrics.get('substitutions', 0)\n",
        "    precision = H / (H + I + S) if (H + I + S) > 0 else 0\n",
        "    recall = H / (H + D + S) if (H + D + S) > 0 else 0\n",
        "    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    evaluation_summary_list.append({\n",
        "        'Field': field, 'Accuracy (%)': round(accuracy, 2), 'WER (%)': round(wer, 2),\n",
        "        'CER (%)': round(cer, 2), 'F1-score (%)': round(f1_score * 100, 2)\n",
        "    })\n",
        "eval_summary_df = pd.DataFrame(evaluation_summary_list)\n",
        "eval_summary_df.insert(0, 'Approach', APPROACH_NAME)\n",
        "eval_summary_df.insert(0, 'Model', MODEL_NAME)\n",
        "print(eval_summary_df.to_string(index=False))\n",
        "print(\"--- ✅ ประเมินผลเสร็จสิ้น ---\\n\")\n",
        "\n",
        "\n",
        "# --- 6. บันทึกผลลัพธ์ (ทำ 2 อย่าง) ---\n",
        "# --- 6.A: บันทึกผลการทดลองนี้แยกไฟล์ (2 ชีต) ---\n",
        "print(f\"--- 💾 กำลังบันทึกรายงานเฉพาะของ {MODEL_NAME} ลงในไฟล์: {INDIVIDUAL_REPORT_PATH} ---\")\n",
        "with pd.ExcelWriter(INDIVIDUAL_REPORT_PATH, engine='openpyxl') as writer:\n",
        "    ocr_results_df.to_excel(writer, sheet_name='Detailed_Results', index=False)\n",
        "    eval_summary_df.drop(columns=['Model', 'Approach']).to_excel(writer, sheet_name='Evaluation_Summary', index=False)\n",
        "print(f\"🎉 บันทึกไฟล์ {INDIVIDUAL_REPORT_PATH} สำเร็จ!\")\n",
        "\n",
        "# --- 6.B: อัปเดตข้อมูลลง Master File สำหรับสร้างกราฟ ---\n",
        "print(f\"--- 💾 กำลังอัปเดตไฟล์ Master Report: {MASTER_OUTPUT_PATH} ---\")\n",
        "SHEET_NAME = 'Master_Evaluation'\n",
        "try:\n",
        "    with pd.ExcelFile(MASTER_OUTPUT_PATH) as xls:\n",
        "        master_df = pd.read_excel(xls, sheet_name=SHEET_NAME)\n",
        "        master_df = master_df[(master_df['Model'] != MODEL_NAME) | (master_df['Approach'] != APPROACH_NAME)]\n",
        "    combined_df = pd.concat([master_df, eval_summary_df], ignore_index=True)\n",
        "except FileNotFoundError:\n",
        "    print(f\"ไม่พบไฟล์ Master เดิม, กำลังสร้างไฟล์ใหม่...\")\n",
        "    combined_df = eval_summary_df\n",
        "with pd.ExcelWriter(MASTER_OUTPUT_PATH, engine='openpyxl') as writer:\n",
        "    combined_df.to_excel(writer, sheet_name=SHEET_NAME, index=False)\n",
        "\n",
        "print(f\"🎉 อัปเดตไฟล์ Master Report สำเร็จ!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAoPk0hwh5PN"
      },
      "source": [
        "# **Easy OCR Boxing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ahhYkSvSBeGm"
      },
      "outputs": [],
      "source": [
        "!pip install easyocr\n",
        "!pip install jiwer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "GkXXektfnj0P"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import jiwer\n",
        "import numpy as np\n",
        "import easyocr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zwsSLHQ7XD1"
      },
      "source": [
        "Easy read B1 => Bl (L-lower) or BI (I)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "gvfYxh7V0Cvb"
      },
      "outputs": [],
      "source": [
        "# --- โค้ดสำหรับดีบักและตรวจสอบ ROI ---\n",
        "import cv2\n",
        "import easyocr\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. กำหนด Path รูปภาพตัวอย่างที่คุณต้องการตรวจสอบ\n",
        "IMAGE_PATH_TO_DEBUG = \"/content/drive/MyDrive/kmitl_dataset/final-preprecessed/KMITL-TEP PILOT-1.png\" # ⬅️ ลองใช้รูปที่ 10 เป็นตัวอย่าง\n",
        "\n",
        "# 2. กำหนดพิกัด ROI ที่คุณใช้อยู่ปัจจุบัน\n",
        "ROI_CONFIG = {\n",
        "    \"name\":           (200, 485, 600, 65),\n",
        "    \"application_no\": (1100, 485, 500, 60),\n",
        "    \"test_date\":      (1310, 545, 400, 60),\n",
        "    \"total\":          (820, 625, 800, 100),\n",
        "    \"grammar\":        (320, 785, 500, 80),\n",
        "    \"reading\":        (1050, 785, 500, 80),\n",
        "    \"speaking\":       (320, 900, 500, 80),\n",
        "    \"writing\":        (1050, 900, 500, 80),\n",
        "}\n",
        "\n",
        "# 3. โหลดโมเดลและรูปภาพ\n",
        "reader = easyocr.Reader(['en'])\n",
        "image = cv2.imread(IMAGE_PATH_TO_DEBUG)\n",
        "image_with_boxes = image.copy()\n",
        "\n",
        "print(\"--- 🕵️‍♂️ ผลการตรวจสอบ ROI แต่ละช่อง ---\")\n",
        "\n",
        "# 4. วนลูปเพื่อวาดกล่องและสกัดข้อมูลทีละช่อง\n",
        "for field_name, (x, y, w, h) in ROI_CONFIG.items():\n",
        "    # ตัดภาพตามพิกัด\n",
        "    roi_image = image[y:y+h, x:x+w]\n",
        "\n",
        "    # ลองอ่านข้อความจากช่องนี้ด้วย EasyOCR\n",
        "    ocr_result = reader.readtext(roi_image, detail=0, paragraph=True)\n",
        "    text = \" \".join(ocr_result).strip()\n",
        "\n",
        "    # พิมพ์ผลลัพธ์ที่ได้จากกล่องนี้\n",
        "    print(f\"  - กล่อง '{field_name}': อ่านได้ -> '{text}'\")\n",
        "\n",
        "    # วาดกล่องสีเขียวลงบนภาพเพื่อดูตำแหน่ง\n",
        "    cv2.rectangle(image_with_boxes, (x, y), (x + w, y + h), (0, 255, 0), 3)\n",
        "    cv2.putText(image_with_boxes, field_name, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (36, 255, 12), 3)\n",
        "\n",
        "# 5. แสดงภาพพร้อมกล่องทั้งหมด\n",
        "plt.figure(figsize=(20, 15))\n",
        "plt.imshow(cv2.cvtColor(image_with_boxes, cv2.COLOR_BGR2RGB))\n",
        "plt.title('ตรวจสอบตำแหน่ง Bounding Boxes')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "NwZhlpZph4jm"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import jiwer\n",
        "import numpy as np\n",
        "import easyocr\n",
        "\n",
        "# --- 1. กำหนดค่าและ Path ที่สำคัญ ---\n",
        "IMAGE_DIR = \"/content/drive/MyDrive/kmitl_dataset/final-preprecessed\"\n",
        "GROUND_TRUTH_PATH = \"/content/drive/MyDrive/kmitl_dataset/dataset/คะแนน TEP (Pilot Study)_Total_IT.xlsx\"\n",
        "OUTPUT_EXCEL_PATH = \"/content/drive/MyDrive/kmitl_dataset/final-excel/easyocr_full_report_with_f1.xlsx\"\n",
        "\n",
        "ROI_CONFIG = {\n",
        "    \"name\":           (200, 485, 600, 65),\n",
        "    \"application_no\": (1100, 485, 500, 60),\n",
        "    \"test_date\":      (1310, 545, 400, 60),\n",
        "    \"total\":          (820, 625, 800, 100),\n",
        "    \"grammar\":        (320, 785, 500, 80),\n",
        "    \"reading\":        (1050, 785, 500, 80),\n",
        "    \"speaking\":       (320, 900, 500, 80),\n",
        "    \"writing\":        (1050, 900, 500, 80),\n",
        "}\n",
        "\n",
        "# --- 2. ฟังก์ชันช่วยเหลือ (เหมือนเดิม) ---\n",
        "def parse_score(text):\n",
        "    match = re.search(r'([A-Z][1-2])\\s*\\((\\d+)\\)', text)\n",
        "    if match:\n",
        "        return match.group(1), match.group(2)\n",
        "    return None, None\n",
        "\n",
        "def get_id_from_filename(filename):\n",
        "    match = re.search(r'(\\d+)\\.(png|jpg|jpeg)$', filename.lower())\n",
        "    if match:\n",
        "        return int(match.group(1))\n",
        "    return None\n",
        "\n",
        "# --- 3. โหลดและตรวจสอบข้อมูลก่อนเริ่ม (เหมือนเดิม) ---\n",
        "print(\"--- 🔍 ขั้นตอนตรวจสอบข้อมูล ---\")\n",
        "gt_df = pd.read_excel(GROUND_TRUTH_PATH)\n",
        "ground_truth_ids = set(gt_df['No.'].unique())\n",
        "print(f\"พบ ID ใน Ground Truth ทั้งหมด: {len(ground_truth_ids)} ID\")\n",
        "allowed_extensions = ['.png', '.jpg', '.jpeg']\n",
        "all_files_in_dir = os.listdir(IMAGE_DIR)\n",
        "image_files = [f for f in all_files_in_dir if f.lower().endswith(tuple(allowed_extensions))]\n",
        "image_files.sort(key=get_id_from_filename)\n",
        "# (ส่วนโค้ดตรวจสอบไฟล์ที่เหลือเหมือนเดิม)\n",
        "print(f\"พบไฟล์รูปภาพในโฟลเดอร์: {len(image_files)} ไฟล์\")\n",
        "print(\"--- ✅ ตรวจสอบข้อมูลเสร็จสิ้น ---\\n\")\n",
        "\n",
        "\n",
        "# --- 4. เริ่มกระบวนการสกัดข้อมูลด้วย EasyOCR (เหมือนเดิม) ---\n",
        "print(\"กำลังโหลดโมเดล EasyOCR...\")\n",
        "reader = easyocr.Reader(['en'])\n",
        "print(\"โหลดโมเดลสำเร็จ!\")\n",
        "\n",
        "results_list = []\n",
        "print(f\"--- 🚀 เริ่มการสกัดข้อมูลจากรูปภาพ {len(image_files)} ไฟล์ ด้วย EasyOCR ---\")\n",
        "for filename in image_files:\n",
        "    image_id = get_id_from_filename(filename)\n",
        "    if image_id is None: continue\n",
        "    gt_row = gt_df[gt_df['No.'] == image_id]\n",
        "    if gt_row.empty: continue\n",
        "\n",
        "    print(f\"  - กำลังประมวลผล ID: {image_id} ({filename})\")\n",
        "    image_path = os.path.join(IMAGE_DIR, filename)\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    extracted_raw = {}\n",
        "    for field_name, (x, y, w, h) in ROI_CONFIG.items():\n",
        "        roi_image = image[y:y+h, x:x+w]\n",
        "        gray_roi = cv2.cvtColor(roi_image, cv2.COLOR_BGR2GRAY)\n",
        "        ocr_result = reader.readtext(gray_roi, detail=0, paragraph=True)\n",
        "        text = \" \".join(ocr_result).strip()\n",
        "        extracted_raw[field_name] = text\n",
        "\n",
        "    g_level, g_score = parse_score(extracted_raw.get('grammar', ''))\n",
        "    r_level, r_score = parse_score(extracted_raw.get('reading', ''))\n",
        "    s_level, s_score = parse_score(extracted_raw.get('speaking', ''))\n",
        "    w_level, w_score = parse_score(extracted_raw.get('writing', ''))\n",
        "    t_level, t_score = parse_score(extracted_raw.get('total', ''))\n",
        "    results_list.append({\n",
        "        \"No.\": image_id,\n",
        "        \"Application No. (GT)\": gt_row.iloc[0][\"Application No.\"], \"Application No. (Pred)\": extracted_raw.get('application_no'),\n",
        "        \"Name (GT)\": gt_row.iloc[0][\"Name\"], \"Name (Pred)\": extracted_raw.get('name'),\n",
        "        \"Grammar_Level (GT)\": gt_row.iloc[0][\"Grammar_Level\"], \"Grammar_Level (Pred)\": g_level,\n",
        "        \"Grammar_Score (GT)\": gt_row.iloc[0][\"Grammar_Score\"], \"Grammar_Score (Pred)\": g_score,\n",
        "        \"Reading_Level (GT)\": gt_row.iloc[0][\"Reading_Level\"], \"Reading_Level (Pred)\": r_level,\n",
        "        \"Reading_Score (GT)\": gt_row.iloc[0][\"Reading_Score\"], \"Reading_Score (Pred)\": r_score,\n",
        "        \"Speaking_Level (GT)\": gt_row.iloc[0][\"Speaking_Level\"], \"Speaking_Level (Pred)\": s_level,\n",
        "        \"Speaking_Score (GT)\": gt_row.iloc[0][\"Speaking_Score\"], \"Speaking_Score (Pred)\": s_score,\n",
        "        \"Writing_Level (GT)\": gt_row.iloc[0][\"Writing_Level\"], \"Writing_Level (Pred)\": w_level,\n",
        "        \"Writing_Score (GT)\": gt_row.iloc[0][\"Writing_Score\"], \"Writing_Score (Pred)\": w_score,\n",
        "        \"Total_Level (GT)\": gt_row.iloc[0][\"Total_Level\"], \"Total_Level (Pred)\": t_level,\n",
        "        \"Total_Score (GT)\": gt_row.iloc[0][\"Total_Score\"], \"Total_Score (Pred)\": t_score,\n",
        "        \"Test Date (GT)\": gt_row.iloc[0][\"Test Date\"], \"Test Date (Pred)\": extracted_raw.get('test_date'),\n",
        "    })\n",
        "\n",
        "ocr_results_df = pd.DataFrame(results_list)\n",
        "print(\"--- ✅ สกัดข้อมูลเสร็จสิ้น ---\\n\")\n",
        "\n",
        "\n",
        "# --- 5. เริ่มกระบวนการประเมินผล (Evaluation) ---\n",
        "print(\"--- 📊 เริ่มการประเมินผล ---\")\n",
        "eval_df = ocr_results_df.fillna('')\n",
        "\n",
        "# ⬇️ 1. อัปเดต fields_to_evaluate ให้ครอบคลุมทุกฟิลด์\n",
        "fields_to_evaluate = {\n",
        "    'Name': ('Name (GT)', 'Name (Pred)'),\n",
        "    'Application No.': ('Application No. (GT)', 'Application No. (Pred)'),\n",
        "    'Test Date': ('Test Date (GT)', 'Test Date (Pred)'),\n",
        "    'Grammar_Level': ('Grammar_Level (GT)', 'Grammar_Level (Pred)'),\n",
        "    'Grammar_Score': ('Grammar_Score (GT)', 'Grammar_Score (Pred)'),\n",
        "    'Reading_Level': ('Reading_Level (GT)', 'Reading_Level (Pred)'),\n",
        "    'Reading_Score': ('Reading_Score (GT)', 'Reading_Score (Pred)'),\n",
        "    'Speaking_Level': ('Speaking_Level (GT)', 'Speaking_Level (Pred)'),\n",
        "    'Speaking_Score': ('Speaking_Score (GT)', 'Speaking_Score (Pred)'),\n",
        "    'Writing_Level': ('Writing_Level (GT)', 'Writing_Level (Pred)'),\n",
        "    'Writing_Score': ('Writing_Score (GT)', 'Writing_Score (Pred)'),\n",
        "    'Total_Level': ('Total_Level (GT)', 'Total_Level (Pred)'),\n",
        "    'Total_Score': ('Total_Score (GT)', 'Total_Score (Pred)'),\n",
        "}\n",
        "\n",
        "evaluation_summary_list = []\n",
        "for field, (gt_col, pred_col) in fields_to_evaluate.items():\n",
        "    ground_truth = eval_df[gt_col].astype(str).tolist()\n",
        "    prediction = eval_df[pred_col].astype(str).tolist()\n",
        "\n",
        "    accuracy = np.mean([1 if gt == pred else 0 for gt, pred in zip(ground_truth, prediction)]) * 100\n",
        "\n",
        "    # ⬇️ 2. คำนวณ Metrics ทั้งหมด รวมถึง F1-Score\n",
        "    error_metrics = jiwer.compute_measures(ground_truth, prediction)\n",
        "\n",
        "    # ดึงค่าต่างๆ ออกมาอย่างปลอดภัยด้วย .get() เพื่อป้องกัน KeyError\n",
        "    wer = error_metrics.get('wer', 0) * 100\n",
        "    cer = error_metrics.get('cer', 0) * 100\n",
        "\n",
        "    # คำนวณ Precision, Recall, และ F1-score\n",
        "    H = error_metrics.get('hits', 0)\n",
        "    I = error_metrics.get('insertions', 0)\n",
        "    D = error_metrics.get('deletions', 0)\n",
        "\n",
        "    precision = H / (H + I) if (H + I) > 0 else 0\n",
        "    recall = H / (H + D) if (H + D) > 0 else 0\n",
        "    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    evaluation_summary_list.append({\n",
        "        'Field': field,\n",
        "        'Accuracy (%)': round(accuracy, 2),\n",
        "        'WER (%)': round(wer, 2),\n",
        "        'CER (%)': round(cer, 2),\n",
        "        'F1-score (%)': round(f1_score * 100, 2) # ⬅️ 3. เพิ่ม F1-score ในผลลัพธ์\n",
        "    })\n",
        "\n",
        "eval_summary_df = pd.DataFrame(evaluation_summary_list)\n",
        "print(eval_summary_df.to_string(index=False))\n",
        "print(\"--- ✅ ประเมินผลเสร็จสิ้น ---\\n\")\n",
        "\n",
        "\n",
        "# --- 6. บันทึกผลลัพธ์ทั้งหมดลงไฟล์ Excel เดียว (2 ชีต) ---\n",
        "print(f\"--- 💾 กำลังบันทึกผลลัพธ์ลงในไฟล์: {OUTPUT_EXCEL_PATH} ---\")\n",
        "with pd.ExcelWriter(OUTPUT_EXCEL_PATH, engine='openpyxl') as writer:\n",
        "    ocr_results_df.to_excel(writer, sheet_name='Detailed_Results', index=False)\n",
        "    eval_summary_df.to_excel(writer, sheet_name='Evaluation_Summary', index=False)\n",
        "print(f\"🎉 บันทึกไฟล์สำเร็จ! ตรวจสอบผลลัพธ์ได้ที่ '{OUTPUT_EXCEL_PATH}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8FJSit88A_r"
      },
      "source": [
        "นี่คือจุดที่ผิดพลาด:\n",
        "\n",
        "1. EasyOCR อ่านผิด: โมเดล OCR มักจะสับสนระหว่างตัวเลข 1 กับตัวอักษร l (L พิมพ์เล็ก) หรือ I (i พิมพ์ใหญ่) ในกรณีนี้ มันอ่าน B1 เป็น Bl\n",
        "2. ฟังก์ชัน parse_score เข้มงวดเกินไป: โค้ดในฟังก์ชัน parse_score ของเราใช้\n",
        "Regular Expression r'([A-Z][1-2])' ซึ่งหมายความว่ามันมองหา \"ตัวอักษรพิมพ์ใหญ่ A-Z\" ตามด้วย \"ตัวเลข 1 หรือ 2\" เท่านั้น\n",
        "3. ผลลัพธ์: เมื่อ parse_score ได้รับข้อความ 'Bl (17)' เข้ามา มันไม่ตรงกับแพทเทิร์นที่กำหนด (เพราะ l ไม่ใช่ 1 หรือ 2) ฟังก์ชันจึงคืนค่า (None, None) ออกไป ทำให้คอลัมน์ (Pred) ใน Excel ของคุณว่างเปล่า และทำให้การประเมินผลรวนไปทั้งหมดครับ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "0wdnOwGd8ba5"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import jiwer\n",
        "import numpy as np\n",
        "import easyocr\n",
        "\n",
        "# --- 1. กำหนดค่าและ Path ที่สำคัญ ---\n",
        "IMAGE_DIR = \"/content/drive/MyDrive/kmitl_dataset/final-preprecessed\"\n",
        "GROUND_TRUTH_PATH = \"/content/drive/MyDrive/kmitl_dataset/dataset/คะแนน TEP (Pilot Study)_Total_IT.xlsx\"\n",
        "OUTPUT_EXCEL_PATH = \"/content/drive/MyDrive/kmitl_dataset/final-excel/easyocr_final_report_single_sheet.xlsx\"\n",
        "\n",
        "ROI_CONFIG = {\n",
        "    \"name\":           (200, 485, 600, 65),\n",
        "    \"application_no\": (1100, 485, 500, 60),\n",
        "    \"test_date\":      (1310, 545, 400, 60),\n",
        "    \"total\":          (820, 625, 800, 100),\n",
        "    \"grammar\":        (320, 785, 500, 80),\n",
        "    \"reading\":        (1050, 785, 500, 80),\n",
        "    \"speaking\":       (320, 900, 500, 80),\n",
        "    \"writing\":        (1050, 900, 500, 80),\n",
        "}\n",
        "\n",
        "# --- 2. ฟังก์ชันช่วยเหลือ (ฉบับแก้ไข) ---\n",
        "def parse_score(text):\n",
        "    if not text: return None, None\n",
        "    processed_text = text.replace('l', '1').replace('I', '1')\n",
        "    match = re.search(r'([A-Z][1-2])\\s*\\((\\d+)\\)', processed_text, re.IGNORECASE)\n",
        "    if match:\n",
        "        level = match.group(1).upper()\n",
        "        score = match.group(2)\n",
        "        return level, score\n",
        "    return None, None\n",
        "\n",
        "def get_id_from_filename(filename):\n",
        "    match = re.search(r'(\\d+)\\.(png|jpg|jpeg)$', filename.lower())\n",
        "    if match: return int(match.group(1))\n",
        "    return None\n",
        "\n",
        "# --- 3. โหลดและตรวจสอบข้อมูลก่อนเริ่ม (เหมือนเดิม) ---\n",
        "print(\"--- 🔍 ขั้นตอนตรวจสอบข้อมูล ---\")\n",
        "gt_df = pd.read_excel(GROUND_TRUTH_PATH)\n",
        "ground_truth_ids = set(gt_df['No.'].unique())\n",
        "print(f\"พบ ID ใน Ground Truth ทั้งหมด: {len(ground_truth_ids)} ID\")\n",
        "allowed_extensions = ['.png', '.jpg', '.jpeg']\n",
        "all_files_in_dir = os.listdir(IMAGE_DIR)\n",
        "image_files = [f for f in all_files_in_dir if f.lower().endswith(tuple(allowed_extensions))]\n",
        "image_files.sort(key=get_id_from_filename)\n",
        "print(f\"พบไฟล์รูปภาพในโฟลเดอร์: {len(image_files)} ไฟล์\")\n",
        "print(\"--- ✅ ตรวจสอบข้อมูลเสร็จสิ้น ---\\n\")\n",
        "\n",
        "# --- 4. เริ่มกระบวนการสกัดข้อมูลด้วย EasyOCR (เหมือนเดิม) ---\n",
        "print(\"กำลังโหลดโมเดล EasyOCR...\")\n",
        "reader = easyocr.Reader(['en'])\n",
        "print(\"โหลดโมเดลสำเร็จ!\")\n",
        "\n",
        "results_list = []\n",
        "print(f\"--- 🚀 เริ่มการสกัดข้อมูลจากรูปภาพ {len(image_files)} ไฟล์ ด้วย EasyOCR ---\")\n",
        "for filename in image_files:\n",
        "    image_id = get_id_from_filename(filename)\n",
        "    if image_id is None: continue\n",
        "    gt_row = gt_df[gt_df['No.'] == image_id]\n",
        "    if gt_row.empty: continue\n",
        "\n",
        "    print(f\"  - กำลังประมวลผล ID: {image_id} ({filename})\")\n",
        "    image_path = os.path.join(IMAGE_DIR, filename)\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    extracted_raw = {}\n",
        "    for field_name, (x, y, w, h) in ROI_CONFIG.items():\n",
        "        roi_image = image[y:y+h, x:x+w]\n",
        "        gray_roi = cv2.cvtColor(roi_image, cv2.COLOR_BGR2GRAY)\n",
        "        ocr_result = reader.readtext(gray_roi, detail=0, paragraph=True)\n",
        "        text = \" \".join(ocr_result).strip()\n",
        "        extracted_raw[field_name] = text\n",
        "\n",
        "    g_level, g_score = parse_score(extracted_raw.get('grammar'))\n",
        "    r_level, r_score = parse_score(extracted_raw.get('reading'))\n",
        "    s_level, s_score = parse_score(extracted_raw.get('speaking'))\n",
        "    w_level, w_score = parse_score(extracted_raw.get('writing'))\n",
        "    t_level, t_score = parse_score(extracted_raw.get('total'))\n",
        "\n",
        "    results_list.append({\n",
        "        \"No.\": image_id,\n",
        "        \"Application No. (GT)\": gt_row.iloc[0][\"Application No.\"], \"Application No. (Pred)\": extracted_raw.get('application_no'),\n",
        "        \"Name (GT)\": gt_row.iloc[0][\"Name\"], \"Name (Pred)\": extracted_raw.get('name'),\n",
        "        \"Grammar_Level (GT)\": gt_row.iloc[0][\"Grammar_Level\"], \"Grammar_Level (Pred)\": g_level,\n",
        "        \"Grammar_Score (GT)\": gt_row.iloc[0][\"Grammar_Score\"], \"Grammar_Score (Pred)\": g_score,\n",
        "        \"Reading_Level (GT)\": gt_row.iloc[0][\"Reading_Level\"], \"Reading_Level (Pred)\": r_level,\n",
        "        \"Reading_Score (GT)\": gt_row.iloc[0][\"Reading_Score\"], \"Reading_Score (Pred)\": r_score,\n",
        "        \"Speaking_Level (GT)\": gt_row.iloc[0][\"Speaking_Level\"], \"Speaking_Level (Pred)\": s_level,\n",
        "        \"Speaking_Score (GT)\": gt_row.iloc[0][\"Speaking_Score\"], \"Speaking_Score (Pred)\": s_score,\n",
        "        \"Writing_Level (GT)\": gt_row.iloc[0][\"Writing_Level\"], \"Writing_Level (Pred)\": w_level,\n",
        "        \"Writing_Score (GT)\": gt_row.iloc[0][\"Writing_Score\"], \"Writing_Score (Pred)\": w_score,\n",
        "        \"Total_Level (GT)\": gt_row.iloc[0][\"Total_Level\"], \"Total_Level (Pred)\": t_level,\n",
        "        \"Total_Score (GT)\": gt_row.iloc[0][\"Total_Score\"], \"Total_Score (Pred)\": t_score,\n",
        "        \"Test Date (GT)\": gt_row.iloc[0][\"Test Date\"], \"Test Date (Pred)\": extracted_raw.get('test_date'),\n",
        "    })\n",
        "\n",
        "ocr_results_df = pd.DataFrame(results_list)\n",
        "print(\"--- ✅ สกัดข้อมูลเสร็จสิ้น ---\\n\")\n",
        "\n",
        "\n",
        "# --- 5. เริ่มกระบวนการประเมินผล (Evaluation) - (นำ F1-Score กลับมา) ---\n",
        "print(\"--- 📊 เริ่มการประเมินผล ---\")\n",
        "\n",
        "def normalize_text(text):\n",
        "    if text is None: return \"\"\n",
        "    text = str(text).strip().lower()\n",
        "    if text.endswith('.0'): text = text[:-2]\n",
        "    return text\n",
        "\n",
        "eval_df = ocr_results_df.fillna('')\n",
        "\n",
        "fields_to_evaluate = {\n",
        "    'Name': ('Name (GT)', 'Name (Pred)'), 'Application No.': ('Application No. (GT)', 'Application No. (Pred)'),\n",
        "    'Test Date': ('Test Date (GT)', 'Test Date (Pred)'), 'Grammar_Level': ('Grammar_Level (GT)', 'Grammar_Level (Pred)'),\n",
        "    'Grammar_Score': ('Grammar_Score (GT)', 'Grammar_Score (Pred)'), 'Reading_Level': ('Reading_Level (GT)', 'Reading_Level (Pred)'),\n",
        "    'Reading_Score': ('Reading_Score (GT)', 'Reading_Score (Pred)'), 'Speaking_Level': ('Speaking_Level (GT)', 'Speaking_Level (Pred)'),\n",
        "    'Speaking_Score': ('Speaking_Score (GT)', 'Speaking_Score (Pred)'), 'Writing_Level': ('Writing_Level (GT)', 'Writing_Level (Pred)'),\n",
        "    'Writing_Score': ('Writing_Score (GT)', 'Writing_Score (Pred)'), 'Total_Level': ('Total_Level (GT)', 'Total_Level (Pred)'),\n",
        "    'Total_Score': ('Total_Score (GT)', 'Total_Score (Pred)'),\n",
        "}\n",
        "\n",
        "evaluation_summary_list = []\n",
        "for field, (gt_col, pred_col) in fields_to_evaluate.items():\n",
        "    ground_truth = [normalize_text(t) for t in eval_df[gt_col]]\n",
        "    prediction = [normalize_text(t) for t in eval_df[pred_col]]\n",
        "\n",
        "    accuracy = np.mean([1 if gt == pred else 0 for gt, pred in zip(ground_truth, prediction)]) * 100\n",
        "\n",
        "    # ใช้ compute_measures เพื่อคำนวณทุกอย่างในครั้งเดียว\n",
        "    error_metrics = jiwer.compute_measures(ground_truth, prediction)\n",
        "    wer = error_metrics.get('wer', 0) * 100\n",
        "    cer = error_metrics.get('cer', 0) * 100\n",
        "\n",
        "    # คำนวณ F1-score จากองค์ประกอบ\n",
        "    H = error_metrics.get('hits', 0)\n",
        "    I = error_metrics.get('insertions', 0)\n",
        "    D = error_metrics.get('deletions', 0)\n",
        "    S = error_metrics.get('substitutions', 0)\n",
        "\n",
        "    precision = H / (H + I + S) if (H + I + S) > 0 else 0\n",
        "    recall = H / (H + D + S) if (H + D + S) > 0 else 0\n",
        "    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    evaluation_summary_list.append({\n",
        "        'Field': field,\n",
        "        'Accuracy (%)': round(accuracy, 2),\n",
        "        'WER (%)': round(wer, 2),\n",
        "        'CER (%)': round(cer, 2),\n",
        "        'F1-score (%)': round(f1_score * 100, 2)\n",
        "    })\n",
        "\n",
        "eval_summary_df = pd.DataFrame(evaluation_summary_list)\n",
        "print(eval_summary_df.to_string(index=False))\n",
        "print(\"--- ✅ ประเมินผลเสร็จสิ้น ---\\n\")\n",
        "\n",
        "\n",
        "# ⬇️⬇️⬇️ --- 6. บันทึกผลลัพธ์ทั้งหมดลงไฟล์ Excel เดียว (ชีตเดียว) --- ⬇️⬇️⬇️\n",
        "print(f\"--- 💾 กำลังบันทึกผลลัพธ์ลงในไฟล์: {OUTPUT_EXCEL_PATH} ---\")\n",
        "SHEET_NAME = 'EasyOCR_Result'\n",
        "\n",
        "with pd.ExcelWriter(OUTPUT_EXCEL_PATH, engine='openpyxl') as writer:\n",
        "    # เขียนตารางผลลัพธ์แบบละเอียดก่อน\n",
        "    ocr_results_df.to_excel(writer, sheet_name=SHEET_NAME, index=False)\n",
        "\n",
        "    # คำนวณคอลัมน์เริ่มต้นสำหรับตารางสรุป (เว้น 2 คอลัมน์)\n",
        "    start_col_for_summary = ocr_results_df.shape[1] + 2\n",
        "\n",
        "    # เขียนตารางสรุปผลลงในชีตเดียวกัน\n",
        "    eval_summary_df.to_excel(writer, sheet_name=SHEET_NAME, index=False, startcol=start_col_for_summary)\n",
        "\n",
        "print(f\"🎉 บันทึกไฟล์สำเร็จ! ตรวจสอบผลลัพธ์ได้ที่ '{OUTPUT_EXCEL_PATH}' ในชีต '{SHEET_NAME}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aH8k_NW1Ay5S"
      },
      "source": [
        "**Final Update to master file**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "CAEhhLZtL0xs"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import jiwer\n",
        "import numpy as np\n",
        "import easyocr\n",
        "\n",
        "# --- 1. กำหนดค่าและ Path ที่สำคัญ ---\n",
        "MODEL_NAME = 'EasyOCR'\n",
        "APPROACH_NAME = 'Boxing'\n",
        "\n",
        "# ⬇️ Path สำหรับบันทึกไฟล์ผลลัพธ์ของโมเดลนี้โดยเฉพาะ (2 ชีต)\n",
        "INDIVIDUAL_REPORT_PATH = \"/content/drive/MyDrive/kmitl_dataset/final-excel/easyocr_boxing_report.xlsx\"\n",
        "# ⬇️ Path สำหรับไฟล์ Master ที่จะรวบรวมผลจากทุกโมเดล\n",
        "MASTER_OUTPUT_PATH = \"/content/drive/MyDrive/kmitl_dataset/final-excel/final_comparison_report.xlsx\"\n",
        "\n",
        "IMAGE_DIR = \"/content/drive/MyDrive/kmitl_dataset/final-preprecessed\"\n",
        "GROUND_TRUTH_PATH = \"/content/drive/MyDrive/kmitl_dataset/dataset/คะแนน TEP (Pilot Study)_Total_IT.xlsx\"\n",
        "\n",
        "ROI_CONFIG = {\n",
        "    \"name\":           (200, 485, 600, 65),\n",
        "    \"application_no\": (1100, 485, 500, 60),\n",
        "    \"test_date\":      (1310, 545, 400, 60),\n",
        "    \"total\":          (820, 625, 800, 100),\n",
        "    \"grammar\":        (320, 785, 500, 80),\n",
        "    \"reading\":        (1050, 785, 500, 80),\n",
        "    \"speaking\":       (320, 900, 500, 80),\n",
        "    \"writing\":        (1050, 900, 500, 80),\n",
        "}\n",
        "\n",
        "# --- 2. ฟังก์ชันช่วยเหลือ (ฉบับสมบูรณ์) ---\n",
        "def parse_score(text):\n",
        "    if not text: return None, None\n",
        "    processed_text = text.replace('l', '1').replace('I', '1')\n",
        "    match = re.search(r'([A-Z][1-2])\\s*\\((\\d+)\\)', processed_text, re.IGNORECASE)\n",
        "    if match: return match.group(1).upper(), match.group(2)\n",
        "    return None, None\n",
        "\n",
        "def get_id_from_filename(filename):\n",
        "    match = re.search(r'(\\d+)\\.(png|jpg|jpeg)$', filename.lower())\n",
        "    if match: return int(match.group(1))\n",
        "    return None\n",
        "\n",
        "def normalize_text(text):\n",
        "    if text is None: return \"\"\n",
        "    text = str(text).strip().lower()\n",
        "    if text.endswith('.0'): text = text[:-2]\n",
        "    return text\n",
        "\n",
        "# --- 3. & 4. โหลดและสกัดข้อมูล (เหมือนเดิม) ---\n",
        "# ... (โค้ดส่วนนี้ไม่มีการเปลี่ยนแปลง) ...\n",
        "print(\"--- 🔍 ขั้นตอนตรวจสอบข้อมูล ---\")\n",
        "gt_df = pd.read_excel(GROUND_TRUTH_PATH)\n",
        "ground_truth_ids = set(gt_df['No.'].unique())\n",
        "print(f\"พบ ID ใน Ground Truth ทั้งหมด: {len(ground_truth_ids)} ID\")\n",
        "allowed_extensions = ['.png', '.jpg', '.jpeg']\n",
        "all_files_in_dir = os.listdir(IMAGE_DIR)\n",
        "image_files = [f for f in all_files_in_dir if f.lower().endswith(tuple(allowed_extensions))]\n",
        "image_files.sort(key=get_id_from_filename)\n",
        "print(f\"พบไฟล์รูปภาพในโฟลเดอร์: {len(image_files)} ไฟล์\")\n",
        "print(\"--- ✅ ตรวจสอบข้อมูลเสร็จสิ้น ---\\n\")\n",
        "print(\"กำลังโหลดโมเดล EasyOCR...\")\n",
        "reader = easyocr.Reader(['en'])\n",
        "print(\"โหลดโมเดลสำเร็จ!\")\n",
        "results_list = []\n",
        "print(f\"--- 🚀 เริ่มการสกัดข้อมูลจากรูปภาพ {len(image_files)} ไฟล์ ด้วย {MODEL_NAME} ({APPROACH_NAME}) ---\")\n",
        "# (ส่วน for loop สกัดข้อมูลเหมือนเดิม)\n",
        "for filename in image_files:\n",
        "    image_id = get_id_from_filename(filename)\n",
        "    if image_id is None: continue\n",
        "    gt_row = gt_df[gt_df['No.'] == image_id]\n",
        "    if gt_row.empty: continue\n",
        "    print(f\"  - กำลังประมวลผล ID: {image_id} ({filename})\")\n",
        "    image_path = os.path.join(IMAGE_DIR, filename)\n",
        "    image = cv2.imread(image_path)\n",
        "    extracted_raw = {}\n",
        "    for field_name, (x, y, w, h) in ROI_CONFIG.items():\n",
        "        roi_image = image[y:y+h, x:x+w]\n",
        "        gray_roi = cv2.cvtColor(roi_image, cv2.COLOR_BGR2GRAY)\n",
        "        ocr_result = reader.readtext(gray_roi, detail=0, paragraph=True)\n",
        "        text = \" \".join(ocr_result).strip()\n",
        "        extracted_raw[field_name] = text\n",
        "    g_level, g_score = parse_score(extracted_raw.get('grammar'))\n",
        "    r_level, r_score = parse_score(extracted_raw.get('reading'))\n",
        "    s_level, s_score = parse_score(extracted_raw.get('speaking'))\n",
        "    w_level, w_score = parse_score(extracted_raw.get('writing'))\n",
        "    t_level, t_score = parse_score(extracted_raw.get('total'))\n",
        "    results_list.append({\n",
        "        \"No.\": image_id,\n",
        "        \"Application No. (GT)\": gt_row.iloc[0][\"Application No.\"], \"Application No. (Pred)\": extracted_raw.get('application_no'),\n",
        "        \"Name (GT)\": gt_row.iloc[0][\"Name\"], \"Name (Pred)\": extracted_raw.get('name'),\n",
        "        \"Test Date (GT)\": gt_row.iloc[0][\"Test Date\"], \"Test Date (Pred)\": extracted_raw.get('test_date'),\n",
        "        \"Grammar_Level (GT)\": gt_row.iloc[0][\"Grammar_Level\"], \"Grammar_Level (Pred)\": g_level,\n",
        "        \"Grammar_Score (GT)\": gt_row.iloc[0][\"Grammar_Score\"], \"Grammar_Score (Pred)\": g_score,\n",
        "        \"Reading_Level (GT)\": gt_row.iloc[0][\"Reading_Level\"], \"Reading_Level (Pred)\": r_level,\n",
        "        \"Reading_Score (GT)\": gt_row.iloc[0][\"Reading_Score\"], \"Reading_Score (Pred)\": r_score,\n",
        "        \"Speaking_Level (GT)\": gt_row.iloc[0][\"Speaking_Level\"], \"Speaking_Level (Pred)\": s_level,\n",
        "        \"Speaking_Score (GT)\": gt_row.iloc[0][\"Speaking_Score\"], \"Speaking_Score (Pred)\": s_score,\n",
        "        \"Writing_Level (GT)\": gt_row.iloc[0][\"Writing_Level\"], \"Writing_Level (Pred)\": w_level,\n",
        "        \"Writing_Score (GT)\": gt_row.iloc[0][\"Writing_Score\"], \"Writing_Score (Pred)\": w_score,\n",
        "        \"Total_Level (GT)\": gt_row.iloc[0][\"Total_Level\"], \"Total_Level (Pred)\": t_level,\n",
        "        \"Total_Score (GT)\": gt_row.iloc[0][\"Total_Score\"], \"Total_Score (Pred)\": t_score,\n",
        "    })\n",
        "ocr_results_df = pd.DataFrame(results_list)\n",
        "print(\"--- ✅ สกัดข้อมูลเสร็จสิ้น ---\\n\")\n",
        "\n",
        "\n",
        "# --- 5. เริ่มกระบวนการประเมินผล (Evaluation) ---\n",
        "print(\"--- 📊 เริ่มการประเมินผล ---\")\n",
        "eval_df = ocr_results_df.fillna('')\n",
        "fields_to_evaluate = {\n",
        "    'Name': ('Name (GT)', 'Name (Pred)'), 'Application No.': ('Application No. (GT)', 'Application No. (Pred)'),\n",
        "    'Test Date': ('Test Date (GT)', 'Test Date (Pred)'), 'Grammar_Level': ('Grammar_Level (GT)', 'Grammar_Level (Pred)'),\n",
        "    'Grammar_Score': ('Grammar_Score (GT)', 'Grammar_Score (Pred)'), 'Reading_Level': ('Reading_Level (GT)', 'Reading_Level (Pred)'),\n",
        "    'Reading_Score': ('Reading_Score (GT)', 'Reading_Score (Pred)'), 'Speaking_Level': ('Speaking_Level (GT)', 'Speaking_Level (Pred)'),\n",
        "    'Speaking_Score': ('Speaking_Score (GT)', 'Speaking_Score (Pred)'), 'Writing_Level': ('Writing_Level (GT)', 'Writing_Level (Pred)'),\n",
        "    'Writing_Score': ('Writing_Score (GT)', 'Writing_Score (Pred)'), 'Total_Level': ('Total_Level (GT)', 'Total_Level (Pred)'),\n",
        "    'Total_Score': ('Total_Score (GT)', 'Total_Score (Pred)'),\n",
        "}\n",
        "\n",
        "evaluation_summary_list = []\n",
        "for field, (gt_col, pred_col) in fields_to_evaluate.items():\n",
        "    ground_truth = [normalize_text(t) for t in eval_df[gt_col]]\n",
        "    prediction = [normalize_text(t) for t in eval_df[pred_col]]\n",
        "    accuracy = np.mean([1 if gt == pred else 0 for gt, pred in zip(ground_truth, prediction)]) * 100\n",
        "    error_metrics = jiwer.compute_measures(ground_truth, prediction)\n",
        "    wer = error_metrics.get('wer', 0) * 100\n",
        "    cer = error_metrics.get('cer', 0) * 100\n",
        "    H = error_metrics.get('hits', 0); I = error_metrics.get('insertions', 0); D = error_metrics.get('deletions', 0); S = error_metrics.get('substitutions', 0)\n",
        "    precision = H / (H + I + S) if (H + I + S) > 0 else 0\n",
        "    recall = H / (H + D + S) if (H + D + S) > 0 else 0\n",
        "    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    evaluation_summary_list.append({\n",
        "        'Field': field, 'Accuracy (%)': round(accuracy, 2), 'WER (%)': round(wer, 2),\n",
        "        'CER (%)': round(cer, 2), 'F1-score (%)': round(f1_score * 100, 2)\n",
        "    })\n",
        "\n",
        "eval_summary_df = pd.DataFrame(evaluation_summary_list)\n",
        "eval_summary_df.insert(0, 'Approach', APPROACH_NAME)\n",
        "eval_summary_df.insert(0, 'Model', MODEL_NAME)\n",
        "print(eval_summary_df.to_string(index=False))\n",
        "print(\"--- ✅ ประเมินผลเสร็จสิ้น ---\\n\")\n",
        "\n",
        "\n",
        "# ⬇️⬇️⬇️ --- 6. บันทึกผลลัพธ์ (ทำ 2 อย่าง) --- ⬇️⬇️⬇️\n",
        "\n",
        "# --- 6.A: บันทึกผลการทดลองนี้แยกไฟล์ (2 ชีต) ---\n",
        "print(f\"--- 💾 กำลังบันทึกรายงานเฉพาะของ {MODEL_NAME} ลงในไฟล์: {INDIVIDUAL_REPORT_PATH} ---\")\n",
        "with pd.ExcelWriter(INDIVIDUAL_REPORT_PATH, engine='openpyxl') as writer:\n",
        "    ocr_results_df.to_excel(writer, sheet_name='Detailed_Results', index=False)\n",
        "    # นำคอลัมน์ Model และ Approach ออกก่อนบันทึกชีตสรุปของไฟล์นี้\n",
        "    eval_summary_df.drop(columns=['Model', 'Approach']).to_excel(writer, sheet_name='Evaluation_Summary', index=False)\n",
        "print(f\"🎉 บันทึกไฟล์ {INDIVIDUAL_REPORT_PATH} สำเร็จ!\")\n",
        "\n",
        "# --- 6.B: อัปเดตข้อมูลลง Master File สำหรับสร้างกราฟ ---\n",
        "print(f\"--- 💾 กำลังอัปเดตไฟล์ Master Report: {MASTER_OUTPUT_PATH} ---\")\n",
        "SHEET_NAME = 'Master_Evaluation'\n",
        "try:\n",
        "    with pd.ExcelFile(MASTER_OUTPUT_PATH) as xls:\n",
        "        master_df = pd.read_excel(xls, sheet_name=SHEET_NAME)\n",
        "        master_df = master_df[(master_df['Model'] != MODEL_NAME) | (master_df['Approach'] != APPROACH_NAME)]\n",
        "    combined_df = pd.concat([master_df, eval_summary_df], ignore_index=True)\n",
        "except FileNotFoundError:\n",
        "    print(f\"ไม่พบไฟล์ Master เดิม, กำลังสร้างไฟล์ใหม่...\")\n",
        "    combined_df = eval_summary_df\n",
        "\n",
        "with pd.ExcelWriter(MASTER_OUTPUT_PATH, engine='openpyxl') as writer:\n",
        "    combined_df.to_excel(writer, sheet_name=SHEET_NAME, index=False)\n",
        "\n",
        "print(f\"🎉 อัปเดตไฟล์ Master Report สำเร็จ!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bplEBKnXiEid"
      },
      "source": [
        "# **TRocr Boxing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "0dWgsHdaNlSV"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "s6CmWx3Siyb0"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import jiwer\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
        "\n",
        "# --- 1. กำหนดค่าและ Path ที่สำคัญ ---\n",
        "MODEL_NAME = 'TrOCR'\n",
        "APPROACH_NAME = 'Boxing'\n",
        "INDIVIDUAL_REPORT_PATH = \"/content/drive/MyDrive/kmitl_dataset/final-excel/trocr_boxing_report.xlsx\"\n",
        "MASTER_OUTPUT_PATH = \"/content/drive/MyDrive/kmitl_dataset/final-excel/final_comparison_report.xlsx\"\n",
        "\n",
        "IMAGE_DIR = \"/content/drive/MyDrive/kmitl_dataset/final-preprecessed\"\n",
        "GROUND_TRUTH_PATH = \"/content/drive/MyDrive/kmitl_dataset/dataset/คะแนน TEP (Pilot Study)_Total_IT.xlsx\"\n",
        "\n",
        "ROI_CONFIG = {\n",
        "    \"name\":           (200, 485, 600, 65),\n",
        "    \"application_no\": (1100, 485, 500, 60),\n",
        "    \"test_date\":      (1310, 545, 400, 60),\n",
        "    \"total\":          (820, 625, 800, 100),\n",
        "    \"grammar\":        (320, 785, 500, 80),\n",
        "    \"reading\":        (1050, 785, 500, 80),\n",
        "    \"speaking\":       (320, 900, 500, 80),\n",
        "    \"writing\":        (1050, 900, 500, 80),\n",
        "}\n",
        "\n",
        "# --- 2. ฟังก์ชันช่วยเหลือ (ฉบับสมบูรณ์) ---\n",
        "def parse_score(text):\n",
        "    if not text: return None, None\n",
        "    processed_text = text.replace('l', '1').replace('I', '1')\n",
        "    match = re.search(r'([A-Z][1-2])\\s*\\((\\d+)\\)', processed_text, re.IGNORECASE)\n",
        "    if match:\n",
        "        return match.group(1).upper(), match.group(2)\n",
        "    return None, None\n",
        "\n",
        "def get_id_from_filename(filename):\n",
        "    match = re.search(r'(\\d+)\\.(png|jpg|jpeg)$', filename.lower())\n",
        "    if match: return int(match.group(1))\n",
        "    return None\n",
        "\n",
        "def normalize_text(text):\n",
        "    if text is None: return \"\"\n",
        "    text = str(text).strip().lower()\n",
        "    if text.endswith('.0'): text = text[:-2]\n",
        "    return text\n",
        "\n",
        "# --- 3. โหลดและตรวจสอบข้อมูลก่อนเริ่ม ---\n",
        "# ... (ส่วนนี้เหมือนเดิมทุกประการ) ...\n",
        "print(\"--- 🔍 ขั้นตอนตรวจสอบข้อมูล ---\")\n",
        "gt_df = pd.read_excel(GROUND_TRUTH_PATH)\n",
        "ground_truth_ids = set(gt_df['No.'].unique())\n",
        "print(f\"พบ ID ใน Ground Truth ทั้งหมด: {len(ground_truth_ids)} ID\")\n",
        "allowed_extensions = ['.png', '.jpg', '.jpeg']\n",
        "all_files_in_dir = os.listdir(IMAGE_DIR)\n",
        "image_files = [f for f in all_files_in_dir if f.lower().endswith(tuple(allowed_extensions))]\n",
        "image_files.sort(key=get_id_from_filename)\n",
        "print(f\"พบไฟล์รูปภาพในโฟลเดอร์: {len(image_files)} ไฟล์\")\n",
        "print(\"--- ✅ ตรวจสอบข้อมูลเสร็จสิ้น ---\\n\")\n",
        "\n",
        "\n",
        "# --- 4. เริ่มกระบวนการสกัดข้อมูลด้วย TrOCR ---\n",
        "# ⬇️ โหลดโมเดลและ Processor ของ TrOCR (ทำครั้งเดียว)\n",
        "print(\"--- 🤖 กำลังโหลดโมเดล TrOCR ---\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"จะใช้ Device: {device}\")\n",
        "\n",
        "processor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-printed')\n",
        "model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed').to(device)\n",
        "print(\"--- ✅ โหลดโมเดลสำเร็จ! ---\\n\")\n",
        "\n",
        "results_list = []\n",
        "print(f\"--- 🚀 เริ่มการสกัดข้อมูลจากรูปภาพ {len(image_files)} ไฟล์ ด้วย {MODEL_NAME} ({APPROACH_NAME}) ---\")\n",
        "for filename in image_files:\n",
        "    image_id = get_id_from_filename(filename)\n",
        "    if image_id is None: continue\n",
        "    gt_row = gt_df[gt_df['No.'] == image_id]\n",
        "    if gt_row.empty: continue\n",
        "\n",
        "    print(f\"  - กำลังประมวลผล ID: {image_id} ({filename})\")\n",
        "    image_path = os.path.join(IMAGE_DIR, filename)\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    extracted_raw = {}\n",
        "    for field_name, (x, y, w, h) in ROI_CONFIG.items():\n",
        "        roi_image = image[y:y+h, x:x+w]\n",
        "\n",
        "        # ⬇️ ขั้นตอนการทำนายผลด้วย TrOCR\n",
        "        # TrOCR ไม่ต้องการ preprocessing แบบเก่า แต่ต้องการภาพแบบ RGB\n",
        "        roi_rgb = cv2.cvtColor(roi_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        pixel_values = processor(images=roi_rgb, return_tensors=\"pt\").pixel_values.to(device)\n",
        "        generated_ids = model.generate(pixel_values)\n",
        "        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "        text = generated_text.strip()\n",
        "\n",
        "        extracted_raw[field_name] = text\n",
        "\n",
        "    # (ส่วน Parse และ Append to results_list เหมือนเดิม)\n",
        "    # ...\n",
        "    g_level, g_score = parse_score(extracted_raw.get('grammar'))\n",
        "    r_level, r_score = parse_score(extracted_raw.get('reading'))\n",
        "    s_level, s_score = parse_score(extracted_raw.get('speaking'))\n",
        "    w_level, w_score = parse_score(extracted_raw.get('writing'))\n",
        "    t_level, t_score = parse_score(extracted_raw.get('total'))\n",
        "    results_list.append({\n",
        "        \"No.\": image_id,\n",
        "        \"Application No. (GT)\": gt_row.iloc[0][\"Application No.\"], \"Application No. (Pred)\": extracted_raw.get('application_no'),\n",
        "        \"Name (GT)\": gt_row.iloc[0][\"Name\"], \"Name (Pred)\": extracted_raw.get('name'),\n",
        "        \"Test Date (GT)\": gt_row.iloc[0][\"Test Date\"], \"Test Date (Pred)\": extracted_raw.get('test_date'),\n",
        "        \"Grammar_Level (GT)\": gt_row.iloc[0][\"Grammar_Level\"], \"Grammar_Level (Pred)\": g_level,\n",
        "        \"Grammar_Score (GT)\": gt_row.iloc[0][\"Grammar_Score\"], \"Grammar_Score (Pred)\": g_score,\n",
        "        \"Reading_Level (GT)\": gt_row.iloc[0][\"Reading_Level\"], \"Reading_Level (Pred)\": r_level,\n",
        "        \"Reading_Score (GT)\": gt_row.iloc[0][\"Reading_Score\"], \"Reading_Score (Pred)\": r_score,\n",
        "        \"Speaking_Level (GT)\": gt_row.iloc[0][\"Speaking_Level\"], \"Speaking_Level (Pred)\": s_level,\n",
        "        \"Speaking_Score (GT)\": gt_row.iloc[0][\"Speaking_Score\"], \"Speaking_Score (Pred)\": s_score,\n",
        "        \"Writing_Level (GT)\": gt_row.iloc[0][\"Writing_Level\"], \"Writing_Level (Pred)\": w_level,\n",
        "        \"Writing_Score (GT)\": gt_row.iloc[0][\"Writing_Score\"], \"Writing_Score (Pred)\": w_score,\n",
        "        \"Total_Level (GT)\": gt_row.iloc[0][\"Total_Level\"], \"Total_Level (Pred)\": t_level,\n",
        "        \"Total_Score (GT)\": gt_row.iloc[0][\"Total_Score\"], \"Total_Score (Pred)\": t_score,\n",
        "    })\n",
        "\n",
        "ocr_results_df = pd.DataFrame(results_list)\n",
        "print(\"--- ✅ สกัดข้อมูลเสร็จสิ้น ---\\n\")\n",
        "\n",
        "# --- 5. เริ่มกระบวนการประเมินผล (Evaluation) ---\n",
        "# ... (ส่วนนี้เหมือนเดิมทุกประการ) ...\n",
        "print(\"--- 📊 เริ่มการประเมินผล ---\")\n",
        "eval_df = ocr_results_df.fillna('')\n",
        "fields_to_evaluate = {\n",
        "    'Name': ('Name (GT)', 'Name (Pred)'), 'Application No.': ('Application No. (GT)', 'Application No. (Pred)'),\n",
        "    'Test Date': ('Test Date (GT)', 'Test Date (Pred)'), 'Grammar_Level': ('Grammar_Level (GT)', 'Grammar_Level (Pred)'),\n",
        "    'Grammar_Score': ('Grammar_Score (GT)', 'Grammar_Score (Pred)'), 'Reading_Level': ('Reading_Level (GT)', 'Reading_Level (Pred)'),\n",
        "    'Reading_Score': ('Reading_Score (GT)', 'Reading_Score (Pred)'), 'Speaking_Level': ('Speaking_Level (GT)', 'Speaking_Level (Pred)'),\n",
        "    'Speaking_Score': ('Speaking_Score (GT)', 'Speaking_Score (Pred)'), 'Writing_Level': ('Writing_Level (GT)', 'Writing_Level (Pred)'),\n",
        "    'Writing_Score': ('Writing_Score (GT)', 'Writing_Score (Pred)'), 'Total_Level': ('Total_Level (GT)', 'Total_Level (Pred)'),\n",
        "    'Total_Score': ('Total_Score (GT)', 'Total_Score (Pred)'),\n",
        "}\n",
        "evaluation_summary_list = []\n",
        "for field, (gt_col, pred_col) in fields_to_evaluate.items():\n",
        "    ground_truth = [normalize_text(t) for t in eval_df[gt_col]]\n",
        "    prediction = [normalize_text(t) for t in eval_df[pred_col]]\n",
        "    accuracy = np.mean([1 if gt == pred else 0 for gt, pred in zip(ground_truth, prediction)]) * 100\n",
        "    error_metrics = jiwer.compute_measures(ground_truth, prediction)\n",
        "    wer = error_metrics.get('wer', 0) * 100\n",
        "    cer = error_metrics.get('cer', 0) * 100\n",
        "    H = error_metrics.get('hits', 0); I = error_metrics.get('insertions', 0); D = error_metrics.get('deletions', 0); S = error_metrics.get('substitutions', 0)\n",
        "    precision = H / (H + I + S) if (H + I + S) > 0 else 0\n",
        "    recall = H / (H + D + S) if (H + D + S) > 0 else 0\n",
        "    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    evaluation_summary_list.append({\n",
        "        'Field': field, 'Accuracy (%)': round(accuracy, 2), 'WER (%)': round(wer, 2),\n",
        "        'CER (%)': round(cer, 2), 'F1-score (%)': round(f1_score * 100, 2)\n",
        "    })\n",
        "eval_summary_df = pd.DataFrame(evaluation_summary_list)\n",
        "eval_summary_df.insert(0, 'Approach', APPROACH_NAME)\n",
        "eval_summary_df.insert(0, 'Model', MODEL_NAME)\n",
        "print(eval_summary_df.to_string(index=False))\n",
        "print(\"--- ✅ ประเมินผลเสร็จสิ้น ---\\n\")\n",
        "\n",
        "\n",
        "# --- 6. บันทึกผลลัพธ์ (ทำ 2 อย่าง) ---\n",
        "# --- 6.A: บันทึกผลการทดลองนี้แยกไฟล์ (2 ชีต) ---\n",
        "print(f\"--- 💾 กำลังบันทึกรายงานเฉพาะของ {MODEL_NAME} ลงในไฟล์: {INDIVIDUAL_REPORT_PATH} ---\")\n",
        "with pd.ExcelWriter(INDIVIDUAL_REPORT_PATH, engine='openpyxl') as writer:\n",
        "    ocr_results_df.to_excel(writer, sheet_name='Detailed_Results', index=False)\n",
        "    eval_summary_df.drop(columns=['Model', 'Approach']).to_excel(writer, sheet_name='Evaluation_Summary', index=False)\n",
        "print(f\"🎉 บันทึกไฟล์ {INDIVIDUAL_REPORT_PATH} สำเร็จ!\")\n",
        "\n",
        "# --- 6.B: อัปเดตข้อมูลลง Master File สำหรับสร้างกราฟ ---\n",
        "print(f\"--- 💾 กำลังอัปเดตไฟล์ Master Report: {MASTER_OUTPUT_PATH} ---\")\n",
        "SHEET_NAME = 'Master_Evaluation'\n",
        "try:\n",
        "    with pd.ExcelFile(MASTER_OUTPUT_PATH) as xls:\n",
        "        master_df = pd.read_excel(xls, sheet_name=SHEET_NAME)\n",
        "        master_df = master_df[(master_df['Model'] != MODEL_NAME) | (master_df['Approach'] != APPROACH_NAME)]\n",
        "    combined_df = pd.concat([master_df, eval_summary_df], ignore_index=True)\n",
        "except FileNotFoundError:\n",
        "    print(f\"ไม่พบไฟล์ Master เดิม, กำลังสร้างไฟล์ใหม่...\")\n",
        "    combined_df = eval_summary_df\n",
        "with pd.ExcelWriter(MASTER_OUTPUT_PATH, engine='openpyxl') as writer:\n",
        "    combined_df.to_excel(writer, sheet_name=SHEET_NAME, index=False)\n",
        "\n",
        "print(f\"🎉 อัปเดตไฟล์ Master Report สำเร็จ!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5YP-TqLmeY3"
      },
      "source": [
        "# Visualization only Pure OCR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9CDGQ_fmeCq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# --- 1. กำหนดค่าและโหลดข้อมูล ---\n",
        "MASTER_REPORT_PATH = \"/content/drive/MyDrive/kmitl_dataset/final-excel/final_comparison_report.xlsx\"\n",
        "SHEET_NAME = 'Master_Evaluation'\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/kmitl_dataset/final-excel/graphs_pure_ocr/\" # ⬅️ สร้างโฟลเดอร์ใหม่สำหรับกราฟชุดนี้\n",
        "\n",
        "# สร้างโฟลเดอร์ถ้ายังไม่มี\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"กำลังโหลดข้อมูลจาก: {MASTER_REPORT_PATH}\")\n",
        "try:\n",
        "    df_all = pd.read_excel(MASTER_REPORT_PATH, sheet_name=SHEET_NAME)\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ ไม่พบไฟล์ Master Report! กรุณาตรวจสอบ Path: {MASTER_REPORT_PATH}\")\n",
        "    exit()\n",
        "\n",
        "# ⬇️⬇️⬇️ ส่วนที่เพิ่มเข้ามา: กรองข้อมูลให้เหลือเฉพาะ Pure OCR ⬇️⬇️⬇️\n",
        "df = df_all[df_all['Approach'].str.contains('Pure OCR', case=False)].copy()\n",
        "print(\"กรองข้อมูลสำเร็จ! แสดงผลเฉพาะแนวทาง Pure OCR เท่านั้น\")\n",
        "\n",
        "\n",
        "print(\"โหลดข้อมูลสำเร็จ! เริ่มสร้างกราฟ...\")\n",
        "\n",
        "# สร้างคอลัมน์ใหม่เพื่อใช้เป็นแกน X ในกราฟให้สวยงาม\n",
        "df['Model_Approach'] = df['Model'] + ' (' + df['Approach'] + ')'\n",
        "\n",
        "\n",
        "# --- 2. สร้างกราฟที่ 1: เปรียบเทียบประสิทธิภาพโดยรวม (ค่าเฉลี่ยของทุกฟิลด์) ---\n",
        "df_agg = df.groupby('Model_Approach')[['Accuracy (%)', 'WER (%)', 'CER (%)', 'F1-score (%)']].mean().reset_index()\n",
        "df_melted = df_agg.melt(id_vars='Model_Approach', var_name='Metric', value_name='Average Score')\n",
        "\n",
        "plt.figure(figsize=(16, 9))\n",
        "sns.barplot(data=df_melted, x='Metric', y='Average Score', hue='Model_Approach', palette='viridis')\n",
        "plt.title('Overall Average Performance Comparison (Pure OCR Only)', fontsize=20, pad=20)\n",
        "plt.ylabel('Average Score (%)', fontsize=14)\n",
        "plt.xlabel('Evaluation Metric', fontsize=14)\n",
        "plt.xticks(rotation=0, ha='center', fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "plt.legend(title='Model (Approach)', fontsize=11)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "\n",
        "# บันทึกกราฟ\n",
        "graph1_path = os.path.join(OUTPUT_DIR, '1_pure_ocr_overall_performance.png')\n",
        "plt.savefig(graph1_path)\n",
        "print(f\"✅ บันทึกกราฟที่ 1 สำเร็จ: {graph1_path}\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# --- 3. สร้างกราฟที่ 2: เจาะลึกประสิทธิภาพในแต่ละฟิลด์ (ตัวอย่าง: Name และ Application No.) ---\n",
        "fields_to_plot = ['Name', 'Application No.']\n",
        "for field in fields_to_plot:\n",
        "    df_field = df[df['Field'] == field]\n",
        "\n",
        "    plt.figure(figsize=(16, 9))\n",
        "    sns.barplot(data=df_field, x='Model_Approach', y='Accuracy (%)', palette='plasma')\n",
        "    plt.title(f'Accuracy Comparison for Field: \"{field}\" (Pure OCR Only)', fontsize=20, pad=20)\n",
        "    plt.ylabel('Accuracy (%)', fontsize=14)\n",
        "    plt.xlabel('Model (Approach)', fontsize=14)\n",
        "    plt.xticks(rotation=15, ha='right', fontsize=12)\n",
        "    plt.yticks(fontsize=12)\n",
        "    plt.ylim(0, 105)\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # บันทึกกราฟ\n",
        "    graph2_path = os.path.join(OUTPUT_DIR, f'2_pure_ocr_accuracy_for_{field.replace(\" \", \"_\")}.png')\n",
        "    plt.savefig(graph2_path)\n",
        "    print(f\"✅ บันทึกกราฟที่ 2 ({field}) สำเร็จ: {graph2_path}\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# --- 4. สร้างกราฟที่ 3: Heatmap แสดงภาพรวมทั้งหมด ---\n",
        "df_pivot = df.pivot_table(index='Field', columns='Model_Approach', values='Accuracy (%)')\n",
        "\n",
        "plt.figure(figsize=(12, 12)) # ปรับขนาดให้เหมาะกับจำนวนโมเดลที่น้อยลง\n",
        "sns.heatmap(df_pivot, annot=True, fmt=\".1f\", linewidths=.5, cmap='RdYlGn', vmin=0, vmax=100)\n",
        "plt.title('Overall Accuracy (%) Heatmap (Pure OCR Only)', fontsize=20, pad=20)\n",
        "plt.ylabel('Data Field', fontsize=14)\n",
        "plt.xlabel('Model (Approach)', fontsize=14)\n",
        "plt.xticks(rotation=20, ha='right')\n",
        "plt.tight_layout()\n",
        "\n",
        "# บันทึกกราฟ\n",
        "graph3_path = os.path.join(OUTPUT_DIR, '3_pure_ocr_accuracy_heatmap.png')\n",
        "plt.savefig(graph3_path)\n",
        "print(f\"✅ บันทึกกราฟที่ 3 สำเร็จ: {graph3_path}\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmAFX2oZm1hJ"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "### **บทสรุปสำหรับผู้บริหาร (Executive Summary)**\n",
        "\n",
        "โมเดลและแนวทางที่ให้ประสิทธิภาพ **ดีที่สุด ถูกต้องและแม่นยำที่สุด** สำหรับการสกัดข้อมูลจากใบรับรองผลคะแนนสอบ KMITL-TEP คือ **TrOCR เมื่อใช้ร่วมกับแนวทาง Boxing OCR** อย่างไม่มีข้อโต้แย้ง โดยให้ค่า Accuracy และ F1-score สูงที่สุด และมีค่า Error Rate (WER/CER) ต่ำที่สุดในทุกการทดสอบ\n",
        "\n",
        "---\n",
        "\n",
        "### **การวิเคราะห์ผลในรายละเอียด**\n",
        "\n",
        "เราสามารถแบ่งการวิเคราะห์ออกเป็น 2 ประเด็นหลัก คือการเปรียบเทียบระหว่าง \"แนวทาง\" และการเปรียบเทียบระหว่าง \"โมเดล\"\n",
        "\n",
        "#### **1. การเปรียบเทียบแนวทาง: Boxing OCR vs. Pure OCR**\n",
        "\n",
        "จากกราฟทั้งหมด โดยเฉพาะ **กราฟที่ 1 (Overall Performance)** และ **กราฟที่ 4 (Heatmap)** แสดงให้เห็นอย่างชัดเจนว่า:\n",
        "\n",
        "* **Boxing OCR (การระบุตำแหน่ง) ชนะขาดลอย:** แนวทางการใช้ Bounding Box ให้ผลลัพธ์ที่แม่นยำกว่าแนวทาง Pure OCR ในทุกๆ โมเดลและทุกๆ Metric ที่วัดผล\n",
        "* **เหตุผล:** เนื่องจากใบรับรองผลคะแนนสอบนี้มี **โครงสร้างที่ตายตัว (Fixed Layout)** การกำหนดขอบเขตข้อมูลที่ชัดเจนก่อนส่งให้ OCR อ่าน จะช่วยลดความผิดพลาดและทำให้โมเดลทำงานได้ง่ายขึ้น ในขณะที่แนวทาง Pure OCR ต้องเจอกับความท้าทายในการทำความเข้าใจโครงสร้างทั้งหมดของหน้า ซึ่งซับซ้อนและมีโอกาสผิดพลาดสูงกว่า\n",
        "\n",
        "#### **2. การเปรียบเทียบโมเดล (เมื่อใช้แนวทาง Boxing ที่ดีที่สุด)**\n",
        "\n",
        "เมื่อเราพิจารณาเฉพาะแนวทาง Boxing ที่ให้ผลดีที่สุด เราสามารถจัดอันดับความแม่นยำของโมเดลได้ดังนี้:\n",
        "\n",
        "🥇 **อันดับที่ 1: TrOCR (Boxing)**\n",
        "* **เป็นผู้ชนะเลิศของโปรเจกต์นี้** จาก Heatmap (กราฟที่ 4) จะเห็นว่าคอลัมน์ของ `TrOCR (Boxing)` เป็นสีเขียวเข้มเกือบทั้งหมด แสดงให้เห็นถึงประสิทธิภาพที่ยอดเยี่ยมและมีความเสถียรสูงในทุกๆ ฟิลด์ข้อมูล\n",
        "* **จุดแข็ง:** มีค่า Accuracy และ F1-score ใกล้เคียง 100% และมีค่า WER/CER ต่ำมาก ซึ่งเหมาะสมอย่างยิ่งสำหรับงานที่ต้องการความถูกต้องสูงสุด\n",
        "\n",
        "🥈 **อันดับที่ 2: EasyOCR (Boxing)**\n",
        "* **เป็นตัวเลือกที่ดีมากและเป็นอันดับสองที่แข็งแกร่ง** ให้ผลลัพธ์ที่แม่นยำสูงในเกือบทุกฟิลด์ (ส่วนใหญ่เป็นสีเขียวอ่อนใน Heatmap) และดีกว่า Pytesseract อย่างเห็นได้ชัด\n",
        "* **จุดแข็ง:** เป็นโมเดลที่สมดุลระหว่างความเร็วและความแม่นยำ ใช้งานง่าย และไม่ต้องการทรัพยากร (GPU) มากเท่า TrOCR\n",
        "\n",
        "🥉 **อันดับที่ 3: Pytesseract (Boxing)**\n",
        "* **เป็นโมเดลพื้นฐานที่ดี** แต่ประสิทธิภาพโดยรวมยังสู้โมเดลที่ใหม่กว่าอย่าง EasyOCR และ TrOCR ไม่ได้ โดยเฉพาะในฟิลด์ที่เป็นตัวเลขหรือตัวอักษรที่ติดกันมากๆ\n",
        "* **จุดแข็ง:** ติดตั้งง่ายและเป็นที่รู้จักแพร่หลาย เหมาะสำหรับงานที่ไม่ต้องการความแม่นยำสูงมากนัก\n",
        "\n",
        "---\n",
        "\n",
        "### **ข้อเสนอแนะสุดท้าย (Final Recommendation)**\n",
        "\n",
        "* **เพื่อความแม่นยำสูงสุด:** หากต้องการระบบที่ **ถูกต้องและน่าเชื่อถือที่สุด** สำหรับนำไปใช้งานจริง (Production) ควรเลือกใช้ **TrOCR ร่วมกับแนวทาง Boxing OCR**\n",
        "* **ตัวเลือกที่สมดุล:** หากมีข้อจำกัดด้านทรัพยากร เช่น ไม่สามารถใช้ GPU ได้ หรือต้องการความเร็วในการประมวลผลสูง **EasyOCR ร่วมกับแนวทาง Boxing OCR** เป็นตัวเลือกที่ยอดเยี่ยม ให้ผลลัพธ์ที่แม่นยำมากเพียงพอสำหรับการใช้งานส่วนใหญ่\n",
        "\n",
        "การทดลองนี้แสดงให้เห็นว่า นอกจากการเลือก \"โมเดล\" ที่ดีแล้ว การเลือก \"แนวทาง\" (Approach) ให้เหมาะสมกับลักษณะของเอกสารก็มีความสำคัญไม่แพ้กันครับ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdoIsYAJg5R_"
      },
      "source": [
        "# Visualization all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "kQu6QBtXg971"
      },
      "outputs": [],
      "source": [
        "pip install matplotlib seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtqVxijphKZO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# --- 1. กำหนดค่าและโหลดข้อมูล ---\n",
        "MASTER_REPORT_PATH = \"/content/drive/MyDrive/kmitl_dataset/final-excel/final_comparison_report.xlsx\"\n",
        "SHEET_NAME = 'Master_Evaluation'\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/kmitl_dataset/final-excel/graphs/\" # โฟลเดอร์สำหรับเก็บกราฟ\n",
        "\n",
        "# สร้างโฟลเดอร์ถ้ายังไม่มี\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"กำลังโหลดข้อมูลจาก: {MASTER_REPORT_PATH}\")\n",
        "try:\n",
        "    df = pd.read_excel(MASTER_REPORT_PATH, sheet_name=SHEET_NAME)\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ ไม่พบไฟล์ Master Report! กรุณาตรวจสอบ Path: {MASTER_REPORT_PATH}\")\n",
        "    exit()\n",
        "\n",
        "print(\"โหลดข้อมูลสำเร็จ! เริ่มสร้างกราฟ...\")\n",
        "\n",
        "# สร้างคอลัมน์ใหม่เพื่อใช้เป็นแกน X ในกราฟให้สวยงาม\n",
        "df['Model_Approach'] = df['Model'] + ' (' + df['Approach'] + ')'\n",
        "\n",
        "\n",
        "# --- 2. สร้างกราฟที่ 1: เปรียบเทียบประสิทธิภาพโดยรวม (ค่าเฉลี่ยของทุกฟิลด์) ---\n",
        "# คำนวณค่าเฉลี่ยของแต่ละ Metric โดยแยกตาม Model และ Approach\n",
        "df_agg = df.groupby('Model_Approach')[['Accuracy (%)', 'WER (%)', 'CER (%)', 'F1-score (%)']].mean().reset_index()\n",
        "\n",
        "# Melt DataFrame เพื่อให้ง่ายต่อการพล็อตด้วย Seaborn\n",
        "df_melted = df_agg.melt(id_vars='Model_Approach', var_name='Metric', value_name='Average Score')\n",
        "\n",
        "plt.figure(figsize=(16, 9))\n",
        "sns.barplot(data=df_melted, x='Metric', y='Average Score', hue='Model_Approach', palette='viridis')\n",
        "plt.title('Overall Average Performance Comparison', fontsize=20, pad=20)\n",
        "plt.ylabel('Average Score (%)', fontsize=14)\n",
        "plt.xlabel('Evaluation Metric', fontsize=14)\n",
        "plt.xticks(rotation=0, ha='center', fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "plt.legend(title='Model (Approach)', fontsize=11)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "\n",
        "# บันทึกกราฟ\n",
        "graph1_path = os.path.join(OUTPUT_DIR, '1_overall_performance.png')\n",
        "plt.savefig(graph1_path)\n",
        "print(f\"✅ บันทึกกราฟที่ 1 สำเร็จ: {graph1_path}\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# --- 3. สร้างกราฟที่ 2: เจาะลึกประสิทธิภาพในแต่ละฟิลด์ (ตัวอย่าง: Name และ Application No.) ---\n",
        "fields_to_plot = ['Name', 'Application No.']\n",
        "for field in fields_to_plot:\n",
        "    df_field = df[df['Field'] == field]\n",
        "\n",
        "    plt.figure(figsize=(16, 9))\n",
        "    sns.barplot(data=df_field, x='Model_Approach', y='Accuracy (%)', palette='plasma')\n",
        "    plt.title(f'Accuracy Comparison for Field: \"{field}\"', fontsize=20, pad=20)\n",
        "    plt.ylabel('Accuracy (%)', fontsize=14)\n",
        "    plt.xlabel('Model (Approach)', fontsize=14)\n",
        "    plt.xticks(rotation=15, ha='right', fontsize=12)\n",
        "    plt.yticks(fontsize=12)\n",
        "    plt.ylim(0, 105) # กำหนดให้แกน Y เต็ม 100%\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # บันทึกกราฟ\n",
        "    graph2_path = os.path.join(OUTPUT_DIR, f'2_accuracy_for_{field.replace(\" \", \"_\")}.png')\n",
        "    plt.savefig(graph2_path)\n",
        "    print(f\"✅ บันทึกกราฟที่ 2 ({field}) สำเร็จ: {graph2_path}\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# --- 4. สร้างกราฟที่ 3: Heatmap แสดงภาพรวมทั้งหมด ---\n",
        "# เราจะใช้ Accuracy เป็นตัวชี้วัดหลักใน Heatmap\n",
        "df_pivot = df.pivot_table(index='Field', columns='Model_Approach', values='Accuracy (%)')\n",
        "\n",
        "plt.figure(figsize=(16, 12))\n",
        "sns.heatmap(df_pivot, annot=True, fmt=\".1f\", linewidths=.5, cmap='RdYlGn', vmin=0, vmax=100)\n",
        "plt.title('Overall Accuracy (%) Heatmap', fontsize=20, pad=20)\n",
        "plt.ylabel('Data Field', fontsize=14)\n",
        "plt.xlabel('Model (Approach)', fontsize=14)\n",
        "plt.xticks(rotation=20, ha='right')\n",
        "plt.tight_layout()\n",
        "\n",
        "# บันทึกกราฟ\n",
        "graph3_path = os.path.join(OUTPUT_DIR, '3_accuracy_heatmap.png')\n",
        "plt.savefig(graph3_path)\n",
        "print(f\"✅ บันทึกกราฟที่ 3 สำเร็จ: {graph3_path}\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4SKR-qrjPom"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "df = pd.read_excel('/content/drive/MyDrive/kmitl_dataset/final-excel/final_comparison_report.xlsx')\n",
        "\n",
        "# Display the first 5 rows\n",
        "print(df.head().to_string(index=False))\n",
        "\n",
        "# Print the column names and their data types\n",
        "print(df.info())\n",
        "\n",
        "# Get unique values in `Model` column\n",
        "print(df['Model'].unique())\n",
        "\n",
        "# Get unique values in `Approach` column\n",
        "print(df['Approach'].unique())\n",
        "\n",
        "# Group by `Model` and `Approach` and calculate the mean of the performance metrics\n",
        "df_agg = df.groupby(['Model', 'Approach']).agg(\n",
        "    Accuracy=('Accuracy (%)', 'mean'),\n",
        "    WER=('WER (%)', 'mean'),\n",
        "    CER=('CER (%)', 'mean'),\n",
        "    F1_score=('F1-score (%)', 'mean')\n",
        ").reset_index()\n",
        "\n",
        "# Print the aggregated DataFrame\n",
        "print(df_agg.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfO3vlLjj29Z"
      },
      "outputs": [],
      "source": [
        "import altair as alt\n",
        "# Create a bar chart for average F1-score by Model and Approach\n",
        "chart = alt.Chart(df_agg).mark_bar().encode(\n",
        " x=alt.X('Model', axis=alt.Axis(title='Model')),\n",
        " y=alt.Y('F1_score', axis=alt.Axis(title='F1-score (%)')),\n",
        " color=alt.Color('Approach', title='Approach'),\n",
        " tooltip=['Model', 'Approach', 'F1_score']\n",
        ").properties(\n",
        " title='Average F1-score by Model and Approach'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdWkgZdlg9jD"
      },
      "source": [
        "1.  **ประสิทธิภาพของโมเดลและแนวทางที่แตกต่างกัน**:\n",
        "    *   **Pytesseract** และ **TrOCR** มีประสิทธิภาพดีที่สุดเมื่อใช้แนวทาง **Boxing** โดยมีค่าเฉลี่ย `Accuracy (%)` และ `F1-score (%)` สูงสุด และ `WER (%)` ต่ำสุด\n",
        "    *   **EasyOCR** มีประสิทธิภาพดีที่สุดเมื่อใช้แนวทาง **Boxing** เช่นกัน แต่มีค่าเฉลี่ย `Accuracy (%)` และ `F1-score (%)` ต่ำกว่าเล็กน้อยเมื่อเทียบกับ Pytesseract และ TrOCR\n",
        "    *   แนวทาง **Pure OCR (Hybrid)** ของ **TrOCR** แสดงประสิทธิภาพที่แย่ที่สุด โดยมี `Accuracy (%)` และ `F1-score (%)` เป็น 0 และ `WER (%)` เป็น 100 ซึ่งอาจบ่งชี้ถึงปัญหาในการใช้งานหรือความไม่เหมาะสมของแนวทางนี้สำหรับโมเดล TrOCR ในชุดข้อมูลนี้\n",
        "    *   แนวทาง **Pure OCR (Spatial)** ของ **Pytesseract** ก็มีประสิทธิภาพต่ำเช่นกัน โดยมี `Accuracy (%)` และ `F1-score (%)` ต่ำ และ `WER (%)` สูง\n",
        "\n",
        "2.  **ความสำคัญของแนวทาง Boxing**:\n",
        "    *   แนวทาง **Boxing** ให้ประสิทธิภาพที่ดีที่สุดสำหรับทุกโมเดล (Pytesseract, EasyOCR, TrOCR) ในแง่ของ `Accuracy (%)`, `WER (%)`, และ `F1-score (%)`\n",
        "    *   สิ่งนี้ชี้ให้เห็นว่าการระบุขอบเขตของข้อความอย่างแม่นยำก่อนการรู้จำด้วย OCR (Optical Character Recognition) มีผลอย่างมากต่อความถูกต้องของผลลัพธ์\n",
        "\n",
        "3.  **ข้อสังเกตเกี่ยวกับ CER (%)**:\n",
        "    *   ค่า `CER (%)` เป็น 0 สำหรับทุกโมเดลและแนวทาง ซึ่งผิดปกติและอาจบ่งชี้ว่าคอลัมน์นี้ไม่ได้สะท้อนถึงความแตกต่างของประสิทธิภาพที่แท้จริง หรืออาจมีข้อผิดพลาดในการรวบรวมข้อมูลหรือการคำนวณ\n",
        "\n",
        "การวิเคราะห์ข้อมูลแสดงให้เห็นถึงประสิทธิภาพของโมเดลและแนวทางต่างๆ ในการรู้จำอักขระด้วยแสง (OCR) โดยมีข้อสังเกตที่สำคัญดังนี้:\n",
        "\n",
        "**ประเด็นสำคัญ:**\n",
        "\n",
        "1.  **ประสิทธิภาพที่แตกต่างกันของโมเดลและแนวทาง:**\n",
        "    *   **Pytesseract** และ **TrOCR** แสดงประสิทธิภาพที่โดดเด่นที่สุดเมื่อใช้แนวทาง **Boxing** โดยมีค่าเฉลี่ยความแม่นยำ (Accuracy) และ F1-score สูงสุด และอัตราข้อผิดพลาดของคำ (WER) ต่ำสุด\n",
        "    *   **EasyOCR** มีประสิทธิภาพที่ดีที่สุดเมื่อใช้แนวทาง **Boxing** เช่นกัน แต่โดยรวมแล้ว Pytesseract และ TrOCR มีประสิทธิภาพที่เหนือกว่า\n",
        "    *   แนวทาง **Pure OCR (Hybrid)** ของ **TrOCR** แสดงประสิทธิภาพที่แย่ที่สุด โดยมีค่าความแม่นยำและ F1-score เป็น 0% และ WER เป็น 100% ซึ่งบ่งชี้ถึงปัญหาสำคัญในการใช้งานหรือความไม่เหมาะสมของแนวทางนี้สำหรับ TrOCR\n",
        "    *   แนวทาง **Pure OCR (Spatial)** ของ **Pytesseract** ก็แสดงประสิทธิภาพที่ค่อนข้างต่ำเช่นกัน\n",
        "\n",
        "2.  **ความสำคัญของแนวทาง Boxing:**\n",
        "    *   แนวทาง **Boxing** ให้ผลลัพธ์ที่ดีที่สุดอย่างสม่ำเสมอสำหรับทุกโมเดลที่ทดสอบ (Pytesseract, EasyOCR, TrOCR) ในทุกตัวชี้วัดประสิทธิภาพ (Accuracy, WER, F1-score)\n",
        "    *   สิ่งนี้เน้นย้ำถึงความสำคัญของการระบุขอบเขตของข้อความอย่างแม่นยำก่อนการประมวลผล OCR ซึ่งส่งผลโดยตรงต่อความถูกต้องของผลลัพธ์\n",
        "\n",
        "3.  **ข้อสังเกตเกี่ยวกับ CER (%):**\n",
        "    *   ค่าอัตราข้อผิดพลาดของอักขระ (CER) เป็น 0% สำหรับทุกโมเดลและแนวทาง ซึ่งเป็นเรื่องผิดปกติและอาจบ่งชี้ว่าคอลัมน์นี้ไม่ได้สะท้อนถึงความแตกต่างของประสิทธิภาพที่แท้จริง หรืออาจมีข้อผิดพลาดในการรวบรวมข้อมูลหรือการคำนวณ\n",
        "\n",
        "**สรุป:**\n",
        "\n",
        "โดยรวมแล้ว แนวทาง **Boxing** เป็นแนวทางที่มีประสิทธิภาพสูงสุดสำหรับงาน OCR ในชุดข้อมูลนี้ โดยเฉพาะอย่างยิ่งเมื่อใช้ร่วมกับโมเดล **Pytesseract** และ **TrOCR**\n",
        "\n",
        "นี่คือแผนภูมิที่แสดงค่าเฉลี่ยความแม่นยำ, อัตราข้อผิดพลาดของคำ (WER), และ F1-score ตามโมเดลและแนวทาง:\n",
        "\n",
        "*   **Average Accuracy by Model and Approach**\n",
        "*   **Average Word Error Rate (WER) by Model and Approach**\n",
        "*   **Average F1-score by Model and Approach**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "ZchumLaT2_-Z",
        "b7zujbWD5iSE",
        "VEo-U66vu107",
        "G0pRHBRmjpUV"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}